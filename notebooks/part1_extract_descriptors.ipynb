{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OOD detection applied to Hallucination Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#/home/lila.roig/.env/ood_env/bin/python "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I. Extract Descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "# -----------------------------------\n",
    "import torch\n",
    "import sys\n",
    "import time \n",
    "import os \n",
    "import pickle\n",
    "# Add the path to the src directory\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\n",
      "Cuda version: 12.6\n",
      "Number of available de GPU : 2\n",
      "GPU 1 : NVIDIA GeForce RTX 4090\n",
      "GPU 2 : NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "# Visualize setup \n",
    "# -----------------------------------\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Cuda version: {torch.version.cuda}\")\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Number of available de GPU : {num_gpus}\")\n",
    "for i in range(num_gpus):\n",
    "    print(f\"GPU {i + 1} : {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define global variables\n",
    "# -----------------------------------\n",
    "SEED = 777\n",
    "BATCH_SIZE = 16\n",
    "MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# Extract descriptors from: prompt, generated answer or both concatenated \n",
    "ACTIVATION_SOURCE = \"prompt\"  # 'prompt', 'generation', 'PromptGeneration'\n",
    "\n",
    "# Define layers to extract descriptors from\n",
    "LAYERS = list(range(1, 31, 2)) + [-1] # (List[int]) - Layers from witch retrieve the scores \n",
    "\n",
    "# Define descriptors aggregations \n",
    "HIDDEN_AGG = [\"avg_emb\", \"last_emb\", \"max_emb\", \"first_gen_emb\", \"hidden_score\", \"feat_var_emb\"]\n",
    "ATTN_AGG = [\"attn_score\"]\n",
    "LOGIT_AGG = [\"perplexity_score\", \"logit_entropy_score\", \"window_logit_entropy_score\"]\n",
    "LOGIT_CONFIG = {\"top_k\": 50, \"window_size\": 1, \"stride\": 1}\n",
    "\n",
    "\n",
    "# Define repository names\n",
    "OUTPUT_DIR = f\"../results/raw/TEST/small_dataset_allConfig_seed{SEED}/\"\n",
    "STR_AGG = 'All'\n",
    "STR_LAYERS = '1:32:2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed everything\n",
    "# -----------------------------------\n",
    "from src.utils.general import seed_all\n",
    "seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear memory to avoid \"CUDA out of memory\"\n",
    "# -----------------------------------\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc5fa66f23014461b15bd75d531761f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model\n",
    "# -----------------------------------\n",
    "from src.model_loader.llama_loader import load_llama\n",
    "\n",
    "model, tokenizer = load_llama(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load ID dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Dataset Information =====\n",
      "Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers', 'original_index', 'is_impossible'],\n",
      "    num_rows: 10000\n",
      "})\n",
      "Mean ground-truth answer length: 3.19, Max length: 29\n",
      "Mean context + question length: 129.88, Max length: 661\n"
     ]
    }
   ],
   "source": [
    "# Load ID dataset\n",
    "# -----------------------------------\n",
    "from src.data_reader.squad_loader import load_id_fit_dataset \n",
    "# Total number of samples in squad v2.0 train set: 86821\n",
    "\n",
    "id_fit_dataset = load_id_fit_dataset()\n",
    "id_fit_dataset = id_fit_dataset.shuffle(SEED) \n",
    "id_fit_dataset = id_fit_dataset.slice(idx_start=0, idx_end=10_000) \n",
    "id_fit_dataset.print_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom src.inference.run_extraction import analyze_single_generation, build_prompt\\nfrom src.inference.hooks import register_forward_activation_hook, extract_token_activations\\n\\n_ = analyze_single_generation(\\n    model=model,\\n    tokenizer=tokenizer,\\n    dataset=id_fit_dataset,\\n    sample_idx=3,\\n    build_prompt_fn=build_prompt,\\n    register_forward_activation_hook_fn=register_forward_activation_hook,\\n    layer_idx=-1,\\n    extract_token_activations_fn=partial(extract_token_activations, mode=EXTRACTION_MODE),\\n) \\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize one generation with the ID dataset\n",
    "# -----------------------------------\n",
    "# TODO: ADAPT TO NEW CODE VERSION\n",
    "'''\n",
    "from src.inference.run_extraction import analyze_single_generation, build_prompt\n",
    "from src.inference.hooks import register_forward_activation_hook, extract_token_activations\n",
    "\n",
    "_ = analyze_single_generation(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=id_fit_dataset,\n",
    "    sample_idx=3,\n",
    "    build_prompt_fn=build_prompt,\n",
    "    register_forward_activation_hook_fn=register_forward_activation_hook,\n",
    "    layer_idx=-1,\n",
    "    extract_token_activations_fn=partial(extract_token_activations, mode=EXTRACTION_MODE),\n",
    ") \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Define more global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute offsets to select the tokens to give to the model\n",
    "# -----------------------------------\n",
    "from src.inference.generation_utils import build_prompt\n",
    "from src.inference.offset_utils import compute_token_offsets\n",
    "\n",
    "if False:\n",
    "    idx = 67\n",
    "    text = build_prompt(id_fit_dataset[idx][\"context\"], id_fit_dataset[idx][\"question\"])\n",
    "    \n",
    "    START_OFFSET, END_OFFSET = compute_token_offsets(\n",
    "        text=text,\n",
    "        tokenizer=tokenizer,\n",
    "        start_phrase=\"Passage:\", \n",
    "        end_phrase=\" [/INST]\",\n",
    "        include_start_phrase=True,\n",
    "        include_end_phrase=False,\n",
    "        debug=True,\n",
    "        )\n",
    "\n",
    "if True:\n",
    "    START_OFFSET=0\n",
    "    END_OFFSET=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT_TITLE: _layer1:32:2_aggAll_prompt_so0_eo0_seed777\n"
     ]
    }
   ],
   "source": [
    "# Define file name\n",
    "OUTPUT_TITLE = f\"_layer{STR_LAYERS}_agg{STR_AGG}_{ACTIVATION_SOURCE}_so{START_OFFSET}_eo{END_OFFSET}_seed{SEED}\"\n",
    "\n",
    "print(f'OUTPUT_TITLE: {OUTPUT_TITLE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Compare responses & create new correct dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start generating ID answers and comparing them to ground-truth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:15<00:00,  2.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...end!\n",
      "ID answers: Time elapsed: 00 min 15 sec\n",
      "\n",
      "Directory '../results/raw/TEST/small_dataset_allConfig_seed777/id_fit_results_answers_seed777' deleted.\n"
     ]
    }
   ],
   "source": [
    "# Retrieve ID generated responses and compare them to ground-truth \n",
    "# -----------------------------------\n",
    "from src.inference.run_extraction import run_filter_generated_answers_by_similarity\n",
    "from src.inference.generation_utils import build_prompt\n",
    "from src.utils.general import print_time_elapsed\n",
    "from src.data_reader.pickle_io import merge_batches_and_cleanup\n",
    "\n",
    "output_path = OUTPUT_DIR + f\"id_fit_results_answers_seed{SEED}\" \n",
    "\n",
    "# Runs batched inference on a dataset using a decoder-only language model.\n",
    "# For each batch, generates answers, computes semantic similarity scores, \n",
    "# and appends the results to a pickle file.\n",
    "print(\"\\nStart generating ID answers and comparing them to ground-truth...\")\n",
    "t0 = time.time()\n",
    "run_filter_generated_answers_by_similarity(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=id_fit_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    idx_start_sample=0,\n",
    "    max_samples=len(id_fit_dataset),\n",
    "    output_path=output_path,\n",
    "    build_prompt_fn=build_prompt\n",
    ")\n",
    "t1 = time.time()\n",
    "print(\"...end!\")\n",
    "print_time_elapsed(t0, t1, label=\"ID answers: \")\n",
    "\n",
    "# Merge all batches, save as a single file and delete batch directory\n",
    "_= merge_batches_and_cleanup(directory=output_path, delete=True, confirm='user') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 112 samples from: ../results/raw/TEST/small_dataset_allConfig_seed777/id_fit_results_answers_seed777.pkl\n",
      "Size before filtering: 112. Size after filtering: 82. Filtered 30 samples.\n"
     ]
    }
   ],
   "source": [
    "# Load ID responses and only keep correct entries \n",
    "# -----------------------------------\n",
    "from src.data_reader.pickle_io import load_pickle_batches\n",
    "from src.utils.general import filter_entries\n",
    "\n",
    "# Load extracted answers \n",
    "id_fit_answers = load_pickle_batches(OUTPUT_DIR + f\"id_fit_results_answers_seed{SEED}.pkl\" )\n",
    "# Only keep rows where the generated responses are similar to the ground-truth answers\n",
    "ids_correct_answers = filter_entries(id_fit_answers, column='is_correct', value=1)[\"id\"]\n",
    "# Create a new dataset contaning only the correct answers \n",
    "id_fit_correct_dataset =  id_fit_dataset.filter_by_column('id', ids_correct_answers)\n",
    "# Save the new correct dataset for later use\n",
    "#id_fit_correct_dataset.save(f\"../data/datasets/id_fit_correct_dataset_small_seed{SEED}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Dataset Information =====\n",
      "Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers', 'original_index', 'is_impossible'],\n",
      "    num_rows: 8008\n",
      "})\n",
      "Mean ground-truth answer length: 3.01, Max length: 27\n",
      "Mean context + question length: 129.19, Max length: 568\n"
     ]
    }
   ],
   "source": [
    "# Load correct dataset\n",
    "# -----------------------------------\n",
    "with open(f\"../data/datasets/id_fit_correct_dataset_small_seed{SEED}.pkl\", \"rb\") as f:\n",
    "    id_fit_correct_dataset = pickle.load(f)\n",
    "\n",
    "id_fit_correct_dataset.print_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Retrieve ID fit descriptors \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1. Retrieve ID fit descriptors from input\n",
    "\n",
    "For this section, `ACTIVATION_SOURCE` is `'prompt'` by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve ID descriptors and save results \n",
    "# -----------------------------------\n",
    "from src.inference.run_extraction import run_prompt_descriptor_extraction\n",
    "from src.inference.generation_utils import build_prompt\n",
    "from src.utils.general import print_time_elapsed\n",
    "from src.data_reader.pickle_io import merge_batches_and_cleanup\n",
    "\n",
    "output_path = f\"{OUTPUT_DIR}id_fit_results{OUTPUT_TITLE}\" \n",
    "\n",
    "# Runs batched inference on a dataset using a decoder-only language model.\n",
    "# For each batch, extracts token-level activations/attention/logits, \n",
    "# and appends the results to a pickle file.\n",
    "print(\"\\nStart retrieving ID fit descriptors from inputs...\")\n",
    "t2 = time.time()\n",
    "res = run_prompt_descriptor_extraction(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=id_fit_correct_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    idx_start_sample=0,\n",
    "    max_samples=len(id_fit_correct_dataset),\n",
    "    save_to_pkl=False,\n",
    "    output_path=output_path,\n",
    "    build_prompt_fn=build_prompt,\n",
    "    layers=LAYERS,  \n",
    "    hidden_agg=HIDDEN_AGG,\n",
    "    attn_agg=ATTN_AGG,\n",
    "    logit_agg=LOGIT_AGG,\n",
    "    logit_config=LOGIT_CONFIG,\n",
    "    start_offset=START_OFFSET,\n",
    "    end_offset=END_OFFSET\n",
    ")\n",
    "t3 = time.time()\n",
    "print(\"...end!\")\n",
    "print_time_elapsed(t2, t3, label=\"ID descriptors: \")\n",
    "\n",
    "# Merge all batches, save as a single file and delete batch directory\n",
    "_= merge_batches_and_cleanup(directory=output_path, delete=True, confirm='user') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free memory \n",
    "del id_fit_dataset \n",
    "del id_fit_correct_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2. Retrieve ID fit descriptors from generated answer\n",
    "\n",
    "For this section, `ACTIVATION_SOURCE` can be either `'prompt'`, `'generation'` or `'promptGeneration'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve ID descriptors and save results \n",
    "# -----------------------------------\n",
    "from src.inference.run_extraction import run_prompt_and_generation_descriptor_extraction\n",
    "from src.inference.generation_utils import build_prompt\n",
    "from src.utils.general import print_time_elapsed\n",
    "from src.data_reader.pickle_io import merge_batches_and_cleanup\n",
    "\n",
    "output_path = f\"{OUTPUT_DIR}id_fit_results{OUTPUT_TITLE}\"\n",
    "\n",
    "# Runs batched inference on a dataset using a decoder-only language model.\n",
    "# For each batch, generates answers, extracts token-level activations for the generated answer,\n",
    "# and appends the results to a pickle file.\n",
    "print(f\"\\nStart retrieving ID fit descriptors from {ACTIVATION_SOURCE}...\")\n",
    "t2 = time.time()\n",
    "res = run_prompt_and_generation_descriptor_extraction(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=id_fit_correct_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    idx_start_sample=0,\n",
    "    max_samples=len(id_fit_correct_dataset),\n",
    "    save_to_pkl=False,\n",
    "    output_path=output_path,\n",
    "    build_prompt_fn=build_prompt,\n",
    "    layers=LAYERS,  \n",
    "    activation_source=ACTIVATION_SOURCE,\n",
    "    hidden_agg=HIDDEN_AGG,\n",
    "    attn_agg=ATTN_AGG,\n",
    "    logit_agg=LOGIT_AGG,\n",
    "    logit_config=LOGIT_CONFIG,\n",
    "    start_offset=START_OFFSET,\n",
    "    end_offset=END_OFFSET\n",
    ")\n",
    "t3 = time.time()\n",
    "print(\"...end!\")\n",
    "print_time_elapsed(t2, t3, label=\"ID descriptors: \")\n",
    "\n",
    "# Merge all batches, save as a single file and delete batch directory\n",
    "_= merge_batches_and_cleanup(directory=output_path, delete=True, confirm='user') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free memory \n",
    "del id_fit_dataset \n",
    "del id_fit_correct_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Load Test datasets (that may contain OOD/Hallucinated samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Dataset Information =====\n",
      "Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers', 'original_index', 'is_impossible'],\n",
      "    num_rows: 1000\n",
      "})\n",
      "Mean ground-truth answer length: 3.06, Max length: 25\n",
      "Mean context + question length: 140.19, Max length: 651\n",
      "\n",
      "===== Dataset Information =====\n",
      "Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers', 'original_index', 'is_impossible'],\n",
      "    num_rows: 1000\n",
      "})\n",
      "No valid ground-truth answers to compute length stats.\n",
      "Mean context + question length: 137.03, Max length: 553\n"
     ]
    }
   ],
   "source": [
    "# Load test datasets\n",
    "# -----------------------------------\n",
    "from src.data_reader.squad_loader import load_id_test_dataset, load_od_test_dataset\n",
    "\n",
    "# Load possible test dataset \n",
    "id_test_dataset = load_id_test_dataset()\n",
    "id_test_dataset = id_test_dataset.shuffle(SEED) \n",
    "id_test_dataset = id_test_dataset.slice(idx_start=0, idx_end=1000) \n",
    "id_test_dataset.print_info()\n",
    "# Total number of samples in squad v2.0 validation answerable set: 5928\n",
    "\n",
    "# Load impossible test dataset \n",
    "od_test_dataset = load_od_test_dataset()\n",
    "od_test_dataset = od_test_dataset.shuffle(SEED) \n",
    "od_test_dataset = od_test_dataset.slice(idx_start=0, idx_end=1000) \n",
    "od_test_dataset.print_info()\n",
    "# Total number of samples in squad v2.0 validation unanswerable set: 5945\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom src.inference.inference_utils import analyze_single_generation, build_prompt\\nfrom src.inference.activation_utils import register_forward_activation_hook, extract_token_activations\\n\\n_ = analyze_single_generation(\\n    model=model,\\n    tokenizer=tokenizer,\\n    dataset=od_test_dataset,\\n    sample_idx=500,\\n    build_prompt_fn=build_prompt,\\n    register_forward_activation_hook_fn=register_forward_activation_hook,\\n    layer_idx=-1,\\n    extract_token_activations_fn=partial(extract_token_activations, mode=EXTRACTION_MODE),\\n) \\n'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize one generation with the test impossible dataset\n",
    "# -----------------------------------\n",
    "# TODO: ADAPT TO NEW CODE VERSION\n",
    "'''\n",
    "from src.inference.inference_utils import analyze_single_generation, build_prompt\n",
    "from src.inference.activation_utils import register_forward_activation_hook, extract_token_activations\n",
    "\n",
    "_ = analyze_single_generation(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=od_test_dataset,\n",
    "    sample_idx=500,\n",
    "    build_prompt_fn=build_prompt,\n",
    "    register_forward_activation_hook_fn=register_forward_activation_hook,\n",
    "    layer_idx=-1,\n",
    "    extract_token_activations_fn=partial(extract_token_activations, mode=EXTRACTION_MODE),\n",
    ") \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Retrieve Test descriptors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1. Retrieve Test descriptors from input\n",
    "\n",
    "For this section, `ACTIVATION_SOURCE` is `'prompt'` by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start retrieving test impossible scores from inputs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:06<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...end!\n",
      "Test impossible scores: Time elapsed: 00 min 06 sec\n",
      "\n",
      "\n",
      "Start retrieving test possible scores from inputs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:08<00:00,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...end!\n",
      "Test possible scores: Time elapsed: 00 min 08 sec\n",
      "\n",
      "Directory '../results/raw/TEST/od_test_results_layer18_-1_score_all_hidden_attn_logit_prompt_so0_eo0' does not exist. Nothing to delete.\n",
      "Directory '../results/raw/TEST/id_test_results_layer18_-1_score_all_hidden_attn_logit_prompt_so0_eo0' does not exist. Nothing to delete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Retrieve test descriptors and save results \n",
    "# -----------------------------------\n",
    "from src.inference.run_extraction import run_prompt_descriptor_extraction\n",
    "from src.inference.generation_utils import build_prompt\n",
    "from src.utils.general import print_time_elapsed\n",
    "from src.data_reader.pickle_io import merge_batches_and_cleanup\n",
    "\n",
    "od_output_path = f\"{OUTPUT_DIR}od_test_results{OUTPUT_TITLE}\" \n",
    "id_output_path = f\"{OUTPUT_DIR}id_test_results{OUTPUT_TITLE}\"\n",
    "\n",
    "# Runs batched inference on a dataset using a decoder-only language model.\n",
    "# For each batch, extracts token-level activations, and appends the results to a pickle file.\n",
    "print(\"\\nStart retrieving test impossible descriptors from inputs...\")\n",
    "t2 = time.time()\n",
    "res = run_prompt_descriptor_extraction(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=od_test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    idx_start_sample=0,\n",
    "    max_samples=len(od_test_dataset),\n",
    "    save_to_pkl=False,\n",
    "    output_path=od_output_path,\n",
    "    build_prompt_fn=build_prompt,\n",
    "    layers=LAYERS,  \n",
    "    hidden_agg=HIDDEN_AGG,\n",
    "    attn_agg=ATTN_AGG,\n",
    "    logit_agg=LOGIT_AGG,\n",
    "    logit_config=LOGIT_CONFIG,\n",
    "    start_offset=START_OFFSET,\n",
    "    end_offset=END_OFFSET\n",
    ")\n",
    "t3 = time.time()\n",
    "print(\"...end!\")\n",
    "print_time_elapsed(t2, t3, label=\"Test impossible descriptors: \")\n",
    "\n",
    "print(\"\\nStart retrieving test possible descriptors from inputs...\")\n",
    "t4 = time.time()\n",
    "res = run_prompt_descriptor_extraction(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=id_test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    idx_start_sample=0,\n",
    "    max_samples=len(id_test_dataset),\n",
    "    save_to_pkl=False,\n",
    "    output_path=id_output_path,\n",
    "    build_prompt_fn=build_prompt,\n",
    "    layers=LAYERS,  \n",
    "    hidden_agg=HIDDEN_AGG,\n",
    "    attn_agg=ATTN_AGG,\n",
    "    logit_agg=LOGIT_AGG,\n",
    "    logit_config=LOGIT_CONFIG,\n",
    "    start_offset=START_OFFSET,\n",
    "    end_offset=END_OFFSET\n",
    ")\n",
    "t5 = time.time()\n",
    "print(\"...end!\")\n",
    "print_time_elapsed(t4, t5, label=\"Test possible descriptors: \")\n",
    "\n",
    "# Merge all batches, save as a single file and delete batch directory\n",
    "_= merge_batches_and_cleanup(directory=od_output_path, delete=True, confirm='user') \n",
    "_= merge_batches_and_cleanup(directory=id_output_path, delete=True, confirm='user') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free memory \n",
    "del od_test_dataset \n",
    "del id_test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2. Retrieve Test descriptors from generated answer\n",
    "\n",
    "For this section, `ACTIVATION_SOURCE` can be either `'prompt'`, `'generation'` or `'promptGeneration'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start retrieving test impossible embeddings from generation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...end!\n",
      "Test impossible embeddings: Time elapsed: 00 min 18 sec\n",
      "\n",
      "\n",
      "Start retrieving test possible embeddings from generation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:20<00:00,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...end!\n",
      "Test possible embeddings: Time elapsed: 00 min 20 sec\n",
      "\n",
      "Directory '../results/raw/TEST/od_test_results_layer18_-1_score_all_hidden_attn_logit_prompt_so0_eo0' does not exist. Nothing to delete.\n",
      "Directory '../results/raw/TEST/id_test_results_layer18_-1_score_all_hidden_attn_logit_prompt_so0_eo0' does not exist. Nothing to delete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Retrieve test descriptors and save results \n",
    "# -----------------------------------\n",
    "from src.inference.run_extraction import run_prompt_and_generation_descriptor_extraction\n",
    "from src.inference.generation_utils import build_prompt\n",
    "from src.utils.general import print_time_elapsed\n",
    "from src.data_reader.pickle_io import merge_batches_and_cleanup\n",
    "\n",
    "od_output_path = f\"{OUTPUT_DIR}od_test_results{OUTPUT_TITLE}\" \n",
    "id_output_path = f\"{OUTPUT_DIR}id_test_results{OUTPUT_TITLE}\"\n",
    "\n",
    "# Runs batched inference on a dataset using a decoder-only language model.\n",
    "# For each batch, extracts token-level activations, and appends the results to a pickle file.\n",
    "print(f\"\\nStart retrieving test impossible descriptors from {ACTIVATION_SOURCE}...\")\n",
    "t2 = time.time()\n",
    "res = run_prompt_and_generation_descriptor_extraction(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=od_test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    idx_start_sample=0,\n",
    "    max_samples=len(od_test_dataset),\n",
    "    save_to_pkl=False,\n",
    "    output_path=od_output_path,\n",
    "    build_prompt_fn=build_prompt,\n",
    "    layers=LAYERS,  \n",
    "    activation_source=ACTIVATION_SOURCE,\n",
    "    hidden_agg=HIDDEN_AGG,\n",
    "    attn_agg=ATTN_AGG,\n",
    "    logit_agg=LOGIT_AGG,\n",
    "    logit_config=LOGIT_CONFIG,\n",
    "    start_offset=START_OFFSET,\n",
    "    end_offset=END_OFFSET\n",
    ")\n",
    "t3 = time.time()\n",
    "print(\"...end!\")\n",
    "print_time_elapsed(t2, t3, label=\"Test impossible descriptors: \")\n",
    "\n",
    "\n",
    "print(f\"\\nStart retrieving test possible descriptors from {ACTIVATION_SOURCE}...\")\n",
    "t4 = time.time()\n",
    "res = run_prompt_and_generation_descriptor_extraction(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=id_test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    idx_start_sample=0,\n",
    "    max_samples=len(id_test_dataset),\n",
    "    save_to_pkl=False,\n",
    "    output_path=id_output_path,\n",
    "    build_prompt_fn=build_prompt,\n",
    "    layers=LAYERS,  \n",
    "    activation_source=ACTIVATION_SOURCE,\n",
    "    hidden_agg=HIDDEN_AGG,\n",
    "    attn_agg=ATTN_AGG,\n",
    "    logit_agg=LOGIT_AGG,\n",
    "    logit_config=LOGIT_CONFIG,\n",
    "    start_offset=START_OFFSET,\n",
    "    end_offset=END_OFFSET\n",
    ")\n",
    "t5 = time.time()\n",
    "print(\"...end!\")\n",
    "print_time_elapsed(t4, t5, label=\"Test possible descriptors: \")\n",
    "\n",
    "# Merge all batches, save as a single file and delete batch directory\n",
    "_= merge_batches_and_cleanup(directory=od_output_path, delete=True, confirm='user') \n",
    "_= merge_batches_and_cleanup(directory=id_output_path, delete=True, confirm='user') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free memory \n",
    "del od_test_dataset \n",
    "del id_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "from src.ood_methods.ood_utils import l2_normalize\n",
    "from src.analysis.evaluation import compute_metrics\n",
    "# if you have cuda version 12:\n",
    "# uv pip install faiss-gpu-cu12\n",
    "import faiss \n",
    "\n",
    "\n",
    "def fit_to_dataset(fit_embeddings: torch.tensor) -> faiss.Index:\n",
    "    \"\"\"\n",
    "    Constructs the FAISS index from ID data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fit_embeddings : torch.tensor\n",
    "        ID embeddings, shape (N, D)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    faiss.Index\n",
    "        FAISS index built on the ID embeddings,  ready for k-NN search\n",
    "    \"\"\"\n",
    "    dim = fit_embeddings.shape[1]  # embedding dimension\n",
    "    fit_embeddings = np.array(fit_embeddings).astype(np.float32) # Convert to array \n",
    "    norm_fit_embeddings  = l2_normalize(fit_embeddings) # Normalize embeddings\n",
    "\n",
    "    cpu_index = faiss.IndexFlatL2(dim) # Create a flat L2 index (exact search, not approximate)\n",
    "\n",
    "    # If GPU requested, move index to GPU\n",
    "    use_gpu = torch.cuda.is_available()\n",
    "    if use_gpu:\n",
    "        res = faiss.StandardGpuResources() # Allocate GPU memory\n",
    "        index = faiss.index_cpu_to_gpu(res, 0, cpu_index) # Move index to GPU\n",
    "    else:\n",
    "        index = cpu_index\n",
    "\n",
    "    # Add normalized ID embeddings to index\n",
    "    index.add(norm_fit_embeddings)\n",
    "    return index\n",
    "\n",
    "\n",
    "\n",
    "def score_tensor(\n",
    "    index: faiss.Index,\n",
    "    inputs: torch.tensor,\n",
    "    nearest: int = 50,\n",
    "    batch_size: int = 4\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute DKNN OOD score for test embeddings.\n",
    "\n",
    "    For each input sample, returns the distance to its k-th nearest neighbor\n",
    "    (in the ID set) and the index of this neighbor in the reference set.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    index : faiss.Index\n",
    "        FAISS index built from ID data\n",
    "    inputs : torch.tensor\n",
    "        Array of test embeddings shape (N, D)\n",
    "    nearest : int\n",
    "        Number of nearest neighbors (k)\n",
    "    batch_size : int\n",
    "        Batch size for processing\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Array of distances to the k-th nearest neighbor of shape (N,)\n",
    "    np.ndarray\n",
    "        Array of indices in the ID set for the k-th nearest neighbor, shape (N,).\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert to numpy float32 array\n",
    "    inputs = np.array(inputs, dtype=np.float32)\n",
    "\n",
    "    # Normalize the test embeddings\n",
    "    norm_inputs = l2_normalize(inputs)\n",
    "\n",
    "    # Allocate list to store distances and indices\n",
    "    all_scores = []\n",
    "    all_indices = []\n",
    "\n",
    "    # Process in mini-batches to avoid memory overflow\n",
    "    for i in range(0, norm_inputs.shape[0], batch_size):\n",
    "        batch = norm_inputs[i:i + batch_size]               # Select batch\n",
    "        distances, indices  = index.search(batch, nearest)  # FAISS k-NN search\n",
    "        kth_dist = distances[:, -1]                         # Score = distance to k-th nearest neighbor\n",
    "        kth_idx = indices[:, -1]                            # Index of the k-th nearest neighbor \n",
    "        all_scores.append(kth_dist)                         # Collect scores\n",
    "        all_indices.append(kth_idx)                         # Collect indices\n",
    "    \n",
    "    # Concatenate results from all batches\n",
    "    return np.concatenate(all_scores),  np.concatenate(all_indices)\n",
    "### INDICES DECALES???? FAISS\n",
    "    \n",
    "\n",
    "def compute_dknn_scores(\n",
    "    id_fit_embeddings: torch.Tensor,\n",
    "    id_test_embeddings: torch.Tensor, \n",
    "    od_test_embeddings: torch.Tensor,\n",
    "    k: int = 5,\n",
    "    batch_size: int = 1000\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Compute DKNN (Deep k-Nearest Neighbors) scores for OOD detection.\n",
    "\n",
    "    This function:\n",
    "    - Fits a FAISS index on in-distribution embeddings\n",
    "    - Computes DKNN scores (distance to k-th nearest neighbor) and indices for \n",
    "      both ID and OOD test samples.\n",
    "\n",
    "    The DKNN score represents the distance to the k-th nearest neighbor in the \n",
    "    ID training set. Higher scores indicate samples that are far from the ID \n",
    "    distribution (likely OOD), while lower scores indicate samples close to \n",
    "    the ID distribution.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    id_fit_embeddings : torch.Tensor\n",
    "        In-distribution training embeddings used to fit the DKNN index.\n",
    "        Shape: [n_fit_samples, embedding_dim]\n",
    "    id_test_embeddings : torch.Tensor\n",
    "        In-distribution test embeddings.\n",
    "        Shape: [n_id_test_samples, embedding_dim]\n",
    "    od_test_embeddings : torch.Tensor\n",
    "        Out-of-distribution test embeddings.\n",
    "        Shape: [n_ood_test_samples, embedding_dim]\n",
    "    k : int, optional (default=5)\n",
    "        Number of nearest neighbors used for scoring.\n",
    "    batch_size : int, optional (default=1000)\n",
    "        Batch size for scoring to manage memory.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dknn_scores_id : np.ndarray,  Shape: [n_id_test_samples]\n",
    "        DKNN scores (distance to k-th nearest neighbor) for ID test samples. \n",
    "    dknn_scores_ood : np.ndarray, Shape: [n_ood_test_samples]\n",
    "        DKNN scores for OOD test samples.  \n",
    "    indices_id : np.ndarray, Shape: [n_id_test_samples]\n",
    "        Indices in the ID set of the k-th nearest neighbor for each ID test sample.\n",
    "    indices_ood : np.ndarray, Shape: [n_ood_test_samples]\n",
    "        Indices in the ID set of the k-th nearest neighbor for each OOD test sample.\n",
    "    \"\"\"\n",
    "    index = fit_to_dataset(id_fit_embeddings)\n",
    "\n",
    "    dknn_scores_id, indices_id   = score_tensor(index, id_test_embeddings, nearest=k, batch_size=batch_size)\n",
    "    dknn_scores_ood, indices_ood = score_tensor(index, od_test_embeddings, nearest=k, batch_size=batch_size)\n",
    "\n",
    "    return dknn_scores_id, dknn_scores_ood, indices_id, indices_ood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of ID scores: 0.007939342\n",
      "Mean of OOD scores: 0.3082438\n"
     ]
    }
   ],
   "source": [
    "# ===================================\n",
    "# Perform DKNN\n",
    "#\n",
    "# A high DeepKNN score (distance to k-th NN) => OOD data (far from ID neighbors)\n",
    "# A low DeepKNN score                        => ID data (close to ID neighbors)\n",
    "# ===================================\n",
    "\n",
    "# Define config\n",
    "# -----------------------------------\n",
    "config = {'k': 1, 'batch_size': BATCH_SIZE}\n",
    "\n",
    "# Compute DKNN scores on test data\n",
    "# -----------------------------------\n",
    "scores_id, scores_ood, indices_id, indices_ood = compute_dknn_scores(\n",
    "    id_fit_embeddings=id_fit_embeddings, \n",
    "    id_test_embeddings=id_test_embeddings, \n",
    "    od_test_embeddings=od_test_embeddings, \n",
    "    k=config['k'], \n",
    "    batch_size=config['batch_size']\n",
    ")\n",
    "print(\"Mean of ID scores:\", np.mean(scores_id))\n",
    "print(\"Mean of OOD scores:\", np.mean(scores_ood))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5a5e1d035bc9f4001a75ae34'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_test_data['id'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([57052, 25678, 59808, ..., 32946, 52359, 60561])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_question_by_id(dataset, target_id):\n",
    "    \"\"\"\n",
    "    Recherche la question correspondant à un id donné dans une liste de dicts SQuAD-like.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : list of dict\n",
    "        Liste des exemples, chaque élément doit avoir une clé 'id'\n",
    "    target_id : str\n",
    "        Identifiant recherché\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    question : str or None\n",
    "        La question correspondante si trouvée, sinon None\n",
    "    \"\"\"\n",
    "    for sample in dataset:\n",
    "        if sample.get('id') == target_id:\n",
    "            return f\"Passage: {sample.get('context')}\\nQuestion: {sample.get('question')}\"\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5a5e1d035bc9f4001a75ae34\n",
      "==== id_test_data: ====\n",
      "Passage: Scholars have debated the relationship and differences within āstika philosophies and with nāstika philosophies, starting with the writings of Indologists and Orientalists of the 18th and 19th centuries, which were themselves derived from limited availability of Indian literature and medieval doxographies. The various sibling traditions included in Hindu philosophies are diverse, and they are united by shared history and concepts, same textual resources, similar ontological and soteriological focus, and cosmology. While Buddhism and Jainism are considered distinct philosophies and religions, some heterodox traditions such as Cārvāka are often considered as distinct schools within Hindu philosophy.\n",
      "Question: What are the two sibling traditions?\n",
      "==== most similar id_fit_data: ====\n",
      "Passage: The world's first Institute of Technology the Berg-Schola (Bergschule) established in Selmecbánya, Kingdom of Hungary by the Court Chamber of Vienna in 1735 providing Further education to train specialists of precious metal and copper mining. In 1762 the institute ranked up to be Academia providing Higher Education courses. After the Treaty of Trianon the institute had to be moved to Sopron.\n",
      "Question: Who established the Berg-Schola?\n",
      "\n",
      "\n",
      "5ace1c4a32bba1001ae49af5\n",
      "==== id_test_data: ====\n",
      "Passage: Since the university's establishment in the city in 1837, the histories of the University of Michigan and Ann Arbor have been closely linked. The town became a regional transportation hub in 1839 with the arrival of the Michigan Central Railroad, and a north—south railway connecting Ann Arbor to Toledo and other markets to the south was established in 1878. Throughout the 1840s and the 1850s settlers continued to come to Ann Arbor. While the earlier settlers were primarily of British ancestry, the newer settlers also consisted of Germans, Irish, and African-Americans. In 1851, Ann Arbor was chartered as a city, though the city showed a drop in population during the Depression of 1873. It was not until the early 1880s that Ann Arbor again saw robust growth, with new immigrants coming from Greece, Italy, Russia, and Poland. Ann Arbor saw increased growth in manufacturing, particularly in milling. Ann Arbor's Jewish community also grew after the turn of the 20th century, and its first and oldest synagogue, Beth Israel Congregation, was established in 1916.\n",
      "Question: What railroad arrived in 1893?\n",
      "==== most similar id_fit_data: ====\n",
      "Passage: NigComSat-1, a Nigerian satellite built in 2004, was Nigeria's third satellite and Africa's first communication satellite. It was launched on 13 May 2007, aboard a Chinese Long March 3B carrier rocket, from the Xichang Satellite Launch Centre in China. The spacecraft was operated by NigComSat and the Nigerian Space Agency, NASRDA. On 11 November 2008, NigComSat-1 failed in orbit after running out of power because of an anomaly in its solar array. It was based on the Chinese DFH-4 satellite bus, and carries a variety of transponders: 4 C-band; 14 Ku-band; 8 Ka-band; and 2 L-band. It was designed to provide coverage to many parts of Africa, and the Ka-band transponders would also cover Italy.\n",
      "Question: where was Nigeria's third satellite launched?\n",
      "\n",
      "\n",
      "5a8793d01d3cee001a6a126b\n",
      "==== id_test_data: ====\n",
      "Passage: The quality of a vacuum is indicated by the amount of matter remaining in the system, so that a high quality vacuum is one with very little matter left in it. Vacuum is primarily measured by its absolute pressure, but a complete characterization requires further parameters, such as temperature and chemical composition. One of the most important parameters is the mean free path (MFP) of residual gases, which indicates the average distance that molecules will travel between collisions with each other. As the gas density decreases, the MFP increases, and when the MFP is longer than the chamber, pump, spacecraft, or other objects present, the continuum assumptions of fluid mechanics do not apply. This vacuum state is called high vacuum, and the study of fluid flows in this regime is called particle gas dynamics. The MFP of air at atmospheric pressure is very short, 70 nm, but at 100 mPa (~6997100000000000000♠1×10−3 Torr) the MFP of room temperature air is roughly 100 mm, which is on the order of everyday objects such as vacuum tubes. The Crookes radiometer turns when the MFP is larger than the size of the vanes.\n",
      "Question: How much matter is left in residual gases?\n",
      "==== most similar id_fit_data: ====\n",
      "Passage: Nearly all beer includes barley malt as the majority of the starch. This is because its fibrous hull remains attached to the grain during threshing. After malting, barley is milled, which finally removes the hull, breaking it into large pieces. These pieces remain with the grain during the mash, and act as a filter bed during lautering, when sweet wort is separated from insoluble grain material. Other malted and unmalted grains (including wheat, rice, oats, and rye, and less frequently, corn and sorghum) may be used. Some brewers have produced gluten-free beer, made with sorghum with no barley malt, for those who cannot consume gluten-containing grains like wheat, barley, and rye.\n",
      "Question: What takes the place of barley malt in gluten-free beer?\n",
      "\n",
      "\n",
      "5a139e17c8eab200188dc9dd\n",
      "==== id_test_data: ====\n",
      "Passage: Until the Church Building Act of 1818, the period saw relatively few churches built in Britain, which was already well-supplied, although in the later years of the period the demand for Non-conformist and Roman Catholic places of worship greatly increased. Anglican churches that were built were designed internally to allow maximum audibility, and visibility, for preaching, so the main nave was generally wider and shorter than in medieval plans, and often there were no side-aisles. Galleries were common in new churches. Especially in country parishes, the external appearance generally retained the familiar signifiers of a Gothic church, with a tower or spire, a large west front with one or more doors, and very large windows along the nave, but all with any ornament drawn from the classical vocabulary. Where funds permitted, a classical temple portico with columns and a pediment might be used at the west front. Decoration inside was very limited, but churches filled up with monuments to the prosperous.\n",
      "Question: What act slowed the building of churches in Britain?\n",
      "==== most similar id_fit_data: ====\n",
      "Passage: Treaties formed an important part of European colonization and, in many parts of the world, Europeans attempted to legitimize their sovereignty by signing treaties with indigenous peoples. In most cases these treaties were in extremely disadvantageous terms to the native people, who often did not appreciate the implications of what they were signing.\n",
      "Question: With whom did Europeans attempt to sign treaties in order to legitimize their sovereignty during colonization?\n",
      "\n",
      "\n",
      "5ad174b5645df0001a2d1c8e\n",
      "==== id_test_data: ====\n",
      "Passage: World War II holds a special place in the American psyche as the country's greatest triumph, and the U.S. military personnel of World War II are frequently referred to as \"the Greatest Generation.\" Over 16 million served (about 11% of the population), and over 400,000 died during the war. The U.S. emerged as one of the two undisputed superpowers along with the Soviet Union, and unlike the Soviet Union, the U.S. homeland was virtually untouched by the ravages of war. During and following World War II, the United States and Britain developed an increasingly strong defense and intelligence relationship. Manifestations of this include extensive basing of U.S. forces in the UK, shared intelligence, shared military technology (e.g. nuclear technology), and shared procurement.\n",
      "Question:  How many Americans served in the military in World War I?\n",
      "==== most similar id_fit_data: ====\n",
      "Passage: Burke claimed that Bolingbroke's arguments against revealed religion could apply to all social and civil institutions as well. Lord Chesterfield and Bishop Warburton (and others) initially thought that the work was genuinely by Bolingbroke rather than a satire. All the reviews of the work were positive, with critics especially appreciative of Burke's quality of writing. Some reviewers failed to notice the ironic nature of the book, which led to Burke stating in the preface to the second edition (1757) that it was a satire.\n",
      "Question: Which lord didn't realize the satirical nature of Burke's book?\n",
      "\n",
      "\n",
      "5a84d2627cf838001a46aac6\n",
      "==== id_test_data: ====\n",
      "Passage: Chickens are medium-sized, chunky birds with an upright stance and characterised by fleshy red combs and wattles on their heads. Males, known as cocks, are usually larger, more boldly coloured, and have more exaggerated plumage than females (hens). Chickens are gregarious, omnivorous, ground-dwelling birds that in their natural surroundings search among the leaf litter for seeds, invertebrates, and other small animals. They seldom fly except as a result of perceived danger, preferring to run into the undergrowth if approached. Today's domestic chicken (Gallus gallus domesticus) is mainly descended from the wild red junglefowl of Asia, with some additional input from grey junglefowl. Domestication is believed to have taken place between 7,000 and 10,000 years ago, and what are thought to be fossilized chicken bones have been found in northeastern China dated to around 5,400 BC. Archaeologists believe domestication was originally for the purpose of cockfighting, the male bird being a doughty fighter. By 4,000 years ago, chickens seem to have reached the Indus Valley and 250 years later, they arrived in Egypt. They were still used for fighting and were regarded as symbols of fertility. The Romans used them in divination, and the Egyptians made a breakthrough when they learned the difficult technique of artificial incubation. Since then, the keeping of chickens has spread around the world for the production of food with the domestic fowl being a valuable source of both eggs and meat.\n",
      "Question: What is thought to have taken place 1,000 to 17,000 years ago?\n",
      "==== most similar id_fit_data: ====\n",
      "Passage: In July 2007, the National Archives announced it would make its collection of Universal Newsreels from 1929 to 1967 available for purchase through CreateSpace, an Amazon.com subsidiary. During the announcement, Weinstein noted that the agreement would \"... reap major benefits for the public-at-large and for the National Archives.\" Adding, \"While the public can come to our College Park, MD research room to view films and even copy them at no charge, this new program will make our holdings much more accessible to millions of people who cannot travel to the Washington, DC area.\" The agreement also calls for CreateSpace partnership to provide the National Archives with digital reference and preservation copies of the films as part of NARA's preservation program.\n",
      "Question: What company did the National Archives partner with make it's Universal Newsreels available online?\n",
      "\n",
      "\n",
      "5a10f49f06e79900185c348b\n",
      "==== id_test_data: ====\n",
      "Passage: The franchise was first created in 1997 as a series of virtual pets, akin to—and influenced in style by—the contemporary Tamagotchi or nano Giga Pet toys. The creatures were first designed to look cute and iconic even on the devices' small screens; later developments had them created with a harder-edged style influenced by American comics. The franchise gained momentum with its first anime incarnation, Digimon Adventure, and an early video game, Digimon World, both released in 1999. Several seasons of the anime and films based on them have aired, and the video game series has expanded into genres such as role-playing, racing, fighting, and MMORPGs. Other media forms have also been released.\n",
      "Question: What was released in 1990\n",
      "==== most similar id_fit_data: ====\n",
      "Passage: Local law enforcement is divided between County Sheriff's Offices and Municipal Police Departments. Tennessee's Constitution requires that each County have an elected Sheriff. In 94 of the 95 counties the Sheriff is the chief law enforcement officer in the county and has jurisdiction over the county as a whole. Each Sheriff's Office is responsible for warrant service, court security, jail operations and primary law enforcement in the unincorporated areas of a county as well as providing support to the municipal police departments. Incorporated municipalities are required to maintain a police department to provide police services within their corporate limits.\n",
      "Question: What proportion of Tennessee counties recognize their Sheriff as their head law enforcement official?\n",
      "\n",
      "\n",
      "5ad394a4604f3c001a3fe716\n",
      "==== id_test_data: ====\n",
      "Passage: The Miami area has a unique dialect, (commonly called the \"Miami accent\") which is widely spoken. The dialect developed among second- or third-generation Hispanics, including Cuban-Americans, whose first language was English (though some non-Hispanic white, black, and other races who were born and raised the Miami area tend to adopt it as well.) It is based on a fairly standard American accent but with some changes very similar to dialects in the Mid-Atlantic (especially the New York area dialect, Northern New Jersey English, and New York Latino English.) Unlike Virginia Piedmont, Coastal Southern American, and Northeast American dialects and Florida Cracker dialect (see section below), \"Miami accent\" is rhotic; it also incorporates a rhythm and pronunciation heavily influenced by Spanish (wherein rhythm is syllable-timed). However, this is a native dialect of English, not learner English or interlanguage; it is possible to differentiate this variety from an interlanguage spoken by second-language speakers in that \"Miami accent\" does not generally display the following features: there is no addition of /ɛ/ before initial consonant clusters with /s/, speakers do not confuse of /dʒ/ with /j/, (e.g., Yale with jail), and /r/ and /rr/ are pronounced as alveolar approximant [ɹ] instead of alveolar tap [ɾ] or alveolar trill [r] in Spanish.\n",
      "Question: What English language influences the Miami dialect?\n",
      "==== most similar id_fit_data: ====\n",
      "Passage: The Sun had the largest circulation of any daily newspaper in the United Kingdom, but in late 2013 slipped to second largest Saturday newspaper behind the Daily Mail. It had an average daily circulation of 2.2 million copies in March 2014. Between July and December 2013 the paper had an average daily readership of approximately 5.5 million, with approximately 31% of those falling into the ABC1 demographic and 68% in the C2DE demographic. Approximately 41% of readers are women. The Sun has been involved in many controversies in its history, including its coverage of the 1989 Hillsborough football stadium disaster. Regional editions of the newspaper for Scotland, Northern Ireland and the Republic of Ireland are published in Glasgow (The Scottish Sun), Belfast (The Sun) and Dublin (The Irish Sun) respectively.\n",
      "Question: What was the sun's circulation per day in March 2014?\n",
      "\n",
      "\n",
      "5a7c9a3de8bc7e001a9e1eef\n",
      "==== id_test_data: ====\n",
      "Passage: In addition, Hegel does believe we can know the structure of God's mind, or ultimate reality. Hegel agrees with Kierkegaard that both reality and humans are incomplete, inasmuch as we are in time, and reality develops through time. But the relation between time and eternity is outside time and this is the \"logical structure\" that Hegel thinks we can know. Kierkegaard disputes this assertion, because it eliminates the clear distinction between ontology and epistemology. Existence and thought are not identical and one cannot possibly think existence. Thought is always a form of abstraction, and thus not only is pure existence impossible to think, but all forms in existence are unthinkable; thought depends on language, which merely abstracts from experience, thus separating us from lived experience and the living essence of all beings. In addition, because we are finite beings, we cannot possibly know or understand anything that is universal or infinite such as God, so we cannot know God exists, since that which transcends time simultaneously transcends human understanding.\n",
      "Question: Kierkegaard eliminated the distinction between what two studies?\n",
      "==== most similar id_fit_data: ====\n",
      "Passage: The national economy grew significantly through agrarian reform, major modernization projects such as the Helwan steel works and the Aswan Dam, and nationalization schemes such as that of the Suez Canal. However, the marked economic growth of the early 1960s took a downturn for the remainder of the decade, only recovering in 1970. Egypt experienced a \"golden age\" of culture during Nasser's presidency, according to historian Joel Gordon, particularly in film, television, theater, radio, literature, fine arts, comedy, poetry, and music. Egypt under Nasser dominated the Arab world in these fields, producing cultural icons.\n",
      "Question: How was Egyptian culture under Nasser refered to?\n",
      "\n",
      "\n",
      "5ad0d39d645df0001a2d05a7\n",
      "==== id_test_data: ====\n",
      "Passage: Burke knew that many members of the Whig Party did not share Fox's views and he wanted to provoke them into condemning the French Revolution. Burke wrote that he wanted to represent the whole Whig party \"as tolerating, and by a toleration, countenancing those proceedings\" so that he could \"stimulate them to a public declaration of what every one of their acquaintance privately knows to be...their sentiments\". Therefore, on 3 August 1791 Burke published his Appeal from the New to the Old Whigs, in which he renewed his criticism of the radical revolutionary programmes inspired by the French Revolution and attacked the Whigs who supported them, as holding principles contrary to those traditionally held by the Whig party.\n",
      "Question: What pamphlet did Fox publish in 3 August 1791?\n",
      "==== most similar id_fit_data: ====\n",
      "Passage: There are also many large plants under construction. The Desert Sunlight Solar Farm under construction in Riverside County, California and Topaz Solar Farm being built in San Luis Obispo County, California are both 550 MW solar parks that will use thin-film solar photovoltaic modules made by First Solar. The Blythe Solar Power Project is a 500 MW photovoltaic station under construction in Riverside County, California. The California Valley Solar Ranch (CVSR) is a 250 megawatt (MW) solar photovoltaic power plant, which is being built by SunPower in the Carrizo Plain, northeast of California Valley. The 230 MW Antelope Valley Solar Ranch is a First Solar photovoltaic project which is under construction in the Antelope Valley area of the Western Mojave Desert, and due to be completed in 2013. The Mesquite Solar project is a photovoltaic solar power plant being built in Arlington, Maricopa County, Arizona, owned by Sempra Generation. Phase 1 will have a nameplate capacity of 150 megawatts.\n",
      "Question: Where is Desert Sunlight Solar Farm?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    qid = id_test_data['id'][i]\n",
    "    print(qid)\n",
    "    print(\"==== id_test_data: ====\")\n",
    "    res = find_question_by_id(od_test_dataset, qid) #od_test_dataset = id data in fact, \n",
    "    print(res)\n",
    "\n",
    "    print(\"==== most similar id_fit_data: ====\")\n",
    "    context  = id_fit_dataset['context'][indices_id[i]+1]\n",
    "    question = id_fit_dataset['question'][indices_id[i]+1]\n",
    "    print(f\"Passage: {context}\\nQuestion: {question}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "squad v2: train (answerable->ID fit, unanswerable) | val (answerable->ID test, unanswerable->OOD test)     => no theme overlap between train|val. Mais: les contextes (paragraphes Wikipédia) utilisés pour générer les questions unanswerable de SQuAD v2.0 sont les mêmes que ceux des questions answerable de SQuAD v1.1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ood_env",
   "language": "python",
   "name": "ood_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
