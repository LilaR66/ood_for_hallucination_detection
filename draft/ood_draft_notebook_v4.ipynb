{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OOD detection applied to Hallucination Detection\n",
    "\n",
    " The goal is to predict if an INPUT prompt  is going to produce an hallucination or not (using OOD detection methods). For now, we don’t look at the output generated by the model, we may consider this in a second time. Retrieve ID samples:  To do this, take a general (easy) QA dataset containing questions along with their true hallucination-free answers. Feed the questions to the model. Let the model generate responses and check if the a given generated response is the same as the real hallucination-free answer. All the correct generated responses will be considered ID. More precisely, the ID dataset will consist of the embeddings of the last token of the last layer of the input (or maybe middle layer) of the correct generated responses.  Test a new sample to see if this is going to be OOD=hallucination: Take another dataset containing questions susceptible to trigger hallucinations along with the true hallucination-free answers (or no answer if the model cannot know the answer by any way and all response that the model might produce will necessarily be hallucinated). Feed a question to the model and let it generate a response. Check by comparing to the hallucination-free answer is that generated response is hallucinated or not. At the same time, apply an OOD detection method on the input question (at the last token last layer) and see if there is a correspondence between a high OOD score and a generated hallucination. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#/home/lila.roig/.env/ood_env/bin/python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Embedding Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "# -----------------------------------\n",
    "import torch\n",
    "import sys\n",
    "import time \n",
    "import os \n",
    "import pickle\n",
    "from functools import partial\n",
    "# Add the path to the src directory\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define global variables\n",
    "# -----------------------------------\n",
    "SEED = 777 #44\n",
    "BATCH_SIZE = 16 #32\n",
    "MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "OUTPUT_DIR = \"../results/raw/TEST/\" \n",
    "PLOT_DIR   = \"../results/figures/\" \n",
    "LAYER = -1      # (integer) - Layer from witch retrieve the embeddings \n",
    "TOKENS = \"-1\"  # (string) - How to retrieve the embeddings \n",
    "K_BEAMS = 1 #3\n",
    "ACTIVATION_SOURCE = \"generation\" # can be 'generation', 'PromptGeneration'\n",
    " \n",
    "if TOKENS==\"0\":\n",
    "    EXTRACTION_MODE = \"first_generated\"\n",
    "elif TOKENS==\"-1\":\n",
    "    EXTRACTION_MODE = \"last\"\n",
    "elif TOKENS==\"Avg\":\n",
    "    EXTRACTION_MODE = \"average\"\n",
    "elif TOKENS==\"Max\":\n",
    "    EXTRACTION_MODE = \"max\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear memory to avoid \"CUDA out of memory\"\n",
    "# -----------------------------------\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.13 (main, Jun  4 2025, 08:57:30) [GCC 13.3.0]\n",
      "Cuda version: 12.6\n",
      "Number of available de GPU : 2\n",
      "GPU 1 : NVIDIA L40S\n",
      "GPU 2 : NVIDIA L40S\n"
     ]
    }
   ],
   "source": [
    "# Visualize setup \n",
    "# -----------------------------------\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Cuda version: {torch.version.cuda}\")\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Number of available de GPU : {num_gpus}\")\n",
    "for i in range(num_gpus):\n",
    "    print(f\"GPU {i + 1} : {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed everything\n",
    "# -----------------------------------\n",
    "from src.utils.general import seed_all\n",
    "seed_all(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb7adaf2738e40278ce7d3210a52ec90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model\n",
    "# ----------------------------------\n",
    "from src.model_loader.llama_loader import load_llama\n",
    "\n",
    "model, tokenizer = load_llama(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8008 samples from: /home/lila.roig/projet_ood/ood_for_hallucination_detection/results/raw/small_dataset_correct_split_allScores/id_fit_results_layer1:32:2_score_all_hidden_attn_logit_prompt_so0_eo0.pkl\n"
     ]
    }
   ],
   "source": [
    "from src.data_reader.pickle_io import load_pickle_batches\n",
    "res = load_pickle_batches(\"/home/lila.roig/projet_ood/ood_for_hallucination_detection/results/raw/small_dataset_correct_split_allScores/id_fit_results_layer1:32:2_score_all_hidden_attn_logit_prompt_so0_eo0.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sdpa\n",
      "sdpa\n"
     ]
    }
   ],
   "source": [
    "print(model.config._attn_implementation )\n",
    "#model.config._attn_implementation = 'eager'\n",
    "print(model.config._attn_implementation )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load ID dataset\n",
    "\n",
    "For the ID general dataset, we are going to use the SQUAD 1.1 dataset: \n",
    "\n",
    "***SQuAD 1.1:** Comprises over 100,000 question-answer pairs derived from more than 500 Wikipedia articles. Each question is paired with a specific segment of text (a span) from the corresponding article that serves as the answer.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Dataset Information =====\n",
      "Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers', 'original_index', 'is_impossible'],\n",
      "    num_rows: 1000\n",
      "})\n",
      "Mean ground-truth answer length: 2.25, Max length: 14\n",
      "Mean context + question length: 146.50, Max length: 342\n"
     ]
    }
   ],
   "source": [
    "# Load ID dataset\n",
    "# -----------------------------------\n",
    "from src.data_reader.squad_loader import load_id_fit_dataset \n",
    "# Total number of samples in squad v1.1: 87599, squad v2.0: 86821\n",
    "\n",
    "id_fit_dataset = load_id_fit_dataset()\n",
    "#id_fit_dataset = id_fit_dataset.shuffle(SEED) \n",
    "id_fit_dataset = id_fit_dataset.slice(idx_start=0, idx_end=1_000) # 10_000\n",
    "id_fit_dataset.print_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development of new solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify extrat_token_activations to add features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizer, PreTrainedModel\n",
    "import torch\n",
    "from torch.utils.hooks import RemovableHandle\n",
    "from typing import Tuple, Literal, List, Optional, Dict\n",
    "\n",
    "### OK ###\n",
    "def extract_token_activations(\n",
    "    selected_layer: torch.Tensor,\n",
    "    attention_mask: torch.Tensor,\n",
    "    device: torch.device,\n",
    "    modes: List[Literal[\n",
    "        \"average\", \"last\", \"max\", \"first_generated\", \n",
    "        \"token_svd_score\", \"feat_var\"\n",
    "    ]] = [\"average\"],\n",
    "    skip_length: Optional[int] = None,\n",
    "    alpha: int = 0.001,\n",
    ") -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"   \n",
    "    Aggregate token-level activations over a specified span for each sequence in a batch,\n",
    "    using various aggregation modes and attention mask.\n",
    "\n",
    "    This function takes as input:\n",
    "      - The layer activations (selected_layer) for each token in a batch of sequences,\n",
    "      - An attention mask (attention_mask) of the same shape, where 1 indicates tokens to include\n",
    "        in the aggregation and 0 marks tokens to ignore.\n",
    "\n",
    "    The attention mask may be the original model mask, or a custom mask generated using\n",
    "    `compute_offset_attention_mask` to dynamically select a sub-span of tokens.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    selected_layer : torch.Tensor\n",
    "        Tensor of shape (batch_size, seq_len, hidden_size) containing model activations for each token.\n",
    "    attention_mask : torch.Tensor\n",
    "        Attention mask of shape (batch_size, seq_len),  1 for real tokens, 0 for padding.\n",
    "    device : torch.device\n",
    "        Device for computation.\n",
    "\n",
    "    modes : List[str]\n",
    "        List of aggregation modes to compute. Computed using only valid tokens where attention_mask == 1.\n",
    "        Supported:\n",
    "        - \"average\": Mean activation vector across valid tokens. Shape: (batch_size, hidden_size)\n",
    "        - \"max\": Element-wise max activation across valid tokens. Shape: (batch_size, hidden_size)\n",
    "        - \"last\": Activation vector of last valid token in each sequence. Shape: (batch_size, hidden_size)\n",
    "        - \"first_generated\": Activation of the first generated valid token in each sequence. Shape: (batch_size, hidden_size)\n",
    "             If skip_length is provided, selects the token starting from that offset. \n",
    "        - \"token_svd_score\": Mean log singular value of the centered Gram matrix over tokens. Shape (batch_size,)\n",
    "            The Gram matrix is computed as Gram_token = Z·J·Z^T, where J is the centering matrix on features.\n",
    "            It quantifies the pairwise similarity between token representations after removing the mean value \n",
    "            of each feature across tokens. Note: This is not a classical covariance matrix.\n",
    "            The log singular values quantifies the effective dimensionality or diversity of the token \n",
    "            activations: higher values reflect more diverse (less redundant) token representations, lower values \n",
    "            indicate more redundancy or alignment.\n",
    "            NOTE: Implementation inpired by \"LLM-Check: Investigating Detection of Hallucinations in \n",
    "            Large Language Models\" (Sriramanan et al. 2024)\n",
    "        - \"feat_var\": Diagonal of the centered feature covariance matrix (variances). Shape: (batch_size, hidden_size)\n",
    "\n",
    "    skip_length : Optional[int]\n",
    "        If provided, used to explicitly select the first generated token (useful for \"first_generated\" mode).\n",
    "    alpha : float\n",
    "        Regularization parameter added to the covariance matrix.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, torch.Tensor or np.ndarray]\n",
    "        Dictionary mapping each mode to its result:\n",
    "            - (batch_size, hidden_size) for \"average\", \"max\", \"last\", \"first_generated\", \"feat_var\"\n",
    "            - (batch_size,) for \"token_svd_score\"\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size, seq_len, hidden_size = selected_layer.shape\n",
    "    aggregated_tokens = {}\n",
    "    \n",
    "    # Move to device \n",
    "    attention_mask = attention_mask.to(selected_layer.device)\n",
    "\n",
    "    # =======================================\n",
    "    # Select the first token with optional offset `skip_length`\n",
    "    # =======================================\n",
    "    if \"first_generated\" in modes:\n",
    "        batch_indices = torch.arange(batch_size, device=device)\n",
    "        if skip_length is not None:\n",
    "            first_indices = torch.full((batch_size,), skip_length, device=device, dtype=torch.long)\n",
    "        else:\n",
    "            first_indices = (attention_mask == 1).float().argmax(dim=1)\n",
    "        first = selected_layer[batch_indices, first_indices] # Shape: (batch_size, hidden_size)\n",
    "        aggregated_tokens[\"first_generated\"] = first\n",
    "\n",
    "    # =======================================\n",
    "    # Select the last token \n",
    "    # =======================================\n",
    "    if \"last\" in modes:\n",
    "        last_indices = attention_mask.shape[1] - 1 - attention_mask.flip(dims=[1]).float().argmax(dim=1)\n",
    "        batch_indices = torch.arange(batch_size, device=device)\n",
    "        last = selected_layer[batch_indices, last_indices]  # Shape: (batch_size, hidden_size)\n",
    "        aggregated_tokens[\"last\"] = last\n",
    "\n",
    "    # =======================================\n",
    "    # Apply mask and compute aggregation \n",
    "    # =======================================\n",
    "    if \"average\" in modes or \"max\" in modes:\n",
    "        # Add one dimension for the broadcast on hidden_size\n",
    "        mask_float = attention_mask.float().unsqueeze(-1)  # (batch_size, num_valid_tokens, 1)\n",
    "        # Apply the mask to the activations: zero out tokens outside the target interval\n",
    "        masked = selected_layer * mask_float\n",
    "        #  Count the number of selected tokens for each sequence (avoid division by zero with clamp)\n",
    "        counts = mask_float.sum(dim=1).clamp(min=1e-6)\n",
    "        if \"average\" in modes:\n",
    "            # Compute the mean activation vector for each sequence over the selected interval\n",
    "            avg = masked.sum(dim=1) / counts # Shape: (batch_size, hidden_size)\n",
    "            aggregated_tokens[\"average\"] = avg\n",
    "        if \"max\" in modes:\n",
    "            # Replace padding with -inf to exclude from max calculation\n",
    "            masked_max = masked.masked_fill(mask_float.logical_not(), float('-inf'))\n",
    "            # Extract maximum values across sequence dimension\n",
    "            max_vals, _ = masked_max.max(dim=1) # Shape: (batch_size, hidden_size)\n",
    "            aggregated_tokens[\"max\"] = max_vals\n",
    "\n",
    "    # =======================================\n",
    "    # Covariance-based metrics\n",
    "    # =======================================\n",
    "    if any(m in modes for m in [\"token_svd_score\", \"feat_var\"]):\n",
    "        token_svd_score = [] \n",
    "        feat_var = []\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            # Select valid tokens \n",
    "            mask = attention_mask[i].bool()\n",
    "            Z = selected_layer[i][mask]  # (num_valid_tokens, hidden_size)\n",
    "            \n",
    "            if Z.shape[0] == 0:\n",
    "                feat_var.append(torch.full((hidden_size,), float('nan')))\n",
    "                token_svd_score.append(float('nan'))\n",
    "                continue\n",
    "            \n",
    "            if Z.dtype != torch.float32:\n",
    "                Z = Z.to(torch.float32)\n",
    "\n",
    "            if \"token_svd_score\" in modes:\n",
    "                # Compute Gram matrix on tokens : Gram_token = Z·J·Z^T\n",
    "                # ---------------------------------------\n",
    "                # Assumes Z is in full precision\n",
    "                # Center the features of Z (i.e., subtract the mean value of each feature across tokens)\n",
    "                J = torch.eye(hidden_size, device=Z.device, dtype=Z.dtype) - (1 / hidden_size) * torch.ones(hidden_size, hidden_size, device=Z.device, dtype=Z.dtype)\n",
    "                # The Gram matrix Gram_token reflects the inner products (similarities) between tokens\n",
    "                Gram_token = torch.matmul(torch.matmul(Z, J), Z.t()) # (num_valid_tokens, num_valid_tokens)\n",
    "                # Regularization for stabilization\n",
    "                Gram_token = Gram_token + alpha * torch.eye(Gram_token.shape[0], device=Z.device, dtype=Z.dtype)\n",
    "            \n",
    "                # Singular value decomposition (SVD) of the token Gram matrix\n",
    "                # ---------------------------------------\n",
    "                if Gram_token.dtype != torch.float32:\n",
    "                    Gram_token = Gram_token.to(torch.float32)\n",
    "                token_svdvals = torch.linalg.svdvals(Gram_token) # Singular Value Decomposition\n",
    "                token_eigscore = torch.log(token_svdvals).mean()  # mult by 2 missing from the paper? \n",
    "                token_svd_score.append(token_eigscore)\n",
    "\n",
    "            if \"feat_var\" in modes:\n",
    "                # Compute covariance matrix on features \n",
    "                # ---------------------------------------\n",
    "                Z_feat_centered = Z - Z.mean(dim=0, keepdim=True) # (num_valid_tokens, hidden_size)\n",
    "                Cov_feat = (Z_feat_centered.t() @ Z_feat_centered) / max(1, Z.shape[0] - 1) # (hidden_size, idden_size)\n",
    "                Cov_feat += alpha * torch.eye(Z.shape[1], device=Z.device, dtype=Z.dtype)\n",
    "                feat_var.append(Cov_feat.diag())\n",
    "            \n",
    "        # Return scores\n",
    "        # ---------------------------------------\n",
    "        if \"feat_var\" in modes:\n",
    "            aggregated_tokens[\"feat_var\"] = torch.stack(feat_var, dim=0) # (batch_size, hidden_size) \n",
    "        if \"token_svd_score\" in modes:\n",
    "            aggregated_tokens[\"token_svd_score\"] = torch.stack(token_svd_score) # (batch_size,) \n",
    "        \n",
    "        # Put everything on CPU\n",
    "        # ---------------------------------------\n",
    "        for key in aggregated_tokens:\n",
    "            aggregated_tokens[key] = aggregated_tokens[key].detach().cpu()\n",
    "\n",
    "    return aggregated_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import List, Literal\n",
    "\n",
    "### OK ### \n",
    "def compute_attn_eig_prod(\n",
    "    prompt_attentions: torch.Tensor,\n",
    "    generation_attentions: List[torch.Tensor],\n",
    "    prompt_attention_mask: torch.Tensor,\n",
    "    generation_attention_mask: torch.Tensor,\n",
    "    mode: Literal[\"prompt\", \"generation\", \"promptGeneration\"] = \"prompt\"\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute a mean log-diagonal attention score (eigenvalue-inspired) for a single layer's \n",
    "    attention map, using attention mask. \n",
    "    \n",
    "    NOTE: Implementation inspired by \n",
    "    \"LLM-Check: Investigating Detection of Hallucinations in Large Language Models\"\n",
    "    (Sriramanan et al. 2024)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    prompt_attentions: torch.Tensor\n",
    "        Tensor of shape (batch_size, n_heads, prompt_len, prompt_len)\n",
    "        Self-attention over the prompt tokens. \n",
    "    generation_attentions: list of torch.Tensor\n",
    "        List of tensors of shape (batch_size, n_heads, 1, prompt_len + t)\n",
    "        Self-attention for each generated token at generation step t (t >= 1).\n",
    "    prompt_attention_mask: torch.Tensor\n",
    "        Tensor of shape (batch_size, prompt_len), 1 where token valid, 0 for padding.\n",
    "    generation_attention_mask: torch.Tensor  \n",
    "        Tensor of shape (batch_size, gen_len), 1 where token valid, 0 for padding.\n",
    "    mode : str, optional\n",
    "        Specifies which part of the attention map to use for the score computation.\n",
    "        Must be one of the following:\n",
    "        - \"prompt\":\n",
    "            Only uses the prompt self-attention map (prompt_attentions). \n",
    "            It is a matrix of shape (batch_size, n_heads, prompt_len, prompt_len).\n",
    "            The diagonal (i.e., self-attention values per token) is extracted,\n",
    "            then the log is taken, followed by a mean over prompt tokens and sum over heads.\n",
    "        - \"generation\":\n",
    "            Only uses the generated self-attention maps (generation_attentions).\n",
    "            Each tensor in generation_attentions has shape (batch_size, n_heads, 1, prompt_len + t),\n",
    "            where t is the generation step. \n",
    "            Instead of concatenating these tensors to obtain the generation attention matrix, \n",
    "            for each step, we directly take the last value along the last axis (i.e., the self-attention\n",
    "            of the newly generated token). These values are stacked across time steps, then we take the log,\n",
    "            compute the mean over time, and sum over heads.\n",
    "        - \"promptGeneration\":\n",
    "            Combines the diagonals from both the prompt and generation attention maps as described above\n",
    "            for \"prompt\" and \"generation\" mode. The two diagonals are concatenated along the token/time axis, \n",
    "            then the log is taken, followed by a mean across all tokens and a sum over heads.\n",
    "            Note: we do **not** concatenate the full prompt and generation attention matrices,\n",
    "            since the diagonal of the combined matrix would only include values from the prompt attention\n",
    "            due to mismatched matrix shapes.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        A NumPy array of shape (batch_size,), where each value is the per-sample attention score.\n",
    "        The score is summed across heads and averaged across tokens (in log-space).\n",
    "    \"\"\"\n",
    "    if mode not in (\"prompt\", \"generation\", \"promptGeneration\"):\n",
    "        raise ValueError(f\"Invalid mode: {mode}. Must be 'prompt', 'generation' or 'promptGeneration'.\")\n",
    "\n",
    "    batch_size, n_heads = prompt_attentions.shape[:2]\n",
    "    if generation_attentions is not None:\n",
    "        gen_len = len(generation_attentions)    \n",
    "    diag_blocks = []\n",
    "\n",
    "    # Move to device\n",
    "    device = prompt_attentions.device\n",
    "    prompt_attention_mask = prompt_attention_mask.to(device)\n",
    "    if generation_attention_mask is not None:\n",
    "        generation_attention_mask = generation_attention_mask.to(device)\n",
    "\n",
    "    # ==============================\n",
    "    # Prompt mode or combined\n",
    "    # ==============================\n",
    "    if mode in (\"prompt\", \"promptGeneration\"):\n",
    "        # Extract diagonal of prompt attentions\n",
    "        prompt_diag = torch.diagonal(prompt_attentions, dim1=-2, dim2=-1) # (batch_size, n_heads, prompt_len)\n",
    "        # Expand prompt mask to (batch_size, n_heads, prompt_len)\n",
    "        p_mask_ext = prompt_attention_mask.unsqueeze(1).expand(-1, n_heads, -1)\n",
    "        diag_blocks.append(prompt_diag)\n",
    "\n",
    "    # ==============================\n",
    "    # Generation mode or combined\n",
    "    # ==============================\n",
    "    if mode in (\"generation\", \"promptGeneration\") and gen_len > 0:\n",
    "        # For each generation step, take the last value along last dim.\n",
    "        gen_diag_steps = [attn[..., -1].squeeze(-1) for attn in generation_attentions] # list of (batch_size, n_heads)\n",
    "        # Stack along time axis (= newly generated tokens)\n",
    "        gen_diag = torch.stack(gen_diag_steps, dim=-1) if gen_diag_steps else None # (batch_size, n_heads, gen_len)\n",
    "        # Expand generation mask to (batch_size, n_heads, gen_len)\n",
    "        g_mask_ext = generation_attention_mask.unsqueeze(1).expand(-1, n_heads, -1)\n",
    "        if gen_diag is not None:\n",
    "            diag_blocks.append(gen_diag)\n",
    "\n",
    "\n",
    "    # Concatenate diagonals along tokens/time dim\n",
    "    all_diags = torch.cat(diag_blocks, dim=-1) # (batch_size, n_heads, N) where N = prompt_len + n_generated (or a subset)\n",
    "    # Build full mask concatenated similarly: (batch_size, n_heads, N)\n",
    "    if mode == \"prompt\":\n",
    "        full_mask = p_mask_ext # (batch_size, n_heads, prompt_len)\n",
    "    elif mode == \"generation\":\n",
    "        full_mask = g_mask_ext  # (batch_size, n_heads, gen_len)\n",
    "    else:  # \"promptGeneration\"\n",
    "        full_mask = torch.cat([p_mask_ext, g_mask_ext], dim=-1)  # (batch_size, n_heads, total_len)\n",
    "\n",
    "    # ==============================\n",
    "    # Compute attention eigen product, ignoring padding tokens \n",
    "    # ==============================\n",
    "    # Clamp very small values to avoid log(0)\n",
    "    all_diags = all_diags.clamp(min=1e-6)\n",
    "    # Compute log\n",
    "    log_all_diags = torch.log(all_diags) # (batch_size, n_heads, N)\n",
    "    # Mask out padding tokens by zeroing out their logs\n",
    "    masked_log_all_diags = log_all_diags * full_mask # (batch_size, n_heads, N)\n",
    "    # Count valid tokens per batch and head to compute mean properly (avoid div by zero)\n",
    "    valid_token_counts = full_mask.sum(dim=-1).clamp(min=1) # (batch_size, n_heads)\n",
    "    # Mean log diag over valid tokens dimension (N)\n",
    "    mean_log_diag = masked_log_all_diags.sum(dim=-1) / valid_token_counts  # (batch_size, n_heads)\n",
    "    # Sum over heads to get final per-sample scores\n",
    "    scores = mean_log_diag.sum(dim=-1).cpu().numpy() # (batch_size,)\n",
    "\n",
    "    return scores  # (batch_size,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OK ###\n",
    "import torch\n",
    "from torch import nn \n",
    "from typing import Callable, Optional, Tuple, Unpack\n",
    "from transformers.models.llama.modeling_llama import apply_rotary_pos_emb, eager_attention_forward, repeat_kv\n",
    "from transformers.cache_utils import Cache\n",
    "from transformers.modeling_flash_attention_utils import FlashAttentionKwargs\n",
    "from transformers.utils import logging\n",
    "logger = logging.get_logger(__name__)\n",
    "from transformers.modeling_utils import ALL_ATTENTION_FUNCTIONS\n",
    "\n",
    "\"\"\"\n",
    "**Problem**\n",
    "When working with Hugging Face's Llama models, I needed to extract attention weights for analysis purposes. \n",
    "However, the default \"eager\" attention implementation, which exposes attention weights, caused instability when running. \n",
    "Specifically, using the \"eager\" backend resulted in hidden states containing NaN values—sometimes even before the \n",
    "computation of Q, K, V. This led to CUDA crashes or completely invalid results.\n",
    "\n",
    "Switching to the \"sdpa\" attention backend resolved the numerical instability: with sdpa, there were no NaNs in \n",
    "hidden states, and the model ran stably, even in challenging configurations. However, sdpa computes attention \n",
    "weights inside a fused, highly optimized kernel and does not expose them—making it impossible to retrieve attention\n",
    " maps for analysis.\n",
    "\n",
    "Trying to \"fix\" eager by forcing float32 on hidden states did not resolve the core issue, since the rest of the model\n",
    "(and its layers) expects float16—leading to incompatibilities and further errors. Thus, neither backend offered both\n",
    " stability and transparency.\n",
    "\n",
    "**Solution**\n",
    "Implement a custom patch for the LlamaAttention forward method, but only on the specific layers where we wanted to \n",
    "access attention weights. The main computation of hidden states uses the stable backend (sdpa by default). \n",
    "This ensures the forward pass and generated sequences remain numerically stable.\n",
    "In parallel, the patch computes attention weights using the \"eager\" mechanism, but solely to extract and return them \n",
    "for inspection. These weights are not used in the model's forward pass and do not affect generation, so any instabilities \n",
    "or NaN handling for these analytical values do not impact the model's outputs.\n",
    "\n",
    "Thanks to this solution, we can now reliably run generation using Llama and access the true attention weights for \n",
    "chosen layers, benefiting both from the stability of \"sdpa\" and the interpretability of the \"eager\" backend, without \n",
    "compromising model correctness.\n",
    "\"\"\"\n",
    "\n",
    "def custom_eager_attention_forward(\n",
    "    module: nn.Module,\n",
    "    query: torch.Tensor,\n",
    "    key: torch.Tensor,\n",
    "    attention_mask: Optional[torch.Tensor],\n",
    "    scaling: float,\n",
    "    dropout: float = 0.0,\n",
    "    **kwargs,\n",
    "):\n",
    "    key_states = repeat_kv(key, module.num_key_value_groups)\n",
    "\n",
    "    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n",
    "    if attention_mask is not None:\n",
    "        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n",
    "        attn_weights = attn_weights + causal_mask\n",
    "\n",
    "    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n",
    "    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n",
    "\n",
    "    return attn_weights\n",
    "\n",
    "def patched_LlamaAttention_forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n",
    "        attention_mask: Optional[torch.Tensor],\n",
    "        past_key_value: Optional[Cache] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        **kwargs: Unpack[FlashAttentionKwargs],\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "        input_shape = hidden_states.shape[:-1]\n",
    "        hidden_shape = (*input_shape, -1, self.head_dim)\n",
    "\n",
    "        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "\n",
    "        cos, sin = position_embeddings\n",
    "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "\n",
    "        if past_key_value is not None:\n",
    "            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n",
    "            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n",
    "            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n",
    "\n",
    "        attention_interface: Callable = eager_attention_forward\n",
    "        if self.config._attn_implementation != \"eager\":\n",
    "            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n",
    "                logger.warning_once(\n",
    "                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n",
    "                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n",
    "                )\n",
    "            else:  \n",
    "                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n",
    "        \n",
    "        # ========================================================\n",
    "        # [1] Forward pass using main attention backend (sdpa / flash)\n",
    "        # This is the output used in the model's autoregressive loop.\n",
    "        # These implementations are optimized (for memory + stability).\n",
    "        # Does not compute attn_weights.\n",
    "        # ========================================================\n",
    "        attn_output, _ = attention_interface(\n",
    "                self,\n",
    "                query_states,\n",
    "                key_states,\n",
    "                value_states,\n",
    "                attention_mask,\n",
    "                dropout=0.0 if not self.training else self.attention_dropout,\n",
    "                scaling=self.scaling,\n",
    "                **kwargs,\n",
    "            )\n",
    "\n",
    "        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "\n",
    "        # ========================================================\n",
    "        # [2] Parallel computation of attention weights using eager attention\n",
    "        # This is to retrieve attention weights only (not used in forward loop)\n",
    "        # It is more numerically unstable (NaN possible with fp16)\n",
    "        # ========================================================\n",
    "        try:\n",
    "            attn_weights = custom_eager_attention_forward(\n",
    "                self,\n",
    "                query_states, \n",
    "                key_states, \n",
    "                attention_mask,\n",
    "                dropout=0.0 if not self.training else self.attention_dropout,\n",
    "                scaling=self.scaling, \n",
    "                **kwargs,\n",
    "            )\n",
    "\n",
    "            # Replace NaNs (if any) by 0.0 (no attention)\n",
    "            if torch.isnan(attn_weights).any():\n",
    "                print(\"[WARN] NaNs detected in attn_weights — replacing with 0.0 (no renormalization)\")\n",
    "                attn_weights = attn_weights.masked_fill(torch.isnan(attn_weights), 0.0)\n",
    "\n",
    "        except Exception as ex:\n",
    "            print(f\"[ERROR] Exception in custom_eager_attention_forward: {ex}\")\n",
    "            attn_weights = None\n",
    "\n",
    "        return attn_output, attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizer, PreTrainedModel\n",
    "import torch\n",
    "from torch.utils.hooks import RemovableHandle\n",
    "from typing import Tuple, Literal, List, Optional, Dict\n",
    "\n",
    "from typing import List, Tuple, Dict\n",
    "from torch.utils.hooks import RemovableHandle\n",
    "from transformers import PreTrainedModel\n",
    "\n",
    "# OLD VERSION: HOOKS ONLY 1 LAYER\n",
    "def register_generation_attention_hook(\n",
    "    model: PreTrainedModel,\n",
    "    captured_attn_list: List[torch.Tensor],\n",
    "    layer_idx: int = -1\n",
    ") -> Tuple[RemovableHandle, dict]:\n",
    "    \"\"\"\n",
    "    Attaches a forward hook to a specific Llama layer's self-attention module \n",
    "    to capture attention maps (weights) during autoregressive text generation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : PreTrainedModel\n",
    "        The Hugging Face causal language model (e.g., Llama 2).\n",
    "    captured_attn_list : List[torch.Tensor]\n",
    "        List which will receive attention tensors after each decoding step.\n",
    "        Each tensor: (batch_size * num_beams, n_heads, tgt_seq_len, src_seq_len)\n",
    "    layer_idx : int\n",
    "        Index of the layer to hook. Defaults to -1 (last layer).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    RemovableHandle\n",
    "        Call handle.remove() after generation to cleanly remove the hook.\n",
    "    call_counter : dict\n",
    "        Counts how many times the hook fired.\n",
    "    \"\"\"\n",
    "    # Raise error if layer_idx not in correct range\n",
    "    num_layers = len(model.model.layers)\n",
    "    \n",
    "    call_counter = {\"count\": 0}\n",
    "    \n",
    "    # Pick correct index if -1 given\n",
    "    idx = layer_idx if layer_idx != -1 else num_layers - 1\n",
    "\n",
    "    def attn_hook_fn(module, input, output):\n",
    "        \"\"\"\n",
    "        Hook: captures the attention weights after the forward pass.\n",
    "        For Llama (transformers >=4.31/hf), output is a tuple:\n",
    "        (attn_output, attn_weights)\n",
    "        \"\"\"\n",
    "        call_counter[\"count\"] += 1\n",
    "        # HuggingFace Llama2 attention forward: output[1] are attn weights\n",
    "        attn_weights = output[1]  # (batch * num_beams, n_heads, tgt_seq_len, src_seq_len)\n",
    "        captured_attn_list.append(attn_weights) #.detach()\n",
    "\n",
    "    # The attention submodule for Llama: \n",
    "    attention_module = model.model.layers[idx].self_attn\n",
    "\n",
    "    # Register hook on the Attention block\n",
    "    # When Pytorch pass through this layer during forward pass, it also execute attn_hook_fn.\n",
    "    handle = attention_module.register_forward_hook(attn_hook_fn)\n",
    "    return handle, call_counter\n",
    "\n",
    "\n",
    "### OK ###\n",
    "# NEW VERSION: HOOKS SEVERAL LAYERS \n",
    "def register_generation_attention_hook(\n",
    "    model: PreTrainedModel,\n",
    "    captured_attn_lists: List[List[torch.Tensor]],\n",
    "    layers_idx_list: List[int]\n",
    ") -> Tuple[List[RemovableHandle], List[Dict[str, int]]]:\n",
    "    \"\"\"\n",
    "    Attaches forward hooks to multiple specified Llama layers' self-attention modules\n",
    "    to capture attention maps (weights) during autoregressive text generation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : PreTrainedModel\n",
    "        The Hugging Face causal language model (e.g., Llama 2).\n",
    "    captured_attn_lists : List[List[torch.Tensor]]\n",
    "        A list containing one list per hooked layer.\n",
    "        Each inner list will receive attention tensors after each decoding step.\n",
    "        Each tensor shape: (batch_size, n_heads, tgt_seq_len, src_seq_len)\n",
    "    layers_idx_list : List[int]\n",
    "        List of indices of layers to hook.\n",
    "        Use -1 to denote the last layer.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[RemovableHandle]\n",
    "        List of handle objects to remove hooks after generation by calling `handle.remove()`.\n",
    "    List[Dict[str, int]]\n",
    "        List of counters (dicts with key 'count') storing how many times each hook fired.\n",
    "    \"\"\"\n",
    "    num_layers = len(model.model.layers)\n",
    "    handles = []\n",
    "    call_counters = [] # count how many times the hook is triggered\n",
    "\n",
    "    # Raise error if layer_idx not in correct range\n",
    "    for i, idx in enumerate(layers_idx_list):\n",
    "        if idx == -1:\n",
    "            idx = num_layers - 1\n",
    "        if not (0 <= idx < num_layers):\n",
    "            raise ValueError(f\"`layer_idx` must be -1 or in [0, {num_layers - 1}], but got {idx}.\")\n",
    "\n",
    "        call_counter = {\"count\": 0}\n",
    "        call_counters.append(call_counter)\n",
    "\n",
    "        def attn_hook_fn(module, input, output, call_counter=call_counter, list_idx=i):\n",
    "            \"\"\"Hook: captures the attention weights after the forward pass.\n",
    "            For Llama (transformers >=4.31/hf), output is a tuple:\n",
    "            (attn_output, attn_weights)\"\"\"\n",
    "            call_counter[\"count\"] += 1\n",
    "            attn_weights = output[1]  # (batch, n_heads, tgt_seq_len, src_seq_len)\n",
    "            captured_attn_lists[list_idx].append(attn_weights.detach())\n",
    "\n",
    "        # The attention submodule for Llama\n",
    "        attention_module = model.model.layers[idx].self_attn\n",
    "        # Register hook on the Attention block\n",
    "        # When Pytorch pass through this layer during forward pass, it also execute attn_hook_fn.\n",
    "        handle = attention_module.register_forward_hook(attn_hook_fn)\n",
    "        handles.append(handle)\n",
    "\n",
    "    return handles, call_counters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "from torch.utils.hooks import RemovableHandle\n",
    "\n",
    "# OLD VERSION: HOOKS ONLY 1 LAYER\n",
    "def register_generation_activation_hook(\n",
    "    model: PreTrainedModel,\n",
    "    captured_hidden_list: List[torch.Tensor],\n",
    "    layer_idx: int = -1\n",
    ") -> Tuple[RemovableHandle, dict]:\n",
    "    \"\"\"\n",
    "    Attaches a forward hook to a specific transformer layer to capture hidden states\n",
    "    during autoregressive text generation i.e., at each decoding step.\n",
    "    (more memory-efficient than using output_hidden_states=True).\n",
    "    Transformer layer = self-attention + FFN + normalization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : PreTrainedModel\n",
    "        The Hugging Face causal language model (e.g., GPT, LLaMA).\n",
    "    captured_hidden_list : List[torch.Tensor]\n",
    "        A list that will be filled with hidden states for each generation step. \n",
    "        Each tensor has shape (batch_size * num_beams, seq_len, hidden_size).\n",
    "    layer_idx : int\n",
    "        Index of the transformer block to hook. Defaults to -1 (the last layer).\n",
    "        Use a positive integer if you want to hook an intermediate layer instead.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    RemovableHandle : A handle object\n",
    "        Call `handle.remove()` after generation to remove the hook.\n",
    "    call_counter : int \n",
    "        Stores the number of times the hook is activated.\n",
    "    \"\"\"\n",
    "    # Raise error if layer_idx not in correct range\n",
    "    num_layers = len(model.model.layers)\n",
    "    if not (layer_idx == -1 or 0 <= layer_idx < num_layers):\n",
    "        raise ValueError(\n",
    "            f\"`layer_idx` must be -1 or in [0, {num_layers - 1}], but got {layer_idx}.\"\n",
    "        )\n",
    "    \n",
    "    call_counter = {\"count\": 0} # count how many times the hook is triggered\n",
    "\n",
    "    def hook_fn(module, input, output):\n",
    "        \"\"\"Function called automatically by PyTorch just after\n",
    "            the layer has produced its output during the forward pass.\"\"\"\n",
    "        \n",
    "        call_counter[\"count\"] += 1 \n",
    "        # output is a tuple (hidden_states,) → keep [0]\n",
    "        if layer_idx == -1:\n",
    "            # Capture the final normalized output \n",
    "            captured_hidden_list.append(model.model.norm(output[0]).detach())  # post RMSNorm!\n",
    "        else:\n",
    "            # Capture raw hidden states before layer normalization\n",
    "            captured_hidden_list.append(output[0].detach()) #### TEST #### \n",
    "    \n",
    "    # Register hook on the transformer block\n",
    "    # When Pytorch pass through this layer during forward pass, it also execute hook_fn.\n",
    "    handle = model.model.layers[layer_idx].register_forward_hook(hook_fn)\n",
    "    \n",
    "    return handle, call_counter\n",
    "\n",
    "### OK ###\n",
    "# NEW VERSION: HOOKS SEVERAL LAYERS \n",
    "def register_generation_activation_hook(\n",
    "    model: PreTrainedModel,\n",
    "    captured_hidden_lists: List[List[torch.Tensor]],  \n",
    "    layers_idx_list: List[int]\n",
    ") -> Tuple[List[RemovableHandle], List[Dict[str,int]]]:\n",
    "    \"\"\"\n",
    "    Attaches a forward hook to a specific transformer layer to capture hidden states\n",
    "    during autoregressive text generation i.e., at each decoding step.\n",
    "    (more memory-efficient than using output_hidden_states=True).\n",
    "    Transformer layer = self-attention + FFN + normalization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : PreTrainedModel\n",
    "        The Hugging Face causal language model (e.g., GPT, LLaMA).\n",
    "    captured_hidden_lists : List[List[torch.Tensor]]\n",
    "        A list containing one list per hooked layer.\n",
    "        Each inner list will be filled with hidden states for each generation step,\n",
    "        each tensor of shape (batch_size, seq_len, hidden_size).\n",
    "    layers_idx_list : List[int]\n",
    "        List of transformer block indices to hook.\n",
    "        Use -1 to denote the last layer.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    List[RemovableHandle]\n",
    "        List of handle objects to remove hooks after generation by calling `handle.remove()`.\n",
    "    List[Dict[str, int]]\n",
    "        List of counters (dicts with key 'count') storing how many times each hook was activated.\n",
    "    \"\"\"\n",
    "    handles = []\n",
    "    call_counters = [] # count how many times the hook is triggered\n",
    "\n",
    "    # Raise error if layer_idx not in correct range\n",
    "    num_layers = len(model.model.layers)\n",
    "    for idx in layers_idx_list:\n",
    "        if not (idx == -1 or 0 <= idx < num_layers):\n",
    "            raise ValueError(f\"`layer_idx` must be -1 or in [0, {num_layers-1}] but got {idx}\")\n",
    "\n",
    "        call_counter = {\"count\": 0}  \n",
    "        call_counters.append(call_counter)\n",
    "\n",
    "        def hook_fn(module, input, output, call_counter=call_counter, idx=idx):\n",
    "            \"\"\"Function called automatically by PyTorch just after\n",
    "            the layer has produced its output during the forward pass.\"\"\"\n",
    "            \n",
    "            call_counter[\"count\"] += 1\n",
    "            # output is a tuple (hidden_states,) -> keep [0]\n",
    "            if idx == -1:\n",
    "                # Capture the final normalized output \n",
    "                captured_hidden_lists[layers_idx_list.index(idx)].append(\n",
    "                    model.model.norm(output[0]).detach())  # post RMSNorm!\n",
    "            else: \n",
    "                # Capture raw hidden states before layer normalization\n",
    "                captured_hidden_lists[layers_idx_list.index(idx)].append(output[0].detach())\n",
    "\n",
    "        # Register hook on the transformer block\n",
    "        # When Pytorch pass through this layer during forward pass, it also execute hook_fn.\n",
    "        handle = model.model.layers[idx].register_forward_hook(hook_fn)\n",
    "        handles.append(handle)\n",
    "\n",
    "    return handles, call_counters\n",
    "\n",
    "\n",
    "from typing import List, Dict\n",
    "\n",
    "### OK ###\n",
    "def verify_call_counters(call_counters: List[Dict[str, int]], name: str = \"hooks\") -> None:\n",
    "    \"\"\"\n",
    "     Checks that all call counters are > 0 and equal.\n",
    "\n",
    "    Args:\n",
    "        call_counters: List of dictionaries with the key 'count'.\n",
    "        name: Descriptive name for the error message.\n",
    "    \n",
    "    Raises:\n",
    "        RuntimeError if a counter is 0 or if the counters differ.\n",
    "    \"\"\"\n",
    "    if not all(counter['count'] > 0 for counter in call_counters):\n",
    "        raise RuntimeError(f\"At least one {name} did not capture any events.\")\n",
    "    \n",
    "    counts = [counter['count'] for counter in call_counters]\n",
    "    if len(set(counts)) > 1:\n",
    "        raise RuntimeError(f\"{name.capitalize()} have inconsistent call counts: {counts}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "### OK ###\n",
    "def compute_perplexity(\n",
    "        prompt_logits: torch.Tensor, \n",
    "        gen_logits: torch.Tensor,\n",
    "        prompt_ids: torch.Tensor, \n",
    "        gen_ids: torch.Tensor,\n",
    "        prompt_attention_mask: torch.Tensor,\n",
    "        gen_attention_mask: torch.Tensor,\n",
    "        mode: Literal[\"prompt\", \"generation\", \"promptGeneration\"] = \"prompt\",\n",
    "        prepend_last_prompt_logit: bool = False,\n",
    "        min_k: float = None,\n",
    "    ) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the per-sample perplexity of language model outputs using logits \n",
    "    and corresponding input token IDs. Logits maked by 0 in the attention mask \n",
    "    are ignored in the computation of the perplexity. \n",
    "    If `min_k` is provided,\n",
    "    it filters the lowest probabilities to compute a restricted perplexity.\n",
    "\n",
    "    Perplexity is defined as:\n",
    "        Perplexity = exp(- mean(log P(token_i | context))) \n",
    "        where token_i is the next token actually predicted\n",
    "\n",
    "    NOTE: This implementation is inspired by:\n",
    "    \"LLM-Check: Investigating Detection of Hallucinations in Large Language Models\"\n",
    "    (Sriramanan et al., 2024)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    prompt_logits : torch.Tensor\n",
    "        Tensor of shape (batch_size, prompt_len, vocab_size) \n",
    "        These are the model's output logits obtained from a standard forward pass over the prompt sequence.\n",
    "    gen_logits : torch.Tensor\n",
    "        Tensor of shape (batch_size, gen_len, vocab_size).\n",
    "        These are the logits obtained during autoregressive decoding using `model.generate()`.\n",
    "    prompt_ids : torch.Tensor\n",
    "        Tensor of shape (batch_size, prompt_len), containing the input token IDs for the prompt.\n",
    "    gen_ids : torch.Tensor\n",
    "        Tensor of shape (batch_size, gen_len), containing the token IDs generated by the model.\n",
    "    prompt_attention_mask: torch.Tensor\n",
    "        Tensor of shape (batch_size, prompt_len), 1 where token valid, 0 for padding.\n",
    "    gen_attention_mask: torch.Tensor  \n",
    "        Tensor of shape (batch_size, gen_len), 1 where token valid, 0 for padding.\n",
    "    mode : str, optional\n",
    "        One of {\"prompt\", \"generation\", \"promptGeneration\"}:\n",
    "        - \"prompt\": compute perplexity only over the prompt.\n",
    "        - \"generation\": compute perplexity only over the generated tokens.\n",
    "        - \"promptGeneration\": compute perplexity over both prompt and generation.\n",
    "    prepend_last_prompt_logit : bool, optional\n",
    "        If True, appends the last logit from the prompt to the beginning of the generation logits.\n",
    "        This is useful when generation logits were computed manually from hidden states \n",
    "        and are therefore shifted by one position (they lack the first prediction step).\n",
    "        => see Notes 2)C) below for more detail. Default is False.\n",
    "        Carreful! The gen_attention_mask must match.\n",
    "    min_k : float, optional\n",
    "        Optional value between 0 and 1. If specified, only the bottom-k lowest-probability\n",
    "        tokens are used for perplexity calculation.\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "        np.ndarray: Per-sample perplexity scores of shape (batch_size,)\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    1) This function computes a \"Pseudo Perplexity\".\n",
    "\n",
    "        The Standard Perplexity requires ground truth tokens:\n",
    "            PPL = exp(-1/N ∑_{t=1}^N log p(w_real_t | w_real_{<t})) where w_real_t are the true next tokens\n",
    "\n",
    "        In our case, we are in pure generation mode (equivalent to teacher forcing on the generated text)\n",
    "        and we don't have acces the real tokens. We thefore compute the Pseudo Perplexity:\n",
    "            PPL_gen = exp(-1/N ∑_{t=1}^N log p(w_gen_t | w_gen_{<t})) where w_gen_t are the generated next tokens\n",
    "        This measures the internal consistency of the model, and how well the model finds its own generation probable. \n",
    "    \n",
    "    2) About token shifting in autoregressive models:\n",
    "\n",
    "        A) When extracting prompt logits with with a standard autoregressive forward pass:\n",
    "            Example: \n",
    "            prompt_outputs = model(inputs['input_ids'], output_logits=True)\n",
    "            prompt_logits = outputs.logits # Tensor of shape (batch_size, promp_len, vocab_size)\n",
    "\n",
    "            - The logit at position *t* predicts the token at position *t+1*.\n",
    "            - The first token has no preceding context and is not predicted.\n",
    "            - When computing log-probabilities, we must **shift the targets one position to the left** \n",
    "            to correctly align logits with target tokens.\n",
    "            \n",
    "            Example: Suppose we have a sequence of tokens (with their token IDs):\n",
    "            | Index | Token | ID  |\n",
    "            |-------|-------|-----| - The model produces logits at positions 0, 1, \n",
    "            | 0     | A     | 10  | and 2 to predict the tokens B, C, and D, respectively.\n",
    "            | 1     | B     | 29  |\n",
    "            | 2     | C     | 305 |  - The logits at position 0 are used to predict\n",
    "            | 3     | D     | 24  |  token B (ID 29).\n",
    "\n",
    "        B) When extracting gen logits during during generation:\n",
    "            Example: \n",
    "            gen_outputs = model.generate(**inputs, max_new_tokens=10, output_logits=True) # gen_len=max_new_tokens\n",
    "            gen_logits = torch.stack(outputs.logits,dim=1)  # Tensor of shape (batch_size, gen_len, vocab_size)\n",
    "            \n",
    "            - The logit at time step *t* predicts the token generated at position *t*.\n",
    "            - Each logit already corresponds to the prediction of the token at this step \n",
    "            - No shifting is needed in this case.\n",
    "\n",
    "        REMARK:\n",
    "        The last prompt logit (i.e., `prompt_logits[:, -1, :]`) predicts the first generated token.\n",
    "        This means: prompt_logits[:, -1, :] = gen_logits[:, 0, :]\n",
    "\n",
    "        C) When computing gen_logits directly from the activations (no build-in methods) \n",
    "            Example:\n",
    "            computed_gen_logits = model.lm_head(gen_hidden_states) \n",
    "            # Tensor of shape (batch_size, gen_len-1, vocab_size), gen_len=max_new_tokens\n",
    "            Typically, `gen_hidden_states` is a Tensor of shape (batch_size, gen_len-1, hidden_size) \n",
    "            computed from gen_outputs.hidden_states. It has shape `gen_len-1`\n",
    "            because there is no hidden state for the final generated token.\n",
    "\n",
    "            - Here, the logit at position *t* predicts the generated token at position *t+1* !! \n",
    "            - Since the last prompt_logits = the first gen_logits, we repend the last prompt\n",
    "            logit to the beginning of computed_gen_logits with `prepend_last_prompt_logit=True`\n",
    "            - We recover case B) with the full gen_logits (as would be returned by model.generate) \n",
    "            - We can use the same gen_attenion_mask as in case B)\n",
    "\n",
    "        D) When computing prompt_logits directly from the activations (no build-in methods) \n",
    "            Example:\n",
    "            computed_prompt_logits = model.lm_head(prompt_hidden_states) \n",
    "            # Tensor of shape (batch_size, prompt_len, vocab_size),\n",
    "            Typically, `prompt_hidden_states` is a Tensor of shape (batch_size, prompt_len, hidden_size) \n",
    "\n",
    "            - The logit at position *t* predicts the token at position *t+1* => recover case A)\n",
    "            - We can use the same prompt_attenion_mask as in case A)\n",
    "\n",
    "        Summary of alignment:\n",
    "            - A) Prompt Logits from forward: \n",
    "                logit at position *t* predicts token at position *t+1* -> shift targets left.\n",
    "            - B) Generation Logits from generate: \n",
    "                logit at position *t* predicts token at position *t* -> no shift.\n",
    "            - C) Computed Generation Logits: \n",
    "                set `append_last_prompt_logit=True` and no shift needed -> go back to case B)\n",
    "            - D) Computed Prompt Logits:\n",
    "                similar to case A), nothing to do. \n",
    "\n",
    "        NOTE: help from issue https://github.com/huggingface/transformers/issues/29664\n",
    "    \"\"\"  \n",
    "\n",
    "    if min_k is not None:\n",
    "        if min_k < 0 or min_k > 1: raise ValueError(\"min_k must be between 0 and 1\")\n",
    "\n",
    "    if mode not in ('prompt','generation','promptGeneration'):\n",
    "        raise ValueError(\"mode must be in {'prompt','generation','promptGeneration'}\")\n",
    "    \n",
    "    # ==============================\n",
    "    # Move to device\n",
    "    # ==============================\n",
    "    prompt_logits = prompt_logits.to(prompt_attention_mask.device)\n",
    "    if gen_logits is not None:\n",
    "        gen_logits = gen_logits.to(gen_attention_mask.device)\n",
    "\n",
    "    # ==============================\n",
    "    # Prepend last logit of prompt to the generation logits if specifed\n",
    "    # ==============================\n",
    "    if prepend_last_prompt_logit:\n",
    "        last_prompt_logit = prompt_logits[:, -1:, :] # (batch_size, 1, vocab_size)\n",
    "        gen_logits = torch.cat([last_prompt_logit, gen_logits], dim=1) # (batch_size, gen_len+1, vocab_size)\n",
    "           \n",
    "    # ==============================\n",
    "    # Apply softmax over vocabulary dimension and take log to get log-probabilities\n",
    "    # ==============================\n",
    "    prompt_log_probs = torch.log_softmax(prompt_logits, dim=-1)  # (batch_size, prompt_len, vocab_size)\n",
    "    if gen_logits is not None:\n",
    "        gen_log_probs = torch.log_softmax(gen_logits, dim=-1)    # (batch_size, gen_len, vocab_size)\n",
    "\n",
    "    # ==============================\n",
    "    # Extraction of prompt log-probs\n",
    "    # ==============================\n",
    "    if mode in (\"prompt\", \"promptGeneration\"):\n",
    "        # In prompt: logit at position t predicts token at t+1 (requires shifting)\n",
    "        # Remove first token from target (no context to predict it)\n",
    "        prompt_target_tokens = prompt_ids[:, 1:] # (batch_size, prompt_len - 1)\n",
    "\n",
    "        prompt_attention_mask_shifted = prompt_attention_mask[:, 1:]  # (batch_size, prompt_len - 1)\n",
    "\n",
    "        # Remove last logit position (since it predicts next token)\n",
    "        prompt_pred_log_probs = prompt_log_probs[:, :-1, :] # shape: (batch_size, prompt_len - 1, vocab_size)\n",
    "\n",
    "        # Retrieves, for each position and each batch, the log-probability corresponding to the next token \n",
    "        # (the one in target_tokens) from all the probas on the vocabulary.\n",
    "        prompt_token_log_probs = prompt_pred_log_probs.gather(\n",
    "            dim=2, index=prompt_target_tokens.unsqueeze(-1)\n",
    "            ).squeeze(-1) # shape: (batch_size, prompt_len - 1)\n",
    "      \n",
    "        # Mask paddings\n",
    "        prompt_token_log_probs = prompt_token_log_probs * prompt_attention_mask_shifted\n",
    "        \n",
    "    # ==============================\n",
    "    # Extraction of generation log-probs\n",
    "    # ==============================\n",
    "    if mode in (\"generation\", \"promptGeneration\"):\n",
    "        # In generation: logit at position t predicts token at position t (no shift needed)\n",
    "        gen_token_log_probs = gen_log_probs.gather(\n",
    "            dim=2, index=gen_ids.unsqueeze(-1)\n",
    "            ).squeeze(-1)  # shape: (batch_size, gen_len)\n",
    "        \n",
    "        # Mask paddings\n",
    "        gen_token_log_probs = gen_token_log_probs * gen_attention_mask\n",
    "    \n",
    "\n",
    "    # ==============================\n",
    "    # Select log-probs according to mode\n",
    "    # ==============================\n",
    "    if mode == \"promptGeneration\":\n",
    "        # Last logit of prompt from the forward pass == first logit of generation from `model.generate()`. \n",
    "        # To compute perplexity over the full sequence:\n",
    "        # - Use prompt_token_log_probs (excluding final prompt token)\n",
    "        # - Use gen_token_log_probs from generation\n",
    "        # Concatenate both to form a complete sequence of predicted log-probs\n",
    "        token_log_probs = torch.cat(\n",
    "            [prompt_token_log_probs, gen_token_log_probs],  \n",
    "            dim=1) # (batch_size, prompt_len - 1 + gen_len)\n",
    "        total_mask = torch.cat(\n",
    "            [prompt_attention_mask_shifted, gen_attention_mask],\n",
    "            dim=1) # (batch_size, prompt_len - 1 + gen_len)\n",
    "    \n",
    "    elif mode == \"prompt\":\n",
    "        token_log_probs = prompt_token_log_probs    # (batch_size, prompt_len - 1)\n",
    "        total_mask = prompt_attention_mask_shifted  # (batch_size, prompt_len - 1)\n",
    "    \n",
    "    elif mode == \"generation\":\n",
    "        token_log_probs = gen_token_log_probs  # (batch_size, gen_len)\n",
    "        total_mask = gen_attention_mask        # (batch_size, gen_len)\n",
    "\n",
    "    # ==============================\n",
    "    # Compute Perplexity ignoring padded tokens\n",
    "    # ==============================\n",
    "    eps = 1e-12  # to avoid division by zero\n",
    "\n",
    "    # Optionally focus only on the k% hardest tokens (lowest log-probs)\n",
    "    if min_k is not None:\n",
    "        # Keep only the min_k fraction of tokens with the lowest log-probs \n",
    "        k = int(min_k * token_log_probs.size(1))  # number of tokens to keep per sample\n",
    "        \n",
    "        # Exclude padding tokens from topk selection\n",
    "        masked_log_probs = token_log_probs.clone()\n",
    "        masked_log_probs[total_mask == 0] = 1e6  \n",
    "\n",
    "        # Use topk with largest=False to get the k tokens with the lowest log-probabilities\n",
    "        topk_vals, _ = torch.topk(masked_log_probs, k=k, dim=1, largest=False)\n",
    "\n",
    "        # Compute perplexity using only the selected subset\n",
    "        ppls = torch.exp(-topk_vals.mean(dim=1))\n",
    "\n",
    "    else:\n",
    "        # Compute perplexity over all predicted tokens\n",
    "        sum_log_probs = (token_log_probs * total_mask).sum(dim=1)\n",
    "        count = total_mask.sum(dim=1).clamp(min=eps)\n",
    "        mean_log_prob = sum_log_probs / count\n",
    "        ppls = torch.exp(-mean_log_prob)\n",
    "\n",
    "    return ppls.cpu().numpy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "### OK ###\n",
    "def compute_logit_entropy(\n",
    "    prompt_logits: torch.Tensor,\n",
    "    gen_logits: torch.Tensor,\n",
    "    prompt_attention_mask: torch.Tensor,\n",
    "    gen_attention_mask: torch.Tensor,\n",
    "    mode: str = \"prompt\",\n",
    "    prepend_last_prompt_logit: bool = False,\n",
    "    top_k: int = None,\n",
    "    window_size: int = None,\n",
    "    stride: int = None\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the per-sample entropy of a language model's output distributions\n",
    "    using its logits and attention masks.\n",
    "    For each token position, the function computes the entropy of the softmax distribution\n",
    "    over the vocabulary. Entropy is averaged over the valid tokens (i.e., those marked\n",
    "    as 1 in the attention mask). If `top_k` is specified, the entropy is computed only\n",
    "    over the top-k logits (highest values) for each position.\n",
    "\n",
    "    Entropy is defined as:\n",
    "        Entropy = -Sum_i p_i * log(p_i)\n",
    "        where p_i = softmax(logits)_i, i=1..vocab_size\n",
    "    \n",
    "    There are two main usage patterns:\n",
    "      - Classic token-level average entropy (if window_size is None): computes the per-token entropy over the\n",
    "        sequence, averages over all valid tokens per sample (optionally using top_k).\n",
    "      - Windowed maximum mean entropy (if window_size is specified): slides a window of width `window_size`\n",
    "        and stride `stride` (default equals window_size: non-overlapping windows, else user-specified) across\n",
    "        the sequence of token entropies, and returns the maximum mean entropy observed in any window for each sample.\n",
    "\n",
    "    Padding tokens are always ignored (via the provided attention masks); only windows where all tokens are valid\n",
    "    are considered in the windowed mode.\n",
    "\n",
    "    NOTE: This implementation is inspired by:\n",
    "    \"LLM-Check: Investigating Detection of Hallucinations in Large Language Models\"\n",
    "    (Sriramanan et al., 2024)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    prompt_logits : torch.Tensor\n",
    "        Tensor of shape (batch_size, prompt_len, vocab_size).\n",
    "        These are the model's output logits obtained from a standard forward pass over the prompt sequence.\n",
    "    gen_logits : torch.Tensor\n",
    "        Tensor of shape (batch_size, gen_len, vocab_size).\n",
    "        These are the logits obtained during autoregressive decoding using `model.generate()`.\n",
    "    prompt_attention_mask : torch.Tensor\n",
    "        Tensor of shape (batch_size, prompt_len). Contains 1 where the token is valid and 0 for padding.\n",
    "    gen_attention_mask : torch.Tensor  \n",
    "        Tensor of shape (batch_size, gen_len). Contains 1 where the token is valid and 0 for padding.\n",
    "    mode : str, optional\n",
    "        Which tokens to use for entropy computation:\n",
    "        - \"prompt\": compute entropy only over the prompt logits/mask.\n",
    "        - \"generation\": compute entropy only over the generated logits/mask.\n",
    "        - \"promptGeneration\": compute entropy over both concatenated prompt and generated logits/mask.\n",
    "    prepend_last_prompt_logit : bool, optional\n",
    "        If True, appends the last logit from the prompt to the beginning of the generation logits.\n",
    "        This is useful when generation logits were computed manually from hidden states \n",
    "        and are therefore shifted by one position (they lack the first prediction step so \n",
    "        the first logit is missing). Default is False.\n",
    "        Carreful! The gen_attention_mask must match.\n",
    "    top_k : int, optional\n",
    "        If specified, only the top_k logits (per token) are used to compute the entropy.\n",
    "        If None, use all logits.\n",
    "    window_size : int, optional\n",
    "        If not None, apply a sliding window of this size across the (valid) sequence of token entropies,\n",
    "        and return the maximum mean entropy over any complete window, for each sample.\n",
    "        If None, simply average the per-token entropies over all valid tokens.\n",
    "    stride : int, optional\n",
    "        Sliding window stride. Only used if window_size is specified.\n",
    "        - If None, defaults to window_size (non-overlapping windows).\n",
    "        - If set, must be a positive integer <= window_size.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Array of shape (batch_size,). For each batch sample, either the average logit entropy\n",
    "        over valid tokens (if window_size is None) or the maximum windowed mean entropy (if window_size is given).\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Padding tokens are always ignored, both in classic and windowed entropy.\n",
    "    - In windowed mode, only windows where all tokens in the window are valid are considered.\n",
    "    - Uses torch.special.entr for numerically stable entropy calculation.\n",
    "    \"\"\"\n",
    "    # ==============================\n",
    "    # Move to device\n",
    "    # ==============================\n",
    "    prompt_logits = prompt_logits.to(prompt_attention_mask.device)\n",
    "    if gen_logits is not None:\n",
    "        gen_logits = gen_logits.to(gen_attention_mask.device)\n",
    "\n",
    "    # Prepend last logit of prompt to the generation logits if specifed\n",
    "    if prepend_last_prompt_logit:\n",
    "        last_prompt_logit = prompt_logits[:, -1:, :] # (batch_size, 1, vocab_size)\n",
    "        gen_logits = torch.cat([last_prompt_logit, gen_logits], dim=1) # (batch_size, gen_len+1, vocab_size)\n",
    "\n",
    "    def entropy_from_logits(logits, attention_mask, top_k=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        logits: (batch_size, seq_len, vocab_size)\n",
    "        attention_mask: (batch_size, seq_len)\n",
    "        top_k: int > 0\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        entropy: (batch_size, seq_len)\n",
    "        attention_mask: (batch_size, seq_len)\n",
    "        \"\"\"\n",
    "\n",
    "        # Convert float16 -> float32 for better accuracy during computations\n",
    "        logits = logits.float()\n",
    "        attention_mask = attention_mask.float()\n",
    "\n",
    "        if top_k is not None:\n",
    "            topk_vals = torch.topk(logits, k=top_k, dim=-1).values  # (batch_size, seq_len, top_k)\n",
    "            probs = F.softmax(topk_vals, dim=-1) # (batch_size, seq_len, top_k)\n",
    "        else:\n",
    "            probs = F.softmax(logits, dim=-1) # (batch_size, seq_len, vocab_size)\n",
    "\n",
    "        # Use torch.special.entr, which automatically handles edge cases\n",
    "        # entropy(x) = -x * log(x) with entropy(0) = 0\n",
    "        entropy = torch.special.entr(probs).sum(dim=-1)  # (batch_size, seq_len)\n",
    "        return entropy, attention_mask # both are (batch_size, seq_len)\n",
    "\n",
    "    def average_entropy(entropy, mask):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        entropy: (batch_size, seq_len)\n",
    "        mask: (batch_size, seq_len)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        avg_entropy: (batch_size,)\n",
    "        \"\"\"\n",
    "        entropy_masked = entropy * mask                    # (batch_size, seq_len)\n",
    "        total_entropy = entropy_masked.sum(dim=-1)         # (batch_size,)\n",
    "        valid_count = mask.sum(dim=-1)                     # (batch_size,)\n",
    "        avg_entropy = total_entropy / (valid_count + 1e-9) # (batch_size,)\n",
    "        return avg_entropy\n",
    "\n",
    "    def max_sliding_window_entropy(entropy, mask, w, stride):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        entropy: (batch_size, seq_len)\n",
    "        mask: (batch_size, seq_len)\n",
    "        w: int > 0\n",
    "        stride: int > 0\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        max_avg_entropy: (batch_size,)\n",
    "        \"\"\"\n",
    "        # Add one dummy channel dimension since conv1d requires 3D tensors\n",
    "        entropy = entropy.unsqueeze(1)  # (batch_size, 1, seq_len)\n",
    "        mask = mask.unsqueeze(1)        # (batch_size, 1, seq_len)\n",
    "\n",
    "        kernel = torch.ones(1, 1, w, device=entropy.device) / w  # shape: (1,1,w)\n",
    "\n",
    "        # padding=0 to avoid artificial values and distorting the calculation\n",
    "        # Ignore windows for which there are not enough elements to form a complete window.\n",
    "        moving_avg = F.conv1d(entropy, kernel, stride=stride, padding=0)  # sliding mean entropy\n",
    "        \n",
    "        # All windows where there is at least one padding token will be ignored with valid_mask\n",
    "        valid_counts = F.conv1d(mask, kernel, stride=stride, padding=0)   # sliding mean mask (valid token ratio)\n",
    "        valid_mask = (valid_counts == 1.0)  # full valid windows only\n",
    "\n",
    "        moving_avg = moving_avg.masked_fill(~valid_mask, float('-inf')) # put -inf where valid_mask==0\n",
    "\n",
    "        max_avg_entropy, _ = moving_avg.max(dim=-1)  # (batch_size, 1)\n",
    "        \n",
    "        return max_avg_entropy.squeeze(1) # (batch_size,)\n",
    "\n",
    "    if top_k is not None:\n",
    "        top_k = int(top_k)\n",
    "        if top_k <= 0 or top_k > prompt_logits.shape[2]:\n",
    "            raise ValueError(\"top_k must be a positive integer less or equal to vocab size\")\n",
    "        \n",
    "    if window_size is not None:\n",
    "        if stride is None:\n",
    "            stride = window_size\n",
    "        else:\n",
    "            stride = int(stride)\n",
    "            if stride <= 0 or stride > window_size:\n",
    "                raise ValueError(\"stride must be a positive integer less or equal to window_size.\")\n",
    "    else:\n",
    "        stride = None\n",
    "\n",
    "    if mode == \"prompt\":\n",
    "        entropy, mask = entropy_from_logits(prompt_logits, prompt_attention_mask, top_k) # both are (batch_size, prompt_len)\n",
    "    elif mode == \"generation\":\n",
    "        entropy, mask = entropy_from_logits(gen_logits, gen_attention_mask, top_k)       # both are (batch_size, gen_len)\n",
    "    elif mode == \"promptGeneration\":\n",
    "        ent_p, mask_p = entropy_from_logits(prompt_logits, prompt_attention_mask, top_k) # both are (batch_size, prompt_len)\n",
    "        ent_g, mask_g = entropy_from_logits(gen_logits, gen_attention_mask, top_k)       # both are (batch_size, gen_len)\n",
    "        entropy = torch.cat([ent_p, ent_g], dim=1) # (batch_size, prompt_len + gen_len)\n",
    "        mask = torch.cat([mask_p, mask_g], dim=1)  # (batch_size, prompt_len + gen_len)\n",
    "    else:\n",
    "        raise ValueError(\"mode must be in {'prompt','generation','promptGeneration'}\")\n",
    "\n",
    "    if window_size is None:\n",
    "        result = average_entropy(entropy, mask)\n",
    "    \n",
    "    else:\n",
    "        if window_size <= 0:\n",
    "            raise ValueError(\"window_size must be a positive integer\")\n",
    "        if window_size > entropy.shape[1]:\n",
    "            raise ValueError(\"window_size greater than sequence length\")\n",
    "        if stride is None:\n",
    "            stride = window_size\n",
    "        else:\n",
    "            stride = int(stride)\n",
    "            if stride <= 0 or stride > window_size:\n",
    "                raise ValueError(\"stride must be a positive integer less or equal to window_size.\")\n",
    "        \n",
    "        window_size = int(window_size)\n",
    "        result = max_sliding_window_entropy(entropy, mask, window_size, stride)\n",
    "    return result.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizer, PreTrainedModel, BatchEncoding\n",
    "import torch\n",
    "from datasets import  Dataset\n",
    "from tqdm import tqdm\n",
    "from typing import List, Callable, Union, Literal, Dict, Tuple\n",
    "from torch.utils.hooks import RemovableHandle\n",
    "\n",
    "from src.inference.offset_utils import (\n",
    "    compute_offset_attention_mask,\n",
    ")\n",
    "from src.inference.generation_utils import (\n",
    "    build_prompt,\n",
    "    extract_batch, \n",
    "    build_generation_attention_mask)\n",
    "\n",
    "### OK ###\n",
    "def generate(\n",
    "    model: PreTrainedModel,\n",
    "    inputs: BatchEncoding,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    max_new_tokens: int = 50,\n",
    "    k_beams: int = 1,\n",
    "    **generate_kwargs\n",
    ") -> Union[torch.Tensor, Dict[str, torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Generate sequences from the model with optional beam search.\n",
    "    Supports advanced options via **generate_kwargs (e.g., output_attentions).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : PreTrainedModel\n",
    "        The language model to use for generation.\n",
    "    inputs : BatchEncoding\n",
    "        Tokenized input prompts.\n",
    "    tokenizer : PreTrainedTokenizer\n",
    "        Tokenizer providing eos and pad token IDs.\n",
    "    max_new_tokens : int, optional\n",
    "        Maximum number of new tokens to generate.\n",
    "    k_beams : int, optional\n",
    "        Number of beams to use. If 1, uses sampling. If >1, beam search is enabled.\n",
    "    **generate_kwargs : dict\n",
    "        Additional keyword arguments passed to `model.generate()`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Union[torch.Tensor, Dict[str, torch.Tensor]]\n",
    "        - If k_beams == 1:\n",
    "            Returns a tensor of generated token IDs: shape (batch_size, prompt_len + gen_len)\n",
    "        - If k_beams > 1:\n",
    "            Returns a dictionary with keys:\n",
    "                - \"sequences\": the generated token IDs\n",
    "                - \"beam_indices\": the beam path for each token\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True    if k_beams == 1 else False,\n",
    "            temperature=0.6   if k_beams == 1 else None,\n",
    "            top_p=0.9         if k_beams == 1 else None,\n",
    "            top_k=50          if k_beams == 1 else None,\n",
    "            num_beams=k_beams,\n",
    "            use_cache=True, \n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.eos_token_id, # Ensures clean padding (right padding)\n",
    "            output_hidden_states=False,      # We rely on the hook to extract hidden states instead (more memory efficient)\n",
    "            output_attentions=False,         # We rely on the hook to extract attention map instead (more memory efficient)\n",
    "            output_logits=True,              # Logits not filtered/truncated by top-k/top-p sampling. Note: `output_scores=True` returns filtered logits. \n",
    "            return_dict_in_generate=True,    # Needed for access to beam_indices when num_beams > 1\n",
    "            early_stopping=False if k_beams == 1 else True, #Generation stops as soon as any sequence hits EOS, even if other candidates have not yet finished.\n",
    "            **generate_kwargs                # For future flexibility (e.g., output_attentions, output_scores)\n",
    "        )\n",
    "        return outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_logit_lens(\n",
    "        model: PreTrainedModel, \n",
    "        hidden_states: torch.Tensor\n",
    "    ) -> torch.Tensor :\n",
    "    \"\"\"\n",
    "    Applies the model's LM head to hidden states to produce logits.\n",
    "\n",
    "    Args:\n",
    "        model: PreTrainedModel with `lm_head` attribute.\n",
    "        hidden_states: Tensor (batch_size, seq_len, hidden_size).\n",
    "\n",
    "    Returns:\n",
    "        logits: Tensor (batch_size, seq_len, vocab_size).\n",
    "\n",
    "    NOTE: \n",
    "    We do not apply layer norm to match transformers llama implementation\n",
    "    \"\"\"\n",
    "    # Apply LM head (linear projection)\n",
    "    logits = model.lm_head(hidden_states)\n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prompt_and_generation_score_extraction(\n",
    "    model: PreTrainedModel,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    dataset: Dataset,\n",
    "    batch_size: int = 4,\n",
    "    idx_start_sample: int = 0,\n",
    "    max_samples: int = 1000,\n",
    "    save_to_pkl: bool = False,\n",
    "    output_path: str = \"outputs/all_batch_results.pkl\",\n",
    "    build_prompt_fn: Callable[[str, str], str] = None,\n",
    "    layers: List[int] = [-1],  \n",
    "    activation_source: Literal[\"prompt\", \"generation\", \"promptGeneration\"] = \"generation\",\n",
    "    hidden_scores: List[str] = [\"average\", \"last\", \"max\", \"first_generated\", \"token_svd_score\", \"feat_var\"],\n",
    "    attn_scores: List[str] = [\"attn_eig_prod\"],\n",
    "    logit_scores: List[str] = [\"perplexity\", \"logit_entropy\", \"window_logit_entropy\"],\n",
    "    logit_config: dict = {\"top_k\": 50, \"window_size\": 1, \"stride\": 1},\n",
    "    start_offset : int = 0,\n",
    "    end_offset : int = 0,\n",
    ") -> Union[List[torch.Tensor], None]:\n",
    "    \"\"\"\n",
    "    Runs batched inference on a dataset using a decoder-only language model.\n",
    "    For each batch, it performs text generation and extracts token-level \n",
    "    hidden activations, attention maps and logit scores from specified transformer layers.\n",
    "    (both from the prompt and the generated text depending on `activation_source`) \n",
    "\n",
    "    The function supports multiple aggregation modes for the activations (`hidden_scores`), attention-based \n",
    "    scores (`attn_scores`), and logit-based scores (`logit_scores`). The `logit_config` argument provides \n",
    "    configuration parameters for logit-based score functions.\n",
    "    \n",
    "    Hidden states and attention maps are captured via forward hooks during generation, \n",
    "    then aggregated  based on token position and attention masks.\n",
    "    \n",
    "    These activations are saved as individual batch files in a specified pickle directory, \n",
    "    allowing efficient incremental storage and later aggregation.\n",
    "    Alternatively, the representations can be returned directly.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : PreTrainedModel\n",
    "        The causal language model to evaluate (e.g., LLaMA).\n",
    "    tokenizer : PreTrainedTokenizer\n",
    "        The corresponding tokenizer.\n",
    "    dataset : Dataset\n",
    "        The input dataset.\n",
    "    batch_size : int\n",
    "        Number of samples per batch.\n",
    "    idx_start_sample : int\n",
    "        Index of the first sample to process from the dataset.\n",
    "    max_samples : int\n",
    "        Total number of examples to process from the dataset, starting from idx_start_sample. \n",
    "    save_to_pkl : bool\n",
    "        If True, activations are appended to the pickle file at output_path.\n",
    "        If False, the function returns a list of activations.\n",
    "    output_path : str\n",
    "        Path to the directory where extracted answers will be saved as individual pickle batch files.\n",
    "    build_prompt_fn : Callable\n",
    "        Function to build a prompt from context and question.\n",
    "    layers : List[int]\n",
    "        List of indices of the transformer layers to extract activations from (default: [-1] for last layer).\n",
    "    activation_source : {\"prompt\", \"generation\", \"promptGeneration\"}\n",
    "        Which part of the sequence to extract activations/attentions/logits from:\n",
    "        - \"prompt\": only from the prompt\n",
    "        - \"generation\": only from the generated answer\n",
    "        - \"promptGeneration\": prompt and generation answer both concatenated\n",
    "    hidden_scores : List[str], optional\n",
    "        List of aggregation modes to compute on token activations. Possible modes include:\n",
    "            \"average\", \"last\", \"max\", \"first_generated\", \"token_svd_score\", \"feat_var\".\n",
    "        These modes are passed to `extract_token_activations` for aggregation. Default includes the above.\n",
    "    attn_scores : List[str], optional\n",
    "        List of attention-based scores to compute. Supported: \"attn_eig_prod\".\n",
    "    logit_scores : List[str], optional\n",
    "        List of logit-based scores to compute. Supported:\n",
    "            \"perplexity\", \"logit_entropy\", \"window_logit_entropy\".\n",
    "    logit_config : dict, optional\n",
    "        Configuration dictionary for logit-based scoring functions, with keys such as:\n",
    "            - \"top_k\": int, number of top logits considered (default 50)\n",
    "            - \"window_size\": int, window size for windowed entropy (default 1)\n",
    "            - \"stride\": int, stride for windowed entropy (default 1)\n",
    "    start_offset : int\n",
    "        Offset from the first non-padding token (must be >= 0). \n",
    "    end_offset : int\n",
    "        Offset from the last non-padding token (must be <= 0, e.g., -3 to remove 3 tokens).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Union[List[dict], None]\n",
    "        If `save_to_pkl` is False, returns a list of dictionaries, one per batch, with each element\n",
    "         of the list having the following structure:\n",
    "            {\n",
    "                \"id\": List[str],  # IDs of batch samples\n",
    "                \"original_indices\": List[int],  # Original dataset indices\n",
    "                \"context\": List[str],\n",
    "                \"question\": List[str],\n",
    "                \"gt_answers\": List[str],        # Ground-truth reference answers\n",
    "                \"gen_answers\": List[str],       # Generated model answers\n",
    "                \"scores\": {\n",
    "                    \"layer_{layer_idx}\": {\n",
    "                        \"hidden\": { \n",
    "                            \"{mode}\": np.ndarray[(batch_size, hidden_size), float], \n",
    "                            ... # one entry per mode in hidden_scores\n",
    "                        },\n",
    "                        \"attention\": {\n",
    "                            \"{attn_score}\": np.ndarray[(batch_size,), float],  \n",
    "                            ...\n",
    "                        }\n",
    "                    },\n",
    "                    \"logits\": {\n",
    "                        \"perplexity\": np.ndarray[(batch_size,), float],\n",
    "                        \"logit_entropy\": np.ndarray[(batch_size,), float],\n",
    "                        \"window_logit_entropy\": np.ndarray[(batch_size,), float] \n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "\n",
    "        If `save_to_pkl` is True, saves each batch's dictionary incrementally to disk and returns None.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    When using model.generate() with output_hidden_states=True (what we are replicating here with the ,\n",
    "    activation hook) use_cache=True and max_new_tokens=30, there is always an offset between the length of the \n",
    "    generated sequence (outputs.sequences.shape[1][prompt_len:]) and the length of len(outputs.hidden_states) : \n",
    "    * outputs.sequences.shape[1] = prompt_len (17) + max_new_tokens (30) = 47\n",
    "    * len(outputs.hidden_states) = max_new_tokens (30)\n",
    "        With : \n",
    "        * outputs.hidden_states[0][layer_idx].shape = (batch_size, prompt_len, hidden_size)           --> includes the prompt ! \n",
    "        * outputs.hidden_states[i][layer_idx].shape = (batch_size, 1, hidden_size) with 1 <= i <= 29  --> stops at 29 ! \n",
    "    *Note* that in our code, outputs.hidden_states and activations are the same. \n",
    "        \n",
    "    Explanation from Hugging Face, April 2024 \n",
    "    (https://github.com/huggingface/transformers/issues/30036):\n",
    "    \"\"\"\n",
    "\n",
    "    if activation_source not in ('prompt', 'generation', 'promptGeneration'):\n",
    "        raise ValueError(\n",
    "                f\"Invalid value for `activation_source`: '{activation_source}'. \"\n",
    "                f\"Expected one of: ['prompt', 'generation', 'promptGeneration'].\"\n",
    "            )    \n",
    "        \n",
    "    # ==============================\n",
    "    # Patch selected layer(s) with custom LlamaAttention Forward function to retrieve attention weights\n",
    "    # ==============================\n",
    "    for idx in layers:  \n",
    "        model.model.layers[idx].self_attn.forward = patched_LlamaAttention_forward.__get__(\n",
    "            model.model.layers[idx].self_attn,\n",
    "            model.model.layers[idx].self_attn.__class__\n",
    "    )\n",
    "        \n",
    "    all_batch_results = []  \n",
    "\n",
    "    for i in tqdm(range(idx_start_sample, idx_start_sample + max_samples, batch_size)):\n",
    "        print(f\"============ {i} ============\")\n",
    "        \n",
    "    \n",
    "        # ==============================\n",
    "        # Prepare input batch\n",
    "        # ==============================\n",
    "        batch = extract_batch(dataset, i, batch_size)\n",
    "        prompts = [build_prompt_fn(s[\"context\"], s[\"question\"]) for s in batch]\n",
    "        inputs = tokenizer(prompts, padding=True, truncation=True, return_tensors=\"pt\").to(model.device)\n",
    "        prompt_ids = inputs[\"input_ids\"] # (batch_size, prompt_len)\n",
    "        prompt_len = prompt_ids.shape[1] # Assumes prompts are padded to same length\n",
    "\n",
    "        print(f\"[INFO] prompt_ids shape: {prompt_ids.shape}, prompt_len: {prompt_len}\")\n",
    "\n",
    "        # ==============================\n",
    "        # Register forward hook to capture layer output\n",
    "        # ==============================\n",
    "        # This hook collects the hidden states at each decoding step. For layer l: \n",
    "        # activations_lists[l] = [act_prompt, act_gen_step1, ..., act_gen_step49] of length 50, if max_new_tokens=50.\n",
    "        # activations_lists[l][k] of shape: (batch_size, seq_len, hidden_size) \n",
    "        activations_lists = [[] for _ in layers]  # one empty list per layer \n",
    "        handle_act, call_counter_act = register_generation_activation_hook(model, activations_lists, layers)\n",
    "\n",
    "        # This hook collects the activations at each decoding step. For layer l: \n",
    "        # attentions_lists[l] = [attn_prompt, attn_gen_step1, ..., attn_gen_step49], of length 50, if max_new_tokens=50.\n",
    "        # activations_lists[l][k] of shape: (batch_size, n_heads, tgt_seq_len, src_seq_len)\n",
    "        #   tgt_seq_len: length of the sequence the model is currently producing (query)\n",
    "        #   src_seq_len: length of the sequence the model is focusing on (key/value)\n",
    "        attentions_lists = [[] for _ in layers]  # one empty list per layer\n",
    "        handle_attn, call_counter_attn = register_generation_attention_hook(model, attentions_lists, layers)\n",
    "        \n",
    "        # ==============================\n",
    "        # Run model generation (hook captures activations and attentions)\n",
    "        # ==============================\n",
    "        # When target layers are reached, hooks execute and saves their output in activations and attentions\n",
    "        print(\"[INFO] Starting generation...\")\n",
    "        outputs = generate(model, inputs, tokenizer, max_new_tokens=50, k_beams=1)\n",
    "        gen_ids = outputs.sequences[:, prompt_len:]\n",
    "        print(f\"[INFO] gen_ids shape: {gen_ids.shape}\")\n",
    "        print(f\"[INFO] Sample generated tokens: {gen_ids}\")\n",
    "\n",
    "        # Remove hooks to avoid memory leaks or duplicate logging\n",
    "        for h in handle_act: h.remove()\n",
    "        for h in handle_attn: h.remove()\n",
    "        \n",
    "        # Verify that hooks worked properly\n",
    "        verify_call_counters(call_counter_act, name=\"activation hooks\")\n",
    "        verify_call_counters(call_counter_attn, name=\"attention hooks\")\n",
    "\n",
    "        # Retrieve text of generated answers\n",
    "        gen_answers = tokenizer.batch_decode(\n",
    "            outputs.sequences[:, prompt_len:], \n",
    "            skip_special_tokens=True\n",
    "        ) # (batch_size,)\n",
    "        ###print(f\"[INFO] Sample generated answer: {gen_answers}\")\n",
    "\n",
    "  \n",
    "\n",
    "        # ===============================\n",
    "        # Build generation and prompt attention mask\n",
    "        # ===============================\n",
    "        # This mask marks which generated tokens are valid (i.e., not padding).\n",
    "        # Positions are marked True up to and including the first eos_token_id\n",
    "        generation_attention_mask = build_generation_attention_mask(\n",
    "            gen_ids=gen_ids, \n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        ) # (batch_size, gen_len)\n",
    "\n",
    "        prompt_attention_mask = inputs[\"attention_mask\"] \n",
    "        # (batch_size, prompt_len)\n",
    "\n",
    "        ###print(f\"[INFO] prompt_attention_mask: {prompt_attention_mask.shape}\")\n",
    "        ###print(f\"[INFO] generation_attention_mask: {generation_attention_mask.shape}\")\n",
    "        ###print(f\"[INFO] generation_attention_mask: {generation_attention_mask}\")\n",
    "\n",
    "        # Modify prompt attention mask with offsets\n",
    "        if start_offset !=0 or end_offset !=0:\n",
    "            ###print(f\"[INFO] Offsetting prompt_attention_mask with start={start_offset}, end={end_offset}\")\n",
    "            prompt_attention_mask, start_indices, end_indices = compute_offset_attention_mask(\n",
    "                attention_mask=prompt_attention_mask, \n",
    "                start_offset=start_offset, \n",
    "                end_offset=end_offset\n",
    "            ) # (batch_size, prompt_len), (batch_size,), (batch_size,)\n",
    "\n",
    "        ###print(f\"[INFO] New prompt_attention_mask shape: {prompt_attention_mask.shape}\")\n",
    "        ###print(f\"[INFO] New prompt_attention_mask : {prompt_attention_mask}\")\n",
    "\n",
    "        # Concatenate the prompt and generation attention mask\n",
    "        prompt_and_gen_attention_mask = torch.cat(\n",
    "            [prompt_attention_mask,\n",
    "            generation_attention_mask],\n",
    "            dim=1\n",
    "        ) # (batch_size, prompt_len + gen_len)\n",
    "\n",
    "        ###print(f\"[INFO] prompt_and_gen_attention_mask shape: {prompt_and_gen_attention_mask.shape}\")\n",
    "        ###print(f\"[INFO] prompt_and_gen_attention_mask : {prompt_and_gen_attention_mask}\")\n",
    "\n",
    "        # ===============================\n",
    "        # Truncate generated token IDs and mask to match activations and attentions\n",
    "        # ===============================\n",
    "        # When N tokens are generated, only the first N-1 tokens have corresponding hidden states.\n",
    "        # So activations[1:] covers only the first N-1 steps. Therefore, we exclude the last\n",
    "        # generated token from outputs.sequences to match activations[1:]. Same for attentions.\n",
    "        truncated_gen_ids = gen_ids[:,:-1] # (gen_len-1,)\n",
    "        truncated_generation_attention_mask = generation_attention_mask[:,:-1] # (batch_size, gen_len-1)\n",
    "        truncated_prompt_and_gen_attention_mask = prompt_and_gen_attention_mask[:,:-1] # (batch_size, prompt_len + gen_len-1)\n",
    "\n",
    "        ###print(f\"[INFO] Truncated gen_ids shape: {truncated_gen_ids.shape}\")\n",
    "        ###print(f\"[INFO] Truncated generation_attention_mask shape: {truncated_generation_attention_mask.shape}\")\n",
    "        ###print(f\"[INFO] Truncated prompt_and_gen_attention_mask shape: {truncated_prompt_and_gen_attention_mask.shape}\")\n",
    "        ###print(f\"[INFO] Truncated gen_ids : {truncated_gen_ids}\")\n",
    "        ###print(f\"[INFO] Truncated generation_attention_mask : {truncated_generation_attention_mask}\")\n",
    "        ###print(f\"[INFO] Truncated prompt_and_gen_attention_mask : {truncated_prompt_and_gen_attention_mask}\")\n",
    "\n",
    "        # *******************************\n",
    "        # START: loop on layers\n",
    "        # *******************************\n",
    "        # Final dictionary on all layers \n",
    "        save_layers_scores = {}\n",
    "\n",
    "        for l, layer_idx in enumerate(layers):\n",
    "            print(f\"\\n----- Layer {layer_idx} -----\")\n",
    "\n",
    "            activations = activations_lists[l]\n",
    "            attentions = attentions_lists[l]\n",
    "\n",
    "            print(\"============\")\n",
    "            print(\"[INFO] Length of activations:\", len(activations))\n",
    "            for i in range(len(activations)):\n",
    "                print(f\"[INFO] Shape  of activations[{i}]: {activations[i].shape}\") \n",
    "            print(\"============\")\n",
    "            print(\"[INFO] Length of attentions:\", len(attentions))\n",
    "            for i in range(len(attentions)):\n",
    "                print(f\"[INFO] Shape  of attentions[{i}]: {attentions[i].shape}\") \n",
    "            print(\"============\")\n",
    "\n",
    "            # Define prompt and generation hidden states \n",
    "            prompt_activations=activations[0]       # `[0]` to include only the prompt part \n",
    "            generation_activations=activations[1:]  # `[1:]` to exclude the prompt part \n",
    "            \n",
    "            # Define prompt and generation attention maps\n",
    "            prompt_attentions=attentions[0]         # `[0]` to include only the prompt part \n",
    "            generation_attentions=attentions[1:]    # `[1:]` to exclude the prompt part \n",
    "\n",
    "            ###print(f\"[DEBUG] prompt_activations shape: {prompt_activations.shape}\")\n",
    "            ###print(f\"[DEBUG] prompt_attentions shape: {prompt_attentions.shape}\")\n",
    "\n",
    "            # ===============================\n",
    "            # Align generated and prompt hidden states\n",
    "            # ===============================\n",
    "            # For each batch item, take the last generated hidden state at this step\n",
    "            stacked_generation_activations = torch.stack(\n",
    "                [h[:, -1, :] for h in generation_activations], dim=1\n",
    "            ) # (batch_size, gen_len, hidden_size)\n",
    "\n",
    "            ###print(f\"[DEBUG] stacked_generation_activations shape: {stacked_generation_activations.shape}\")\n",
    "\n",
    "            # Concatenate the prompt and generation aligned hidden states  \n",
    "            prompt_and_gen_activations = torch.cat(\n",
    "                [stacked_generation_activations, # (batch_size, gen_len, hidden_size)\n",
    "                prompt_activations],             # (batch_size, prompt_len, hidden_size)\n",
    "                dim=1\n",
    "            ) # (batch_size, prompt_len + gen_len, hidden_size)\n",
    "            \n",
    "            ###print(f\"[DEBUG] prompt_and_gen_activations shape: {prompt_and_gen_activations.shape}\")\n",
    "\n",
    "            # ==============================\n",
    "            # Extract token activations from captured layer, based on source\n",
    "            # ==============================\n",
    "            ###print(f\"[INFO] Activation source: {activation_source}\")\n",
    "            \n",
    "            if hidden_scores is not None and len(hidden_scores) > 0:\n",
    "                if activation_source == \"generation\":\n",
    "                    # Return only the token activations from the generated answer \n",
    "                    selected_token_vecs = extract_token_activations(                 ##### extract_token_activations_fn \n",
    "                            selected_layer=stacked_generation_activations, \n",
    "                            attention_mask=truncated_generation_attention_mask, \n",
    "                            device=stacked_generation_activations.device,\n",
    "                            modes=hidden_scores,\n",
    "                        ) # (batch_size, hidden_size)\n",
    "                    \n",
    "                elif activation_source == \"prompt\":    \n",
    "                    # Return only the token activations from the prompt\n",
    "                    selected_token_vecs = extract_token_activations(\n",
    "                            selected_layer=prompt_activations, \n",
    "                            attention_mask=prompt_attention_mask, \n",
    "                            device=prompt_activations.device,\n",
    "                            modes=hidden_scores,\n",
    "                        ) # (batch_size, hidden_size)\n",
    "                    \n",
    "                else: # activation_source == \"promptGeneration\"\n",
    "                    # Return token activations from the concatenated prompt + generated answer \n",
    "                    selected_token_vecs = extract_token_activations(\n",
    "                            selected_layer=prompt_and_gen_activations, \n",
    "                            attention_mask=truncated_prompt_and_gen_attention_mask, \n",
    "                            device=prompt_and_gen_activations.device,\n",
    "                            skip_length=prompt_len,\n",
    "                            modes=hidden_scores,\n",
    "                            # skip_length: exclude prompt from computation if \n",
    "                            # mode=='first_generated' in `extract_token_activations_fn`\n",
    "                        ) # (batch_size, hidden_size)\n",
    "                \n",
    "                ###print(f\"[RESULT] selected_token_vecs sample:\\n{selected_token_vecs}\")\n",
    "                #result.update(selected_token_vecs) \n",
    "                hidden_results = {}\n",
    "                for mode in hidden_scores:\n",
    "                    if mode in selected_token_vecs:\n",
    "                        hidden_results[mode] = selected_token_vecs[mode].cpu().numpy() #[vec.cpu().numpy() for vec in selected_token_vecs[mode]] \n",
    "                    #else:\n",
    "                    #    hidden_results[mode] = None \n",
    "                #layers_scores[f\"layer_{layer_idx}\"] = {\"hidden\": hidden_results}\n",
    "                save_layers_scores.setdefault(f\"layer_{layer_idx}\", {}).update({\"hidden\": hidden_results})\n",
    "\n",
    "\n",
    "        \n",
    "            # ==============================\n",
    "            # Extract attention eigen score\n",
    "            # ==============================\n",
    "            #attn_eig_prod = None\n",
    "            if attn_scores is not None and 'attn_eig_prod' in attn_scores:\n",
    "                attn_eig_prod = compute_attn_eig_prod(\n",
    "                        prompt_attentions=prompt_attentions, \n",
    "                        generation_attentions=generation_attentions,\n",
    "                        prompt_attention_mask=prompt_attention_mask, \n",
    "                        generation_attention_mask=truncated_generation_attention_mask,\n",
    "                        mode=activation_source,\n",
    "                )\n",
    "                ###print(f\"[RESULT] attn_eig_prod:\\n{attn_eig_prod}\")\n",
    "\n",
    "                # Store result in dict\n",
    "                #layers_scores[f\"layer_{layer_idx}\"] = {\"attention\": {\"attn_eig_prod\": attn_eig_prod}}\n",
    "                save_layers_scores.setdefault(f\"layer_{layer_idx}\", {}).update({\"attention\": {\"attn_eig_prod\": attn_eig_prod}})\n",
    "            '''\n",
    "            layers_scores[f\"layer_{layer_idx}\"] = {\n",
    "            \"hidden\": hidden_results,\n",
    "            \"attention\": {\"attn_eig_prod\": attn_eig_prod } #if attn_eig_prod is not None else None}\n",
    "            }\n",
    "            '''\n",
    "\n",
    "            # ==============================\n",
    "            # Extract logits scores\n",
    "            # ==============================\n",
    "            # if this is the last layer, use regular way to compute logits \n",
    "            # since there is small differences when computing prompt activations from forward pass and prompt activations\n",
    "            # from model.generate() resulting in different logits. \n",
    "            if logit_scores is not None and len(logit_scores) > 0: \n",
    "                logits_results = {}\n",
    "                if layer_idx != -1 and layer_idx != model.config.num_hidden_layers -1:\n",
    "                    with torch.no_grad():\n",
    "                        prompt_logits = apply_logit_lens(model, prompt_activations) # (batch, prompt_len, vocab_size)\n",
    "                        gen_logits = apply_logit_lens(model, stacked_generation_activations) # (batch, gen_len-1, vocab_size)\n",
    "                    # First gen_logits is missing (shape gen_len-1) -> use `prepend_last_prompt_logit=True`\n",
    "                    # to retrive first generated logit (see spec of function `compute_perplexity` for more details)\n",
    "                    prepend_last_prompt_logit = True\n",
    "                else: #last layer\n",
    "                    # ==============================\n",
    "                    # Forward pass to the model to retrieve prompt logits \n",
    "                    # ==============================\n",
    "                    with torch.no_grad():\n",
    "                        prompt_logits = model(input_ids=inputs[\"input_ids\"]).logits # (batch, prompt_len, vocab_size)\n",
    "                    gen_logits = torch.stack(outputs.logits, dim=1)  # (batch, gen_len, vocab_size)\n",
    "                    prepend_last_prompt_logit = False\n",
    "\n",
    "                print(f\"[INFO] prompt_logits.shape: {prompt_logits.shape}\")\n",
    "                print(f\"[INFO] gen_logits.shape: {gen_logits.shape}\")\n",
    "\n",
    "                if 'perplexity' in logit_scores:\n",
    "                    perplexity = compute_perplexity(\n",
    "                        prompt_logits=prompt_logits, \n",
    "                        gen_logits=gen_logits,\n",
    "                        prompt_ids=prompt_ids, \n",
    "                        gen_ids=gen_ids,\n",
    "                        prompt_attention_mask=prompt_attention_mask,\n",
    "                        gen_attention_mask=generation_attention_mask,\n",
    "                        prepend_last_prompt_logit=prepend_last_prompt_logit,\n",
    "                        mode=activation_source,\n",
    "                        min_k=None\n",
    "                    )\n",
    "                    logits_results['perplexity'] = perplexity\n",
    "\n",
    "                if 'logit_entropy' in logit_scores:\n",
    "                    if logit_config is None:\n",
    "                        raise ValueError(\"logit_entropy is required but logit_config is None\")\n",
    "                    logit_entropy = compute_logit_entropy(\n",
    "                        prompt_logits=prompt_logits,\n",
    "                        gen_logits=gen_logits,\n",
    "                        prompt_attention_mask=prompt_attention_mask,\n",
    "                        gen_attention_mask=generation_attention_mask,\n",
    "                        mode=activation_source,\n",
    "                        prepend_last_prompt_logit=prepend_last_prompt_logit,\n",
    "                        top_k=logit_config['top_k'],\n",
    "                        window_size=None,\n",
    "                        stride=None\n",
    "                    )\n",
    "                    logits_results['logit_entropy'] = logit_entropy\n",
    "\n",
    "                if 'window_logit_entropy' in logit_scores:\n",
    "                    if logit_config is None:\n",
    "                        raise ValueError(\"window_logit_entropy is required but logit_config is None\")\n",
    "                    window_logit_entropy = compute_logit_entropy(\n",
    "                        prompt_logits=prompt_logits,\n",
    "                        gen_logits=gen_logits,\n",
    "                        prompt_attention_mask=prompt_attention_mask,\n",
    "                        gen_attention_mask=generation_attention_mask,\n",
    "                        mode=activation_source,\n",
    "                        prepend_last_prompt_logit=prepend_last_prompt_logit,\n",
    "                        top_k=logit_config['top_k'], # default 50\n",
    "                        window_size=logit_config['window_size'], # default 1\n",
    "                        stride=logit_config['stride'] # default 1\n",
    "                    )\n",
    "                    logits_results['window_logit_entropy'] = window_logit_entropy\n",
    "\n",
    "                if logits_results:\n",
    "                    save_layers_scores.setdefault(f\"layer_{layer_idx}\", {}).update({\"logits\": logits_results})\n",
    "\n",
    "        # *******************************\n",
    "        # END: loop on layers\n",
    "        # *******************************\n",
    "\n",
    "        '''\n",
    "        save_logits_scores = {}\n",
    "        #perplexity = None; logit_entropy=None; window_logit_entropy=None\n",
    "        # if \n",
    "        if logit_scores is not None:\n",
    "            if 'perplexity' in logit_scores:\n",
    "                perplexity = compute_perplexity(\n",
    "                    prompt_logits=prompt_logits, \n",
    "                    gen_logits=gen_logits,\n",
    "                    prompt_ids=prompt_ids, \n",
    "                    gen_ids=gen_ids,\n",
    "                    prompt_attention_mask=prompt_attention_mask,\n",
    "                    gen_attention_mask=generation_attention_mask,\n",
    "                    mode=activation_source,\n",
    "                    min_k=None\n",
    "                )\n",
    "                ###print(f\"[RESULT] perplexity:\\n{perplexity}\")\n",
    "                save_logits_scores['perplexity'] = perplexity #if perplexity is not None else None\n",
    "\n",
    "            if 'logit_entropy' in logit_scores:\n",
    "                if logit_config is None:\n",
    "                    raise ValueError(\"logit_entropy is required but logit_config is None\")\n",
    "                logit_entropy = compute_logit_entropy(\n",
    "                    prompt_logits=prompt_logits,\n",
    "                    gen_logits=gen_logits,\n",
    "                    prompt_attention_mask=prompt_attention_mask,\n",
    "                    gen_attention_mask=generation_attention_mask,\n",
    "                    mode=activation_source,\n",
    "                    top_k=logit_config['top_k'], # default 50,\n",
    "                    window_size=None,\n",
    "                    stride=None\n",
    "                )\n",
    "                ###print(f\"[RESULT] logit_entropy:\\n{logit_entropy}\")\n",
    "                save_logits_scores['logit_entropy'] = logit_entropy #if logit_entropy is not None else None\n",
    "        \n",
    "            if 'window_logit_entropy' in logit_scores:\n",
    "                if logit_config is None:\n",
    "                    raise ValueError(\"window_logit_entropy is required but logit_config is None\")\n",
    "                window_logit_entropy = compute_logit_entropy(\n",
    "                    prompt_logits=prompt_logits,\n",
    "                    gen_logits=gen_logits,\n",
    "                    prompt_attention_mask=prompt_attention_mask,\n",
    "                    gen_attention_mask=generation_attention_mask,\n",
    "                    mode=activation_source,\n",
    "                    top_k=logit_config['top_k'], # default 50\n",
    "                    window_size=logit_config['window_size'], # default 1\n",
    "                    stride=logit_config['stride'] # default 1\n",
    "                )\n",
    "                ###print(f\"[RESULT] window_logit_entropy:\\n{window_logit_entropy}\")\n",
    "                save_logits_scores['window_logit_entropy'] = window_logit_entropy #if window_logit_entropy is not None else None\n",
    "        '''\n",
    "        #print(f\"[RESULT] FINAL RESULT:\\n{result}\")\n",
    "        #print(f\"[RESULT] FINAL logits_scores:\\n{logits_scores}\")\n",
    "\n",
    "        # ==============================\n",
    "        # Store results (to file or memory)\n",
    "        # ==============================\n",
    "        batch_results = {\n",
    "            \"id\": [s['id'] for s in batch],\n",
    "            \"original_indices\": [s['original_index'] for s in batch],\n",
    "            \"context\": [s['context'] for s in batch],\n",
    "            \"question\": [s['question'] for s in batch],\n",
    "            \"gt_answers\": [s['answers'] for s in batch],\n",
    "            \"gen_answers\": gen_answers,\n",
    "            \"scores\": {**save_layers_scores} #, **({\"logits\": save_logits_scores} if save_logits_scores else {})}\n",
    "        }\n",
    "\n",
    "        from src.data_reader.pickle_io import save_batch_pickle\n",
    "\n",
    "        if save_to_pkl:\n",
    "            #append_to_pickle(output_path, batch_results)\n",
    "            save_batch_pickle(batch_data=batch_results, output_dir=output_path, batch_idx=i)\n",
    "        else:\n",
    "            all_batch_results.append(batch_results)\n",
    "\n",
    "    if not save_to_pkl:\n",
    "        return all_batch_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ 0 ============\n",
      "[INFO] prompt_ids shape: torch.Size([2, 281]), prompt_len: 281\n",
      "[INFO] Starting generation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] gen_ids shape: torch.Size([2, 10])\n",
      "[INFO] Sample generated tokens: tensor([[29871,   443, 12011,   519,     2,     2,     2,     2,     2,     2],\n",
      "        [29871,  6106,   292,   322,  6025,  3277,  5100,  2187, 29889,     2]],\n",
      "       device='cuda:0')\n",
      "\n",
      "----- Layer 18 -----\n",
      "============\n",
      "[INFO] Length of activations: 10\n",
      "[INFO] Shape  of activations[0]: torch.Size([2, 281, 4096])\n",
      "[INFO] Shape  of activations[1]: torch.Size([2, 1, 4096])\n",
      "[INFO] Shape  of activations[2]: torch.Size([2, 1, 4096])\n",
      "[INFO] Shape  of activations[3]: torch.Size([2, 1, 4096])\n",
      "[INFO] Shape  of activations[4]: torch.Size([2, 1, 4096])\n",
      "[INFO] Shape  of activations[5]: torch.Size([2, 1, 4096])\n",
      "[INFO] Shape  of activations[6]: torch.Size([2, 1, 4096])\n",
      "[INFO] Shape  of activations[7]: torch.Size([2, 1, 4096])\n",
      "[INFO] Shape  of activations[8]: torch.Size([2, 1, 4096])\n",
      "[INFO] Shape  of activations[9]: torch.Size([2, 1, 4096])\n",
      "============\n",
      "[INFO] Length of attentions: 10\n",
      "[INFO] Shape  of attentions[0]: torch.Size([2, 32, 281, 281])\n",
      "[INFO] Shape  of attentions[1]: torch.Size([2, 32, 1, 282])\n",
      "[INFO] Shape  of attentions[2]: torch.Size([2, 32, 1, 283])\n",
      "[INFO] Shape  of attentions[3]: torch.Size([2, 32, 1, 284])\n",
      "[INFO] Shape  of attentions[4]: torch.Size([2, 32, 1, 285])\n",
      "[INFO] Shape  of attentions[5]: torch.Size([2, 32, 1, 286])\n",
      "[INFO] Shape  of attentions[6]: torch.Size([2, 32, 1, 287])\n",
      "[INFO] Shape  of attentions[7]: torch.Size([2, 32, 1, 288])\n",
      "[INFO] Shape  of attentions[8]: torch.Size([2, 32, 1, 289])\n",
      "[INFO] Shape  of attentions[9]: torch.Size([2, 32, 1, 290])\n",
      "============\n",
      "[INFO] prompt_logits.shape: torch.Size([2, 281, 32000])\n",
      "[INFO] gen_logits.shape: torch.Size([2, 9, 32000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Clear memory to avoid \"CUDA out of memory\"\n",
    "# -----------------------------------\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "\n",
    "start_offset = 0\n",
    "end_offset = 0\n",
    "result = run_prompt_and_generation_score_extraction(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=id_fit_dataset,\n",
    "    batch_size=2,\n",
    "    idx_start_sample=0,\n",
    "    max_samples=2,\n",
    "    save_to_pkl = False,\n",
    "    output_path = OUTPUT_DIR + \"all_batch_resultsTEST.pkl\",\n",
    "    build_prompt_fn=build_prompt,\n",
    "    layers = [18],  \n",
    "    activation_source = \"promptGeneration\",\n",
    "    hidden_scores=[\"average\", \"last\", \"max\", \"first_generated\", \"token_svd_score\", \"feat_var\"],\n",
    "    attn_scores=[\"attn_eig_prod\"],\n",
    "    logit_scores=[\"perplexity\",\"logit_entropy\", \"window_logit_entropy\"],\n",
    "    logit_config={\"top_k\": 50, \"window_size\": 1, \"stride\": 1},\n",
    "    start_offset = start_offset,\n",
    "    end_offset = end_offset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': ['56be85543aeaaa14008c9063', '56be85543aeaaa14008c9065'],\n",
       "  'original_indices': [0, 1],\n",
       "  'context': ['Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".',\n",
       "   'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".'],\n",
       "  'question': ['When did Beyonce start becoming popular?',\n",
       "   'What areas did Beyonce compete in when she was growing up?'],\n",
       "  'gt_answers': [{'answer_start': 269, 'text': 'in the late 1990s'},\n",
       "   {'answer_start': 207, 'text': 'singing and dancing'}],\n",
       "  'gen_answers': [' unanswerable', ' Singing and dancing competitions.'],\n",
       "  'scores': {'layer_18': {'hidden': {'average': array([[-0.09061667, -0.18593608,  0.08875383, ..., -0.0436042 ,\n",
       "             -0.03706677,  0.03211577],\n",
       "            [-0.08099116, -0.13936505,  0.09960753, ..., -0.04558053,\n",
       "             -0.06095891,  0.02628676]], dtype=float32),\n",
       "     'last': array([[-0.5137 ,  0.756  ,  0.322  , ..., -0.673  , -0.3308 , -0.06866],\n",
       "            [-1.063  , -0.769  ,  0.4219 , ..., -0.536  , -0.2406 ,  0.6763 ]],\n",
       "           dtype=float16),\n",
       "     'max': array([[1.2480469, 1.7958984, 1.3808594, ..., 1.5712891, 1.9023438,\n",
       "             1.5634766],\n",
       "            [1.2480469, 1.7958984, 1.3779297, ..., 1.5712891, 1.9013672,\n",
       "             1.5615234]], dtype=float32),\n",
       "     'first_generated': array([[ 0.4077  ,  0.1605  ,  0.12085 , ...,  0.002045,  0.231   ,\n",
       "             -0.5107  ],\n",
       "            [-0.11    ,  0.75    , -0.2834  , ...,  0.1133  , -0.3132  ,\n",
       "             -0.4797  ]], dtype=float16),\n",
       "     'token_svd_score': array([5.937284, 6.223494], dtype=float32),\n",
       "     'feat_var': array([[0.21548717, 0.26665634, 0.23205729, ..., 0.19720535, 0.19079132,\n",
       "             0.14846477],\n",
       "            [0.2127446 , 0.31783444, 0.24079652, ..., 0.19848394, 0.20921347,\n",
       "             0.1560042 ]], dtype=float32)},\n",
       "    'attention': {'attn_eig_prod': array([-154.8, -154.6], dtype=float16)},\n",
       "    'logits': {'perplexity': array([16434.578, 15388.555], dtype=float32)}}}}]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OK ###\n",
    "from typing import Any\n",
    "import glob\n",
    "def load_and_merge_pickles(directory: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Load and recursively merge all batch pickle files from a directory into a single results dictionary.\n",
    "\n",
    "    Each pickle file must contain a dictionary with the same structure across batches.\n",
    "\n",
    "    The function merges nested dictionaries, concatenating values along the batch axis.\n",
    "    Supported merge strategies:\n",
    "        - Lists are extended (concatenated).\n",
    "        - NumPy arrays are concatenated along the first dimension (axis=0).\n",
    "        - Nested dictionaries are merged recursively.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    directory : str\n",
    "        Path to the directory containing batch pickle files. Files must match the pattern '*.pkl'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, Any]\n",
    "        A recursively merged dictionary where:\n",
    "            - Leaf values are lists or arrays aggregated from all batches.\n",
    "            - Nested dictionaries (e.g., \"scores\" → \"layer_0\" → \"hidden\") are merged in depth.\n",
    "    \"\"\"\n",
    "    def recursive_merge(dest: dict, src: dict):\n",
    "        for key, value in src.items():\n",
    "            if key not in dest:\n",
    "                dest[key] = value if not isinstance(value, dict) else recursive_merge({}, value)\n",
    "            else:\n",
    "                if isinstance(value, dict) and isinstance(dest[key], dict):\n",
    "                    recursive_merge(dest[key], value)\n",
    "                elif isinstance(value, list):\n",
    "                    dest[key].extend(value)\n",
    "                elif hasattr(value, 'shape'):  # numpy array\n",
    "                    import numpy as np\n",
    "                    dest[key] = np.concatenate([dest[key], value], axis=0)\n",
    "                else:\n",
    "                    raise ValueError(f\"Cannot merge key '{key}' with type {type(value)}\")\n",
    "\n",
    "        return dest\n",
    "\n",
    "    merged = {}\n",
    "    files = sorted(glob.glob(os.path.join(directory, \"*.pkl\")))\n",
    "    for file in files:\n",
    "        with open(file, \"rb\") as f:\n",
    "            batch = pickle.load(f)\n",
    "            recursive_merge(merged, batch)\n",
    "\n",
    "    return merged\n",
    "\n",
    "from src.data_reader.pickle_io import save_merged_pickle, load_pickle_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = OUTPUT_DIR + \"all_batch_resultsTEST.pkl\"\n",
    "merged = load_and_merge_pickles(dir)\n",
    "save_merged_pickle(merged, dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prompt_score_extraction(\n",
    "    model: PreTrainedModel,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    dataset: Dataset,\n",
    "    batch_size: int = 4,\n",
    "    idx_start_sample: int = 0,\n",
    "    max_samples: int = 1000,\n",
    "    save_to_pkl: bool = False,\n",
    "    output_path: str = \"outputs/all_batch_results.pkl\",\n",
    "    build_prompt_fn: Callable[[str, str], str] = None,\n",
    "    layers: List[int] = [-1],  \n",
    "    hidden_scores: List[str] = [\"average\", \"last\", \"max\", \"first_generated\", \"token_svd_score\", \"feat_var\"],\n",
    "    attn_scores: List[str] = [\"attn_eig_prod\"],\n",
    "    logit_scores: List[str] = [\"perplexity\", \"logit_entropy\", \"window_logit_entropy\"],\n",
    "    logit_config: dict = {\"top_k\": 50, \"window_size\": 1, \"stride\": 1},\n",
    "    start_offset : int = 0,\n",
    "    end_offset : int = 0,\n",
    ") -> Union[List[torch.Tensor], None]:\n",
    "    \"\"\"\n",
    "    Runs batched inference on a dataset using a decoder-only language model.\n",
    "    For each batch, it runs a forward pass on the prompt and extracts token-level hidden \n",
    "    activations, attention maps and logit scores from specified transformer layers.\n",
    "\n",
    "    The function supports multiple aggregation modes for the activations (`hidden_scores`), attention-based \n",
    "    scores (`attn_scores`), and logit-based scores (`logit_scores`). The `logit_config` argument provides \n",
    "    configuration parameters for logit-based score functions.\n",
    "    \n",
    "    Hidden states and attention maps are captured via forward hooks, \n",
    "    then aggregated based on token position and attention masks.\n",
    "    \n",
    "    These activations are saved as individual batch files in a specified pickle directory, \n",
    "    allowing efficient incremental storage and later aggregation.\n",
    "    Alternatively, the representations can be returned directly.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : PreTrainedModel\n",
    "        The causal language model to evaluate (e.g., LLaMA).\n",
    "    tokenizer : PreTrainedTokenizer\n",
    "        The corresponding tokenizer.\n",
    "    dataset : Dataset\n",
    "        The input dataset.\n",
    "    batch_size : int\n",
    "        Number of samples per batch.\n",
    "    idx_start_sample : int\n",
    "        Index of the first sample to process from the dataset.\n",
    "    max_samples : int\n",
    "        Total number of examples to process from the dataset, starting from idx_start_sample. \n",
    "    save_to_pkl : bool\n",
    "        If True, activations are appended to the pickle file at output_path.\n",
    "        If False, the function returns a list of activations.\n",
    "    output_path : str\n",
    "        Path to the directory where extracted answers will be saved as individual pickle batch files.\n",
    "    build_prompt_fn : Callable\n",
    "        Function to build a prompt from context and question.\n",
    "    layers : List[int]\n",
    "        List of indices of the transformer layers to extract activations from (default: [-1] for last layer).\n",
    "    hidden_scores : List[str], optional\n",
    "        List of aggregation modes to compute on token activations. Possible modes include:\n",
    "            \"average\", \"last\", \"max\", \"first_generated\", \"token_svd_score\", \"feat_var\".\n",
    "        These modes are passed to `extract_token_activations` for aggregation. Default includes the above.\n",
    "    attn_scores : List[str], optional\n",
    "        List of attention-based scores to compute. Supported: \"attn_eig_prod\".\n",
    "    logit_scores : List[str], optional\n",
    "        List of logit-based scores to compute. Supported:\n",
    "            \"perplexity\", \"logit_entropy\", \"window_logit_entropy\".\n",
    "    logit_config : dict, optional\n",
    "        Configuration dictionary for logit-based scoring functions, with keys such as:\n",
    "            - \"top_k\": int, number of top logits considered (default 50)\n",
    "            - \"window_size\": int, window size for windowed entropy (default 1)\n",
    "            - \"stride\": int, stride for windowed entropy (default 1)\n",
    "    start_offset : int\n",
    "        Offset from the first non-padding token (must be >= 0). \n",
    "    end_offset : int\n",
    "        Offset from the last non-padding token (must be <= 0, e.g., -3 to remove 3 tokens).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Union[List[dict], None]\n",
    "        If `save_to_pkl` is False, returns a list of dictionaries, one per batch, with each element\n",
    "         of the list having the following structure:\n",
    "            {\n",
    "                \"id\": List[str],  # IDs of batch samples\n",
    "                \"original_indices\": List[int],  # Original dataset indices\n",
    "                \"context\": List[str],\n",
    "                \"question\": List[str],\n",
    "                \"gt_answers\": List[str],        # Ground-truth reference answers\n",
    "                \"gen_answers\": List[str],       # Generated model answers\n",
    "                \"scores\": {\n",
    "                    \"layer_{layer_idx}\": {\n",
    "                        \"hidden\": { \n",
    "                            \"{mode}\": np.ndarray[(batch_size, hidden_size), float], \n",
    "                            ... # one entry per mode in hidden_scores\n",
    "                        },\n",
    "                        \"attention\": {\n",
    "                            \"{attn_score}\": np.ndarray[(batch_size,), float],  \n",
    "                            ...\n",
    "                        }\n",
    "                    },\n",
    "                    \"logits\": {\n",
    "                        \"perplexity\": np.ndarray[(batch_size,), float],\n",
    "                        \"logit_entropy\": np.ndarray[(batch_size,), float],\n",
    "                        \"window_logit_entropy\": np.ndarray[(batch_size,), float] \n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "\n",
    "        If `save_to_pkl` is True, saves each batch's dictionary incrementally to disk and returns None.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    When using model.generate() with output_hidden_states=True (what we are replicating here with the ,\n",
    "    activation hook) use_cache=True and max_new_tokens=30, there is always an offset between the length of the \n",
    "    generated sequence (outputs.sequences.shape[1][prompt_len:]) and the length of len(outputs.hidden_states) : \n",
    "    * outputs.sequences.shape[1] = prompt_len (17) + max_new_tokens (30) = 47\n",
    "    * len(outputs.hidden_states) = max_new_tokens (30)\n",
    "        With : \n",
    "        * outputs.hidden_states[0][layer_idx].shape = (batch_size, prompt_len, hidden_size)           --> includes the prompt ! \n",
    "        * outputs.hidden_states[i][layer_idx].shape = (batch_size, 1, hidden_size) with 1 <= i <= 29  --> stops at 29 ! \n",
    "    *Note* that in our code, outputs.hidden_states and activations are the same. \n",
    "        \n",
    "    Explanation from Hugging Face, April 2024 \n",
    "    (https://github.com/huggingface/transformers/issues/30036):\n",
    "    \"\"\"\n",
    "\n",
    "        \n",
    "    # ==============================================================  \n",
    "    # [PATCH] Replace LlamaAttention.forward on target layers by\n",
    "    #  custom module to extract attention weights\n",
    "    # ==============================================================\n",
    "    for idx in layers:  \n",
    "        model.model.layers[idx].self_attn.forward = patched_LlamaAttention_forward.__get__(\n",
    "            model.model.layers[idx].self_attn,\n",
    "            model.model.layers[idx].self_attn.__class__\n",
    "    )\n",
    "        \n",
    "    # ==============================================================  \n",
    "    # [LOOP] Process batches of examples  \n",
    "    # ==============================================================\n",
    "    all_batch_results = []  \n",
    "\n",
    "    for i in tqdm(range(idx_start_sample, idx_start_sample + max_samples, batch_size)):\n",
    "      \n",
    "        # ----------------------------------------------------------\n",
    "        # [BATCH INPUT] Extract and tokenize prompts\n",
    "        # ----------------------------------------------------------\n",
    "        batch = extract_batch(dataset, i, batch_size)\n",
    "        prompts = [build_prompt_fn(s[\"context\"], s[\"question\"]) for s in batch]\n",
    "        inputs = tokenizer(prompts, padding=True, truncation=True, return_tensors=\"pt\").to(model.device)\n",
    "        prompt_ids = inputs[\"input_ids\"] # (batch_size, prompt_len)\n",
    "        prompt_attention_mask = inputs[\"attention_mask\"] \n",
    "\n",
    "        # ----------------------------------------------------------\n",
    "        # [HOOKS] Register hooks to capture hidden states and attentions\n",
    "        # ----------------------------------------------------------\n",
    "        # This hook collects the hidden states. For layer l: \n",
    "        # activations_lists[l] = [act_prompt], \n",
    "        # activations_lists[l][0] of shape: (batch_size, prompt_len, hidden_size) \n",
    "        activations_lists = [[] for _ in layers]  # one empty list per layer \n",
    "        handle_act, call_counter_act = register_generation_activation_hook(model, activations_lists, layers)\n",
    "\n",
    "        # This hook collects the activations at each decoding step. For layer l: \n",
    "        # attentions_lists[l] = [attn_prompt], \n",
    "        # activations_lists[l][0] of shape: (batch_size, n_heads, prompt_len, prompt_len)\n",
    "        attentions_lists = [[] for _ in layers]  # one empty list per layer\n",
    "        handle_attn, call_counter_attn = register_generation_attention_hook(model, attentions_lists, layers)\n",
    "        \n",
    "        # ----------------------------------------------------------\n",
    "        # [FOWARD PASS] Run model with hooks to capture intermediate states\n",
    "        # ----------------------------------------------------------\n",
    "        # Pass inputs through the model. When the target layer is reached,\n",
    "        # the hook executes and saves its output in captured_hidden.\n",
    "        if logit_scores is not None and len(logit_scores) > 0:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs, return_dict=True, return_logits=True)\n",
    "            prompt_logits = outputs.logits\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs, return_dict=True)\n",
    "        \n",
    "        # Remove hooks to avoid memory leaks or duplicate logging\n",
    "        for h in handle_act: h.remove()\n",
    "        for h in handle_attn: h.remove()\n",
    "        \n",
    "        # Verify that hooks worked properly\n",
    "        verify_call_counters(call_counter_act, name=\"activation hooks\")\n",
    "        verify_call_counters(call_counter_attn, name=\"attention hooks\")\n",
    "\n",
    "\n",
    "        # ----------------------------------------------------------\n",
    "        # [OFFSET] Modify prompt mask with offset, if specified\n",
    "        # ----------------------------------------------------------\n",
    "        if start_offset !=0 or end_offset !=0:\n",
    "            prompt_attention_mask, start_indices, end_indices = compute_offset_attention_mask(\n",
    "                attention_mask=prompt_attention_mask, \n",
    "                start_offset=start_offset, \n",
    "                end_offset=end_offset\n",
    "            ) # (batch_size, prompt_len), (batch_size,), (batch_size,)\n",
    "\n",
    "\n",
    "        # **********************************************************\n",
    "        # [LAYER LOOP] Extract activation and attention-based scores for each specified layer \n",
    "        # **********************************************************\n",
    "        save_layers_scores = {}\n",
    "\n",
    "        for l, layer_idx in enumerate(layers):\n",
    "\n",
    "            activations = activations_lists[l]\n",
    "            attentions = attentions_lists[l]\n",
    "\n",
    "            # Define prompt and generation hidden states \n",
    "            prompt_activations=activations[0]    \n",
    "            \n",
    "            # Define prompt and generation attention maps\n",
    "            prompt_attentions=attentions[0]        \n",
    "\n",
    "            # ------------------------------------------------------\n",
    "            # [HIDDEN SCORES] Extract token-level activations/hidden-states\n",
    "            # ------------------------------------------------------\n",
    "            if hidden_scores is not None and len(hidden_scores) > 0:\n",
    "                # Return only the token activations from the prompt\n",
    "                selected_token_vecs = extract_token_activations(\n",
    "                        selected_layer=prompt_activations, \n",
    "                        attention_mask=prompt_attention_mask, \n",
    "                        device=prompt_activations.device,\n",
    "                        modes=hidden_scores,\n",
    "                    ) # (batch_size, hidden_size)\n",
    " \n",
    "                # Save results to dict\n",
    "                hidden_results = {}\n",
    "                for mode in hidden_scores:\n",
    "                    if mode in selected_token_vecs:\n",
    "                        hidden_results[mode] = selected_token_vecs[mode].cpu().numpy()\n",
    "                save_layers_scores.setdefault(f\"layer_{layer_idx}\", {}).update({\"hidden\": hidden_results})\n",
    "\n",
    "            # ------------------------------------------------------\n",
    "            # [ATTENTION SCORES] Extract attention eigenvalue-based metric\n",
    "            # ------------------------------------------------------\n",
    "            if attn_scores is not None and 'attn_eig_prod' in attn_scores:\n",
    "                attn_eig_prod = compute_attn_eig_prod(\n",
    "                        prompt_attentions=prompt_attentions, \n",
    "                        generation_attentions=None,\n",
    "                        prompt_attention_mask=prompt_attention_mask, \n",
    "                        generation_attention_mask=None,\n",
    "                        mode='prompt',\n",
    "                )\n",
    "                # Save results to dict\n",
    "                save_layers_scores.setdefault(f\"layer_{layer_idx}\", {}).update({\"attention\": {\"attn_eig_prod\": attn_eig_prod}}) \n",
    "        \n",
    "        # **********************************************************\n",
    "        # [END LAYER LOOP] \n",
    "        # **********************************************************\n",
    "\n",
    "        save_logits_scores = {}\n",
    "        # ------------------------------------------------------\n",
    "        # [LOGIT SCORES] Compute metrics from model logits\n",
    "        # ------------------------------------------------------\n",
    "        if logit_scores is not None:\n",
    "            if 'perplexity' in logit_scores:\n",
    "                perplexity = compute_perplexity(\n",
    "                    prompt_logits=prompt_logits, \n",
    "                    gen_logits=None,\n",
    "                    prompt_ids=prompt_ids, \n",
    "                    gen_ids=None,\n",
    "                    prompt_attention_mask=prompt_attention_mask,\n",
    "                    gen_attention_mask=None,\n",
    "                    mode='prompt',\n",
    "                    min_k=None\n",
    "                )\n",
    "                # Save results to dict\n",
    "                save_logits_scores['perplexity'] = perplexity \n",
    "\n",
    "            if 'logit_entropy' in logit_scores:\n",
    "                if logit_config is None:\n",
    "                    raise ValueError(\"logit_entropy is required but logit_config is None\")\n",
    "                logit_entropy = compute_logit_entropy(\n",
    "                    prompt_logits=prompt_logits,\n",
    "                    gen_logits=None,\n",
    "                    prompt_attention_mask=prompt_attention_mask,\n",
    "                    gen_attention_mask=None,\n",
    "                    mode='prompt',\n",
    "                    top_k=logit_config['top_k'], \n",
    "                    window_size=None,\n",
    "                    stride=None\n",
    "                )\n",
    "                # Save results to dict\n",
    "                save_logits_scores['logit_entropy'] = logit_entropy \n",
    "        \n",
    "            if 'window_logit_entropy' in logit_scores:\n",
    "                if logit_config is None:\n",
    "                    raise ValueError(\"window_logit_entropy is required but logit_config is None\")\n",
    "                window_logit_entropy = compute_logit_entropy(\n",
    "                    prompt_logits=prompt_logits,\n",
    "                    gen_logits=None,\n",
    "                    prompt_attention_mask=prompt_attention_mask,\n",
    "                    gen_attention_mask=None,\n",
    "                    mode='prompt',\n",
    "                    top_k=logit_config['top_k'],\n",
    "                    window_size=logit_config['window_size'], \n",
    "                    stride=logit_config['stride'] \n",
    "                )\n",
    "                # Save results to dict\n",
    "                save_logits_scores['window_logit_entropy'] = window_logit_entropy \n",
    "\n",
    "\n",
    "        # ==========================================================\n",
    "        # [OUTPUT] Store extracted results (to memory or file)\n",
    "        # ==========================================================\n",
    "        batch_results = {\n",
    "            \"id\": [s['id'] for s in batch],\n",
    "            \"original_indices\": [s['original_index'] for s in batch],\n",
    "            \"context\": [s['context'] for s in batch],\n",
    "            \"question\": [s['question'] for s in batch],\n",
    "            \"gt_answers\": [s['answers'] for s in batch],\n",
    "            \"scores\": {**save_layers_scores, **({\"logits\": save_logits_scores} if save_logits_scores else {})}\n",
    "        }\n",
    "\n",
    "        from src.data_reader.pickle_io import save_batch_pickle\n",
    "\n",
    "        if save_to_pkl:\n",
    "            save_batch_pickle(batch_data=batch_results, output_dir=output_path, batch_idx=i)\n",
    "        else:\n",
    "            all_batch_results.append(batch_results)\n",
    "\n",
    "    if not save_to_pkl:\n",
    "        return all_batch_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.02it/s]\n"
     ]
    }
   ],
   "source": [
    "# Clear memory to avoid \"CUDA out of memory\"\n",
    "# -----------------------------------\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "\n",
    "start_offset = 0\n",
    "end_offset = 0\n",
    "result = run_prompt_score_extraction(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=id_fit_dataset,\n",
    "    batch_size=2,\n",
    "    idx_start_sample=0,\n",
    "    max_samples=2,\n",
    "    save_to_pkl = False,\n",
    "    output_path = OUTPUT_DIR + \"all_batch_resultsTEST.pkl\",\n",
    "    build_prompt_fn=build_prompt,\n",
    "    layers = [18,-1],  \n",
    "    hidden_scores=[\"average\", \"last\", \"max\", \"first_generated\", \"token_svd_score\", \"feat_var\"],\n",
    "    attn_scores=[\"attn_eig_prod\"],\n",
    "    logit_scores=[\"perplexity\", \"logit_entropy\", \"window_logit_entropy\"],\n",
    "    logit_config={\"top_k\": 50, \"window_size\": 1, \"stride\": 1},\n",
    "    start_offset = start_offset,\n",
    "    end_offset = end_offset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': ['56be85543aeaaa14008c9063', '56be85543aeaaa14008c9065'],\n",
       "  'original_indices': [0, 1],\n",
       "  'context': ['Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".',\n",
       "   'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".'],\n",
       "  'question': ['When did Beyonce start becoming popular?',\n",
       "   'What areas did Beyonce compete in when she was growing up?'],\n",
       "  'gt_answers': [{'answer_start': 269, 'text': 'in the late 1990s'},\n",
       "   {'answer_start': 207, 'text': 'singing and dancing'}],\n",
       "  'scores': {'layer_18': {'hidden': {'average': array([[-0.08444164, -0.1926425 ,  0.08120184, ..., -0.04712849,\n",
       "             -0.04205577,  0.03879314],\n",
       "            [-0.07987061, -0.16546772,  0.08573577, ..., -0.0456568 ,\n",
       "             -0.04712473,  0.02181374]], dtype=float32),\n",
       "     'last': array([[-0.9688 , -0.762  ,  0.3179 , ..., -0.5405 , -0.07275,  0.642  ],\n",
       "            [-1.063  , -0.769  ,  0.4219 , ..., -0.536  , -0.2406 ,  0.6763 ]],\n",
       "           dtype=float16),\n",
       "     'max': array([[1.25     , 1.7949219, 1.3789062, ..., 1.5722656, 1.9023438,\n",
       "             1.5605469],\n",
       "            [1.2480469, 1.7958984, 1.3779297, ..., 1.5712891, 1.9013672,\n",
       "             1.5615234]], dtype=float32),\n",
       "     'first_generated': array([[-0.281 , -0.434 ,  0.2283, ...,  0.1516, -0.0885, -0.4124],\n",
       "            [-0.281 , -0.434 ,  0.2283, ...,  0.1516, -0.0885, -0.4124]],\n",
       "           dtype=float16),\n",
       "     'token_svd_score': array([6.2225194, 6.2249575], dtype=float32),\n",
       "     'feat_var': array([[0.2154574 , 0.27277166, 0.23229782, ..., 0.20216343, 0.19452685,\n",
       "             0.15710126],\n",
       "            [0.2169381 , 0.29965937, 0.2400306 , ..., 0.19819002, 0.20228037,\n",
       "             0.15605938]], dtype=float32)},\n",
       "    'attention': {'attn_eig_prod': array([-155.1, -155. ], dtype=float16)}},\n",
       "   'layer_-1': {'hidden': {'average': array([[ 0.50851715, -0.717234  ,  0.18985613, ...,  0.19579802,\n",
       "             -0.28334397, -0.14787038],\n",
       "            [ 0.5065286 , -0.7306724 ,  0.26053172, ...,  0.15503646,\n",
       "             -0.28712153, -0.18489715]], dtype=float32),\n",
       "     'last': array([[-0.2136, -1.462 ,  1.592 , ..., -1.143 ,  1.736 ,  2.104 ],\n",
       "            [-0.4844, -1.537 ,  1.12  , ..., -1.054 ,  1.619 ,  1.984 ]],\n",
       "           dtype=float16),\n",
       "     'max': array([[5.1132812, 4.796875 , 5.921875 , ..., 4.875    , 6.5351562,\n",
       "             3.5117188],\n",
       "            [5.109375 , 4.8007812, 6.1328125, ..., 4.8671875, 6.53125  ,\n",
       "             3.9042969]], dtype=float32),\n",
       "     'first_generated': array([[ 0.2886  , -0.075   ,  0.01497 , ...,  0.01741 ,  0.001204,\n",
       "              0.0376  ],\n",
       "            [ 0.2886  , -0.075   ,  0.01497 , ...,  0.01741 ,  0.001204,\n",
       "              0.0376  ]], dtype=float16),\n",
       "     'token_svd_score': array([8.98634 , 8.982739], dtype=float32),\n",
       "     'feat_var': array([[2.715007 , 3.168153 , 3.0687072, ..., 2.9259877, 3.031339 ,\n",
       "             1.5256512],\n",
       "            [2.752368 , 3.1549442, 3.2870016, ..., 3.0042598, 3.0037503,\n",
       "             1.6118357]], dtype=float32)},\n",
       "    'attention': {'attn_eig_prod': array([-120.56, -121.1 ], dtype=float16)}},\n",
       "   'logits': {'perplexity': array([7.0864315, 6.8185225], dtype=float32),\n",
       "    'logit_entropy': array([0.41475955, 0.4028572 ], dtype=float32),\n",
       "    'window_logit_entropy': array([3.51466  , 3.5172026], dtype=float32)}}}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test that all modules work correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.10s/it]\n"
     ]
    }
   ],
   "source": [
    "from src.inference.generation_utils import build_prompt\n",
    "\n",
    "from src.inference.run_extraction import run_prompt_and_generation_score_extraction\n",
    "\n",
    "result = run_prompt_and_generation_score_extraction(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=id_fit_dataset,\n",
    "    batch_size=2,\n",
    "    idx_start_sample= 0,\n",
    "    max_samples= 2,\n",
    "    save_to_pkl = False,\n",
    "    output_path = \"outputs/all_batch_results.pkl\",\n",
    "    build_prompt_fn=build_prompt,\n",
    "    layers = [18,-1],  \n",
    "    activation_source = \"promptGeneration\",\n",
    "    hidden_scores=[\"average\", \"last\", \"max\", \"first_generated\", \"token_svd_score\", \"feat_var\"],\n",
    "    attn_scores=['attn_scores'],\n",
    "    logit_scores=[\"perplexity\", \"logit_entropy\", \"window_logit_entropy\"],\n",
    "    logit_config={\"top_k\": 50, \"window_size\": 1, \"stride\": 1},\n",
    "    start_offset = 0,\n",
    "    end_offset = 0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': ['56be85543aeaaa14008c9063', '56be85543aeaaa14008c9065'],\n",
       "  'original_indices': [0, 1],\n",
       "  'context': ['Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".',\n",
       "   'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".'],\n",
       "  'question': ['When did Beyonce start becoming popular?',\n",
       "   'What areas did Beyonce compete in when she was growing up?'],\n",
       "  'gt_answers': [{'answer_start': 269, 'text': 'in the late 1990s'},\n",
       "   {'answer_start': 207, 'text': 'singing and dancing'}],\n",
       "  'gen_answers': [' unanswerable', ' Singing and dancing competitions.'],\n",
       "  'scores': {'layer_18': {'hidden': {'average': array([[-0.09061667, -0.18593608,  0.08875383, ..., -0.0436042 ,\n",
       "             -0.03706677,  0.03211577],\n",
       "            [-0.08099116, -0.13936505,  0.09960753, ..., -0.04558053,\n",
       "             -0.06095891,  0.02628676]], dtype=float32),\n",
       "     'last': array([[-0.5137 ,  0.756  ,  0.322  , ..., -0.673  , -0.3308 , -0.06866],\n",
       "            [-1.063  , -0.769  ,  0.4219 , ..., -0.536  , -0.2406 ,  0.6763 ]],\n",
       "           dtype=float16),\n",
       "     'max': array([[1.2480469, 1.7958984, 1.3808594, ..., 1.5712891, 1.9023438,\n",
       "             1.5634766],\n",
       "            [1.2480469, 1.7958984, 1.3779297, ..., 1.5712891, 1.9013672,\n",
       "             1.5615234]], dtype=float32),\n",
       "     'first_generated': array([[ 0.4077  ,  0.1605  ,  0.12085 , ...,  0.002045,  0.231   ,\n",
       "             -0.5107  ],\n",
       "            [-0.11    ,  0.75    , -0.2834  , ...,  0.1133  , -0.3132  ,\n",
       "             -0.4797  ]], dtype=float16),\n",
       "     'token_svd_score': array([5.937284, 6.223494], dtype=float32),\n",
       "     'feat_var': array([[0.21548717, 0.26665634, 0.23205729, ..., 0.19720535, 0.19079132,\n",
       "             0.14846477],\n",
       "            [0.2127446 , 0.31783444, 0.24079652, ..., 0.19848394, 0.20921347,\n",
       "             0.1560042 ]], dtype=float32)}},\n",
       "   'layer_-1': {'hidden': {'average': array([[ 0.5160655 , -0.6679448 ,  0.1866345 , ...,  0.22468485,\n",
       "             -0.278713  , -0.15280303],\n",
       "            [ 0.45985   , -0.7667821 ,  0.28836334, ...,  0.09987896,\n",
       "             -0.2650622 , -0.18045038]], dtype=float32),\n",
       "     'last': array([[ 1.799 , -1.349 ,  0.9697, ..., -1.789 ,  2.201 , -1.615 ],\n",
       "            [-0.4844, -1.537 ,  1.12  , ..., -1.054 ,  1.619 ,  1.984 ]],\n",
       "           dtype=float16),\n",
       "     'max': array([[5.1015625, 4.796875 , 5.9179688, ..., 4.8671875, 6.53125  ,\n",
       "             2.9335938],\n",
       "            [5.109375 , 4.8007812, 6.1328125, ..., 4.8671875, 6.53125  ,\n",
       "             3.9042969]], dtype=float32),\n",
       "     'first_generated': array([[ 0.7485, -1.93  , -1.098 , ...,  0.8423,  0.528 , -1.531 ],\n",
       "            [ 0.9043, -0.541 ,  3.035 , ...,  0.5723, -1.7295, -2.809 ]],\n",
       "           dtype=float16),\n",
       "     'token_svd_score': array([8.673517, 8.975086], dtype=float32),\n",
       "     'feat_var': array([[2.7490346, 3.256351 , 3.0166478, ..., 2.8845932, 3.0144863,\n",
       "             1.4453605],\n",
       "            [2.7786372, 3.2145367, 3.2309015, ..., 3.0622432, 2.9439812,\n",
       "             1.6561061]], dtype=float32)}},\n",
       "   'logits': {'perplexity': array([5.2908435, 6.3824706], dtype=float32),\n",
       "    'logit_entropy': array([0.6590029 , 0.39033875], dtype=float32),\n",
       "    'window_logit_entropy': array([3.7204087, 3.5182292], dtype=float32)}}}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonctions du papier LLM-check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hidden_acts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mLength of outputs.hidden_states:  7 \u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m( = number of generated tokens - 1 element to exclude last generated token + 1 element for prompt)\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m \u001b[33;03m**** Un score par couche, par sample (et pas par head, car les hidden states ne sont pas splittés par head). ****\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# dans compute_scores()\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer_num \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[43mhidden_acts\u001b[49m[\u001b[32m0\u001b[39m])):\n\u001b[32m     21\u001b[39m     mt_score.append(get_svd_eval(hidden_acts, layer_num, tok_lens, use_toklens)[\u001b[32m0\u001b[39m])\n\u001b[32m     22\u001b[39m     indiv_scores[mt][\u001b[33m\"\u001b[39m\u001b[33mHly\u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(layer_num)].append(mt_score[-\u001b[32m1\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'hidden_acts' is not defined"
     ]
    }
   ],
   "source": [
    "# Fonctions exactement comme dans le papier LLM check \n",
    "\n",
    "\"\"\"\n",
    "Length of outputs.hidden_states:  7 \n",
    "( = number of generated tokens - 1 element to exclude last generated token + 1 element for prompt)\n",
    "L : layer, batch_size = 2, hidden_size = 4096, prompt_len = 329\n",
    "Shape of outputs.hidden_states[0][L]: torch.Size([2, 329, 4096])\n",
    "Shape of activations[1][L]: torch.Size([2, 1, 4096])\n",
    "Shape of activations[-1][L]: torch.Size([2, 1, 4096])\n",
    "\n",
    "hidden_act = [x[0].to(torch.float32).detach().cpu() for x in outputs.hidden_states]\n",
    "\n",
    "Pour chaque couche (layer_num), tu calcules un score SVD sur les activations \n",
    "cachées de cette couche (pour chaque sample).\n",
    "get_svd_eval retourne un score par sample pour cette couche.\n",
    "Ces scores sont stockés dans indiv_scores[mt][\"HlyX\"] (X = numéro de la couche).\n",
    "Donc :\n",
    "\n",
    "**** Un score par couche, par sample (et pas par head, car les hidden states ne sont pas splittés par head). ****\n",
    "\"\"\"\n",
    "# dans compute_scores()\n",
    "for layer_num in range(1, len(hidden_acts[0])):\n",
    "    mt_score.append(get_svd_eval(hidden_acts, layer_num, tok_lens, use_toklens)[0])\n",
    "    indiv_scores[mt][\"Hly\" + str(layer_num)].append(mt_score[-1])\n",
    "\n",
    "\n",
    "def get_svd_eval(hidden_acts, layer_num=15, tok_lens=[], use_toklens=True):\n",
    "    \"\"\"Evaluate hidden states at a given layer using SVD-based scoring.\n",
    "\n",
    "    For each sample, this function extracts the hidden states at a specified layer,\n",
    "    optionally slices them according to `tok_lens`, and computes the SVD-based score.\n",
    "\n",
    "    Args:\n",
    "        hidden_acts (list): A list of tuples, each containing hidden states for all layers\n",
    "            for a single sample.\n",
    "        layer_num (int, optional): The layer index to evaluate. Defaults to 15.\n",
    "        tok_lens (list, optional): A list of (start, end) indices for each sample to slice\n",
    "            the hidden states. Defaults to [].\n",
    "        use_toklens (bool, optional): Whether to slice the hidden states using `tok_lens`.\n",
    "            Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        np.array: An array of SVD-based scores for each sample.\n",
    "    \"\"\"\n",
    "    svd_scores = []\n",
    "    for i in range(len(hidden_acts)): # loop sur les samples \n",
    "        Z = hidden_acts[i][layer_num] # activations pour le sample i à la couche layer_num, shape (seq_len, hidden_size)\n",
    "\n",
    "        if use_toklens and tok_lens[i]:\n",
    "            i1, i2 = tok_lens[i][0], tok_lens[i][1]\n",
    "            Z = Z[i1:i2, :]\n",
    "\n",
    "        Z = torch.transpose(Z, 0, 1)\n",
    "        svd_scores.append(centered_svd_val(Z).item())\n",
    "    # print(\"Sigma matrix shape:\",Z.shape[1])\n",
    "    return np.stack(svd_scores)\n",
    "\n",
    "\n",
    "def centered_svd_val(Z, alpha=0.001):\n",
    "    \"\"\"Compute the mean log singular value of a centered covariance matrix.\n",
    "\n",
    "    This function centers the data and computes the singular value decomposition\n",
    "    (SVD) of the resulting covariance matrix. It then returns the mean of the\n",
    "    log singular values, regularized by `alpha`.\n",
    "\n",
    "    Args:\n",
    "        Z (torch.Tensor): A 2D tensor representing features hidden acts.\n",
    "        alpha (float, optional): Regularization parameter added to the covariance matrix.\n",
    "            Defaults to 0.001.\n",
    "\n",
    "    Returns:\n",
    "        float: The mean of the log singular values of the centered covariance matrix.\n",
    "    \"\"\"\n",
    "    # assumes Z is in full precision\n",
    "    # Center the lines of  Z (i.e. subtract the average of each line).\n",
    "    # Allows to study variance without bias due to a non-zero mean.\n",
    "    J = torch.eye(Z.shape[0]) - (1 / Z.shape[0]) * torch.ones(Z.shape[0], Z.shape[0])\n",
    "    # Compute column-centered covariance matrix of Z\n",
    "    Sigma = torch.matmul(torch.matmul(Z.t(), J), Z)\n",
    "    # Regularization for stabilization\n",
    "    Sigma = Sigma + alpha * torch.eye(Sigma.shape[0])\n",
    "    # Singular Value Decomposition\n",
    "    svdvals = torch.linalg.svdvals(Sigma)\n",
    "    # Final Score\n",
    "    eigscore = torch.log(svdvals).mean() # multiplication by 2 missing from the paper ? \n",
    "    return eigscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tok_in:  tensor([[    1,  2581, 26099,  1597,  9577,   432,   324,   347,  9115,   332]],\n",
      "       device='cuda:0')\n",
      "tok_in.shape:  torch.Size([1, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(logit):  1\n",
      "logit.shape : torch.Size([1, 10, 32000])\n",
      "=============len(attn): 32\n",
      "logits[i]:  tensor([[ 1.0217e-01, -2.1973e-01,  3.1348e-01,  ...,  1.3281e+00,\n",
      "          1.8799e+00,  6.4502e-01],\n",
      "        [-6.7344e+00, -6.9375e+00, -1.1934e+00,  ..., -2.8359e+00,\n",
      "         -7.2969e+00, -3.1680e+00],\n",
      "        [-3.2051e+00, -3.1211e+00, -3.9014e-01,  ..., -2.3281e+00,\n",
      "         -7.9648e+00, -3.3086e+00],\n",
      "        ...,\n",
      "        [-3.1445e+00, -4.4258e+00,  2.3438e+00,  ..., -4.1289e+00,\n",
      "         -7.4336e+00, -1.7598e+00],\n",
      "        [-3.5176e+00, -2.7402e+00,  4.0391e+00,  ..., -1.5322e+00,\n",
      "         -2.4648e+00, -5.1270e-03],\n",
      "        [-4.9492e+00, -4.7539e+00,  3.8203e+00,  ..., -4.7812e+00,\n",
      "         -6.2500e+00, -4.0703e+00]], dtype=torch.float16)\n",
      "logits[i].shape:  torch.Size([10, 32000])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 256\u001b[39m\n\u001b[32m    253\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=============len(attn):\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(attn))\n\u001b[32m    254\u001b[39m tok_in = tok_in.cpu()\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m \u001b[43mcompute_scores\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mlogit\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_acts\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mhidden_act\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# hidden_acts est la liste des activations de chaque couche pour ce sample\u001b[39;49;00m\n\u001b[32m    259\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattns\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmt_list\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlogit\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# ['hidden', 'attns', 'logit'],\u001b[39;49;00m\n\u001b[32m    261\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtok_ins\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtok_in\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 62\u001b[39m, in \u001b[36mcompute_scores\u001b[39m\u001b[34m(logits, hidden_acts, attns, tok_ins, mt_list, indiv_scores, tok_lens, use_toklens)\u001b[39m\n\u001b[32m     60\u001b[39m mt_score = []\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mt == \u001b[33m\"\u001b[39m\u001b[33mlogit\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m     mt_score.append(\u001b[43mperplexity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtok_ins\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtok_lens\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m])\n\u001b[32m     63\u001b[39m     \u001b[38;5;66;03m#indiv_scores[mt][\"perplexity\"].append(mt_score[-1])\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m     \u001b[38;5;66;03m#mt_score.append(window_logit_entropy(logits, tok_lens, w=1)[0])\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     68\u001b[39m     \u001b[38;5;66;03m#mt_score.append(logit_entropy(logits, tok_lens, top_k=50)[0])\u001b[39;00m\n\u001b[32m     69\u001b[39m     \u001b[38;5;66;03m#indiv_scores[mt][\"logit_entropy\"].append(mt_score[-1])\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m mt == \u001b[33m\"\u001b[39m\u001b[33mhidden\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 181\u001b[39m, in \u001b[36mperplexity\u001b[39m\u001b[34m(logits, tok_ins, tok_lens, min_k)\u001b[39m\n\u001b[32m    178\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mlogits[i]: \u001b[39m\u001b[33m\"\u001b[39m, logits[i])\n\u001b[32m    179\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mlogits[i].shape: \u001b[39m\u001b[33m\"\u001b[39m, logits[i].shape)\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m i1, i2 = \u001b[43mtok_lens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m[\u001b[32m0\u001b[39m], tok_lens[i][\u001b[32m1\u001b[39m]\n\u001b[32m    183\u001b[39m pr = torch.log(softmax(logits[i]))[torch.arange(i1, i2) - \u001b[32m1\u001b[39m, tok_ins[i][\u001b[32m0\u001b[39m, i1:i2]]\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m min_k \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Faire tourner les fonctions du papier LLM check sur ma machine \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def get_model_vals(model, tok_in):\n",
    "    \"\"\"Run the model forward pass to obtain logits, hidden states, and attention scores.\n",
    "\n",
    "    Args:\n",
    "        model: A pretrained model compatible with the transformers API.\n",
    "        tok_in (torch.Tensor): A tensor of tokenized input IDs.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple `(logits, hidden_states, attentions)` where:\n",
    "        logits (torch.Tensor): Output logits from the model.\n",
    "        hidden_states (tuple of torch.Tensor): Hidden states from each model layer.\n",
    "        attentions (tuple of torch.Tensor): Attention weights from each model layer.\n",
    "    \"\"\"\n",
    "    kwargs = {\n",
    "        \"input_ids\": tok_in,\n",
    "        \"use_cache\": False,\n",
    "        \"past_key_values\": None,\n",
    "        \"output_attentions\": True,\n",
    "        \"output_hidden_states\": True,\n",
    "        \"return_dict\": True,\n",
    "    }\n",
    "    with torch.no_grad():\n",
    "        output = model(**kwargs)\n",
    "    return output.logits, output.hidden_states, output.attentions\n",
    "\n",
    "\n",
    "\n",
    "#def compute_scores(logits, hidden_acts, attns, scores,  mt_list, tok_ins, indiv_scores=None, tok_lens=[], use_toklens=False):\n",
    "def compute_scores(logits, hidden_acts, attns, tok_ins, mt_list = ['hidden'], indiv_scores=None, tok_lens=[], use_toklens=False):\n",
    "    \"\"\"Compute various evaluation scores (e.g., perplexity, entropy, SVD scores) from model outputs.\n",
    "\n",
    "    This function takes model outputs (logits, hidden states, attentions) and computes\n",
    "    a list of metric scores defined by `mt_list`. The computed scores are appended\n",
    "    to `scores` and `indiv_scores` dictionaries for tracking.\n",
    "\n",
    "    NOTE: The indiv_scores score dictionary will be saved to disk and then used for final metric computation in\n",
    "    check scores ipynb\n",
    "\n",
    "    Args:\n",
    "        logits: Model logits.\n",
    "        hidden_acts: Hidden activations.\n",
    "        attns: Attention matrices.\n",
    "        scores (list): A list to store aggregated scores across samples.\n",
    "        indiv_scores (dict): A dictionary to store metric-specific scores for each sample\n",
    "        mt_list (list): A list of metric types to compute.\n",
    "        tok_ins: A list of tokenized inputs for each sample.\n",
    "        tok_lens: A list of tuples indicating the start and end token indices for each sample.\n",
    "        use_toklens (bool, optional): Whether to use `tok_lens` to slice sequences. Defaults to True.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If an invalid metric type is encountered in `mt_list`.\n",
    "    \"\"\"\n",
    "    j=0\n",
    "    sample_scores = []\n",
    "    for mt in mt_list:\n",
    "        mt_score = []\n",
    "        if mt == \"logit\":\n",
    "            mt_score.append(perplexity(logits, tok_ins, tok_lens)[0])\n",
    "            #indiv_scores[mt][\"perplexity\"].append(mt_score[-1])\n",
    "\n",
    "            #mt_score.append(window_logit_entropy(logits, tok_lens, w=1)[0])\n",
    "            #indiv_scores[mt][\"window_entropy\"].append(mt_score[-1])\n",
    "\n",
    "            #mt_score.append(logit_entropy(logits, tok_lens, top_k=50)[0])\n",
    "            #indiv_scores[mt][\"logit_entropy\"].append(mt_score[-1])\n",
    "\n",
    "        elif mt == \"hidden\":\n",
    "            print(\"=============== j ===============\", j)\n",
    "            j+=1\n",
    "            for layer_num in range(1, len(hidden_acts[0])):\n",
    "                print(\"****** layer_num: *******\", layer_num)\n",
    "                mt_score.append(get_svd_eval(hidden_acts, layer_num, tok_lens, use_toklens)[0])\n",
    "                #indiv_scores[mt][\"Hly\" + str(layer_num)].append(mt_score[-1])\n",
    "\n",
    "        elif mt == \"attns\":\n",
    "            for layer_num in range(1, len(attns[0])):\n",
    "                mt_score.append(get_attn_eig_prod(attns, layer_num, tok_lens, use_toklens)[0])\n",
    "                #indiv_scores[mt][\"Attn\" + str(layer_num)].append(mt_score[-1])\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Invalid method type\")\n",
    "\n",
    "        sample_scores.extend(mt_score)\n",
    "    #scores.append(sample_scores)\n",
    "\n",
    "def centered_svd_val(Z, alpha=0.001):\n",
    "    \"\"\"Compute the mean log singular value of a centered covariance matrix.\n",
    "\n",
    "    This function centers the data and computes the singular value decomposition\n",
    "    (SVD) of the resulting covariance matrix. It then returns the mean of the\n",
    "    log singular values, regularized by `alpha`.\n",
    "\n",
    "    Args:\n",
    "        Z (torch.Tensor): A 2D tensor representing features hidden acts.\n",
    "        alpha (float, optional): Regularization parameter added to the covariance matrix.\n",
    "            Defaults to 0.001.\n",
    "\n",
    "    Returns:\n",
    "        float: The mean of the log singular values of the centered covariance matrix.\n",
    "    \"\"\"\n",
    "    # assumes Z is in full precision\n",
    "    print(\"Z.shape[0]: \", Z.shape[0])\n",
    "    print(\"--Z.shape: \", Z.shape)\n",
    "    J = torch.eye(Z.shape[0]) - (1 / Z.shape[0]) * torch.ones(Z.shape[0], Z.shape[0])\n",
    "    print(\"J.shape: \", J.shape)\n",
    "    Sigma = torch.matmul(torch.matmul(Z.t(), J), Z)\n",
    "    Sigma = Sigma + alpha * torch.eye(Sigma.shape[0])\n",
    "    print(\"Sigma.shape: \", Sigma.shape)\n",
    "    svdvals = torch.linalg.svdvals(Sigma)\n",
    "    eigscore = torch.log(svdvals).mean()\n",
    "    return eigscore\n",
    "\n",
    "def get_svd_eval(hidden_acts, layer_num=15, tok_lens=[], use_toklens=False):\n",
    "    \"\"\"Evaluate hidden states at a given layer using SVD-based scoring.\n",
    "\n",
    "    For each sample, this function extracts the hidden states at a specified layer,\n",
    "    optionally slices them according to `tok_lens`, and computes the SVD-based score.\n",
    "\n",
    "    Args:\n",
    "        hidden_acts (list): A list of tuples, each containing hidden states for all layers\n",
    "            for a single sample.\n",
    "        layer_num (int, optional): The layer index to evaluate. Defaults to 15.\n",
    "        tok_lens (list, optional): A list of (start, end) indices for each sample to slice\n",
    "            the hidden states. Defaults to [].\n",
    "        use_toklens (bool, optional): Whether to slice the hidden states using `tok_lens`.\n",
    "            Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        np.array: An array of SVD-based scores for each sample.\n",
    "    \"\"\"\n",
    "    svd_scores = []\n",
    "    print(\"len(hidden_acts): \", len(hidden_acts))\n",
    "    for i in range(len(hidden_acts)):\n",
    "        print(\"i: \", i)\n",
    "        Z = hidden_acts[i][layer_num]\n",
    "        print(\"Z.shape: \", Z.shape) # (seq_len, hidden_size)\n",
    "\n",
    "        if use_toklens and tok_lens[i]:\n",
    "            i1, i2 = tok_lens[i][0], tok_lens[i][1]\n",
    "            Z = Z[i1:i2, :]\n",
    "\n",
    "        Z = torch.transpose(Z, 0, 1) # (hidden_size, seq_len)\n",
    "        print(\"Z.T.shape: \", Z.shape)\n",
    "        svd_scores.append(centered_svd_val(Z).item())\n",
    "        print(\"len(svd_scores)\", len(svd_scores))\n",
    "        print(\"svd_scores[0]: \", svd_scores)\n",
    "    # print(\"Sigma matrix shape:\",Z.shape[1])\n",
    "    return np.stack(svd_scores)\n",
    "\n",
    "\n",
    "def perplexity(logits, tok_ins, tok_lens, min_k=None):\n",
    "    \"\"\"Compute the perplexity of model predictions for given tokenized inputs.\n",
    "\n",
    "    This function computes the perplexity by taking the negative log probability\n",
    "    of the correct tokens and exponentiating the mean. If `min_k` is provided,\n",
    "    it filters the lowest probabilities to compute a restricted perplexity.\n",
    "\n",
    "    Args:\n",
    "        logits: A list or array of model logits (samples x seq_len x vocab_size).\n",
    "        tok_ins: A list of tokenized input IDs for each sample.\n",
    "        tok_lens (list): A list of (start, end) indices specifying the portion of the\n",
    "            sequence to evaluate.\n",
    "        min_k (float, optional): A fraction of tokens to consider from the lowest\n",
    "            probabilities. If not None, only these tokens are considered.\n",
    "\n",
    "    Returns:\n",
    "        np.array: An array of perplexity values for each sample.\n",
    "    \"\"\"\n",
    "    softmax = torch.nn.Softmax(dim=-1)\n",
    "    ppls = []\n",
    "\n",
    "    \n",
    "    for i in range(len(logits)):\n",
    "        print(\"logits[i]: \", logits[i])\n",
    "        print(\"logits[i].shape: \", logits[i].shape)\n",
    "        \n",
    "        i1, i2 = tok_lens[i][0], tok_lens[i][1]\n",
    "        \n",
    "        pr = torch.log(softmax(logits[i]))[torch.arange(i1, i2) - 1, tok_ins[i][0, i1:i2]]\n",
    "        if min_k is not None:\n",
    "            pr = torch.topk(pr, k=int(min_k * len(pr)), largest=False).values\n",
    "        ppls.append(torch.exp(-pr.mean()).item())\n",
    "\n",
    "    return np.stack(ppls)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_attn_eig_prod(attns, layer_num=15, tok_lens=[], use_toklens=True):\n",
    "    \"\"\"Compute an eigenvalue-based attention score by analyzing attention matrices.\n",
    "\n",
    "    This function takes the attention matrices of a given layer and for each sample,\n",
    "    computes the mean log of the diagonal elements (assumed to be eigenvalues) across\n",
    "    all attention heads. Slices are applied if `tok_lens` is used.\n",
    "\n",
    "    Args:\n",
    "        attns (list): A list of tuples, each containing attention matrices for all layers\n",
    "            and heads for a single sample.\n",
    "        layer_num (int, optional): The layer index to evaluate. Defaults to 15.\n",
    "        tok_lens (list, optional): A list of (start, end) indices for each sample to slice\n",
    "            the attention matrices. Defaults to [].\n",
    "        use_toklens (bool, optional): Whether to slice the attention matrices using `tok_lens`.\n",
    "            Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        np.array: An array of computed attention-based eigenvalue scores for each sample.\n",
    "    \"\"\"\n",
    "    attn_scores = []\n",
    "\n",
    "    for i in range(len(attns)):  # iterating over number of samples\n",
    "        eigscore = 0.0\n",
    "        counter = 0\n",
    "        for attn_head_num in range(len(attns[i][layer_num])):  # iterating over number of attn heads\n",
    "            counter += 1\n",
    "            # attns[i][layer_num][j] is of size seq_len x seq_len = [10,10] if 10 tokens in the sentence\n",
    "            Sigma = attns[i][layer_num][attn_head_num]\n",
    "            #print(\"Attention, Sigma.shape: \", Sigma.shape)\n",
    "\n",
    "            if use_toklens and tok_lens[i]:\n",
    "                i1, i2 = tok_lens[i][0], tok_lens[i][1]\n",
    "                Sigma = Sigma[i1:i2, i1:i2]\n",
    "\n",
    "            eigscore += torch.log(torch.diagonal(Sigma, 0)).mean()\n",
    "            #print(\"eigscore: \", eigscore)\n",
    "             \n",
    "        attn_scores.append(eigscore.item())\n",
    "        #print(\"len(attn_scores): \", len(attn_scores))\n",
    "        res = np.stack(attn_scores)\n",
    "        #print(\"res.shape: \", res.shape)\n",
    "\n",
    "    #print(\"Counter: \", counter)\n",
    "    return res\n",
    "\n",
    "\n",
    "prompts = [\"Je suis une très jolie fleur\"]\n",
    "inputs = tokenizer(prompts, padding=True, truncation=True, return_tensors=\"pt\").to(model.device)\n",
    "tok_in  = inputs['input_ids']\n",
    "print(\"tok_in: \", tok_in)\n",
    "print(\"tok_in.shape: \", tok_in.shape)\n",
    "\n",
    "logit, hidden_act, attn = get_model_vals(model, tok_in.to(0))\n",
    "print(\"len(logit): \", len(logit))\n",
    "print(\"logit.shape :\", logit.shape)\n",
    "\n",
    "# Unpacking the values into lists on CPU\n",
    "logit = logit[0].cpu()\n",
    "hidden_act = [x[0].to(torch.float32).detach().cpu() for x in hidden_act]\n",
    "attn = [x[0].to(torch.float32).detach().cpu() for x in attn]\n",
    "print(\"=============len(attn):\", len(attn))\n",
    "tok_in = tok_in.cpu()\n",
    "\n",
    "compute_scores(\n",
    "    [logit],\n",
    "    hidden_acts=[hidden_act], # hidden_acts est la liste des activations de chaque couche pour ce sample\n",
    "    attns= [attn],\n",
    "    mt_list= ['logit'], # ['hidden', 'attns', 'logit'],\n",
    "    tok_ins=[tok_in]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brouillon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (694832033.py, line 2)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31melif mode == \"cov_svd\":\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "    # ================\n",
    "    elif mode == \"cov_svd\":\n",
    "        # Compute the mean of the log of singular values of the centered covariance \n",
    "        # for each sample in the batch, taking into account only valid tokens.\n",
    "        svd_scores = []\n",
    "        batch_size = selected_layer.shape[0]\n",
    "        for i in range(batch_size):\n",
    "            # Select valid tokens \n",
    "            mask = attention_mask[i].bool()\n",
    "            Z = selected_layer[i][mask]  # (num_valid_tokens, hidden_size)\n",
    "            if Z.shape[0] == 0:\n",
    "                svd_scores.append(float('nan'))\n",
    "                continue\n",
    "            # Transpose to have (hidden_size, num_valid_tokens)\n",
    "            Z = Z.transpose(0, 1)\n",
    "            # Assumes Z is in full precision\n",
    "            # Center the features of Z (subtract the average of each line) and compute covariance on tokens.\n",
    "            # Allows to study variance without bias due to a non-zero mean.\n",
    "            d = Z.shape[0] # hidden_size\n",
    "            J = torch.eye(d, device=Z.device) - (1 / d) * torch.ones(d, d, device=Z.device)\n",
    "            # Compute centered covariance matrix of Z\n",
    "            Sigma = torch.matmul(torch.matmul(Z, J), Z.t())\n",
    "            # Regularization for stabilization\n",
    "            Sigma = Sigma + alpha * torch.eye(Sigma.shape[0], device=Z.device)\n",
    "            # Singular Value Decomposition\n",
    "            svdvals = torch.linalg.svdvals(Sigma)\n",
    "            # Final Score\n",
    "            eigscore = torch.log(svdvals).mean() # mult by 2 missing from the paper? \n",
    "            svd_scores.append(eigscore.item())\n",
    "        aggregated_tokens = np.array(svd_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizer, PreTrainedModel\n",
    "import torch\n",
    "from torch.utils.hooks import RemovableHandle\n",
    "from typing import Tuple, Literal, List, Optional, Dict\n",
    "\n",
    "def extract_token_activations(\n",
    "    selected_layer: torch.Tensor,\n",
    "    attention_mask: torch.Tensor,\n",
    "    device: torch.device,\n",
    "    modes: List[Literal[\n",
    "        \"average\", \"last\", \"max\", \"first_generated\", \n",
    "        \"token_cov_svd\", \"feat_cov_svd\", \n",
    "        \"token_cov_stats\", \"feat_cov_stats\", \"feat_cov_var\"\n",
    "    ]] = [\"average\"],\n",
    "    skip_length: Optional[int] = None,\n",
    "    alpha: int = 0.001,\n",
    ") -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"   \n",
    "    Aggregate token-level activations over a specified span for each sequence in a batch,\n",
    "    using various aggregation modes and attention mask.\n",
    "\n",
    "    This function takes as input:\n",
    "      - The layer activations (selected_layer) for each token in a batch of sequences,\n",
    "      - An attention mask (attention_mask) of the same shape, where 1 indicates tokens to include\n",
    "        in the aggregation and 0 marks tokens to ignore.\n",
    "\n",
    "    The attention mask may be the original model mask, or a custom mask generated using\n",
    "    `compute_offset_attention_mask` to dynamically select a sub-span of tokens.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    selected_layer : torch.Tensor\n",
    "        Tensor of shape (batch_size, seq_len, hidden_size) containing model activations for each token.\n",
    "    attention_mask : torch.Tensor\n",
    "        Attention mask of shape (batch_size, seq_len),  1 for real tokens, 0 for padding.\n",
    "    device : torch.device\n",
    "        Device for computation.\n",
    "    modes : List[str]\n",
    "        List of aggregation modes to compute. Computed using only valid tokens where attention_mask == 1.\n",
    "        Supported:\n",
    "        - \"average\": Mean activation vector across valid tokens. Shape: (batch_size, hidden_size)\n",
    "        - \"max\": Element-wise max activation across valid tokens. Shape: (batch_size, hidden_size)\n",
    "        - \"last\": Activation vector of last valid token in each sequence. Shape: (batch_size, hidden_size)\n",
    "        - \"first_generated\": Activation of the first generated valid token in each sequence. Shape: (batch_size, hidden_size)\n",
    "             If skip_length is provided, selects the token starting from that offset. \n",
    "        - \"token_cov_svd\": Mean log singular value of the centered token covariance matrix. Shape: (batch_size,)\n",
    "        - \"feat_cov_svd\": Mean log singular value of the centered feature covariance matrix. Shape: (batch_size,)\n",
    "        - \"token_cov_stats\": Statistics (mean, std, min, max) of the centered token covariance matrix. Shape: (batch_size, 4)\n",
    "        - \"feat_cov_stats\": Statistics (mean, std, min, max) of the centered feature covariance matrix. Shape: (batch_size, 4)\n",
    "        - \"feat_cov_var\": Diagonal of the centered feature covariance matrix (variances). Shape: (batch_size, hidden_size)\n",
    "\n",
    "    skip_length : Optional[int]\n",
    "        If provided, used to explicitly select the first generated token (useful for \"first_generated\" mode).\n",
    "    alpha : float\n",
    "        Regularization parameter added to the covariance matrix.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, torch.Tensor or np.ndarray]\n",
    "        Dictionary mapping each mode to its result:\n",
    "            - (batch_size, hidden_size) for \"average\", \"max\", \"last\", \"first_generated\", \"feat_cov_var\"\n",
    "            - (batch_size,) for \"token_cov_svd\", \"feat_cov_svd\"\n",
    "            -  (batch_size, 4) for \"token_cov_stats\", \"feat_cov_stats\"\n",
    "        \n",
    "    NOTE: computation `token_cov_svd` score from: \n",
    "    \"LLM-Check: Investigating Detection of Hallucinations in Large Language Models\"\n",
    "    (Sriramanan et al. 2024)\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size, seq_len, hidden_size = selected_layer.shape\n",
    "    print(f\"batch_size: {batch_size}, hidden_size: {hidden_size}\")\n",
    "    aggregated_tokens = {}\n",
    "    \n",
    "    # Move to device \n",
    "    attention_mask = attention_mask.to(selected_layer.device)\n",
    "    print(\"selected_layer.device\", selected_layer.device)\n",
    "    print(\"attention_mask.devce\", attention_mask.device)\n",
    "\n",
    "    # =======================================\n",
    "    # Select the first token with optional offset `skip_length`\n",
    "    # =======================================\n",
    "    if \"first_generated\" in modes:\n",
    "        batch_indices = torch.arange(batch_size, device=device)\n",
    "        if skip_length is not None:\n",
    "            first_indices = torch.full((batch_size,), skip_length, device=device, dtype=torch.long)\n",
    "        else:\n",
    "            first_indices = (attention_mask == 1).float().argmax(dim=1)\n",
    "        first = selected_layer[batch_indices, first_indices] # Shape: (batch_size, hidden_size)\n",
    "        aggregated_tokens[\"first_generated\"] = first\n",
    "\n",
    "    # =======================================\n",
    "    # Select the last token \n",
    "    # =======================================\n",
    "    if \"last\" in modes:\n",
    "        last_indices = attention_mask.shape[1] - 1 - attention_mask.flip(dims=[1]).float().argmax(dim=1)\n",
    "        batch_indices = torch.arange(batch_size, device=device)\n",
    "        last = selected_layer[batch_indices, last_indices]  # Shape: (batch_size, hidden_size)\n",
    "        aggregated_tokens[\"last\"] = last\n",
    "\n",
    "    # =======================================\n",
    "    # Apply mask and compute aggregation \n",
    "    # =======================================\n",
    "    if \"average\" in modes or \"max\" in modes:\n",
    "        # Add one dimension for the broadcast on hidden_size\n",
    "        mask_float = attention_mask.float().unsqueeze(-1)  # (batch_size, num_valid_tokens, 1)\n",
    "        # Apply the mask to the activations: zero out tokens outside the target interval\n",
    "        masked = selected_layer * mask_float\n",
    "        #  Count the number of selected tokens for each sequence (avoid division by zero with clamp)\n",
    "        counts = mask_float.sum(dim=1).clamp(min=1e-6)\n",
    "        if \"average\" in modes:\n",
    "            # Compute the mean activation vector for each sequence over the selected interval\n",
    "            avg = masked.sum(dim=1) / counts # Shape: (batch_size, hidden_size)\n",
    "            aggregated_tokens[\"average\"] = avg\n",
    "        if \"max\" in modes:\n",
    "            # Replace padding with -inf to exclude from max calculation\n",
    "            masked_max = masked.masked_fill(mask_float.logical_not(), float('-inf'))\n",
    "            # Extract maximum values across sequence dimension\n",
    "            max_vals, _ = masked_max.max(dim=1) # Shape: (batch_size, hidden_size)\n",
    "            aggregated_tokens[\"max\"] = max_vals\n",
    "\n",
    "    # =======================================\n",
    "    # Covariance-based metrics\n",
    "    # =======================================\n",
    "    if any(m in modes for m in [\"token_cov_svd\", \"feat_cov_svd\", \"token_cov_stats\", \"feat_cov_stats\", \"feat_cov_var\"]):\n",
    "        token_cov_svd = [] \n",
    "        feat_cov_svd = [] \n",
    "        token_cov_stats = []\n",
    "        feat_cov_stats = []\n",
    "        feat_cov_var = []\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            # Select valid tokens \n",
    "            mask = attention_mask[i].bool()\n",
    "            Z = selected_layer[i][mask]  # (num_valid_tokens, hidden_size)\n",
    "            if Z.shape[0] == 0:\n",
    "                feat_cov_var.append(torch.full((hidden_size,), float('nan')))\n",
    "                token_cov_svd.append(float('nan'))\n",
    "                feat_cov_svd.append(float('nan'))\n",
    "                token_cov_stats.append(dict())\n",
    "                feat_cov_stats.append(dict())\n",
    "                continue\n",
    "            \n",
    "            if Z.dtype != torch.float32:\n",
    "                Z = Z.to(torch.float32)\n",
    "            num_valid_tokens = Z.shape[0]\n",
    "\n",
    "            # Compute covariance matrix on tokens : Sigma_token \n",
    "            # ---------------------------------------\n",
    "            # Assumes Z is in full precision\n",
    "            # Center the features of Z (subtract the average of each line) and compute covariance on tokens.\n",
    "            # Allows to study variance without bias due to a non-zero mean.\n",
    "            J = torch.eye(hidden_size, device=Z.device, dtype=Z.dtype) - (1 / hidden_size) * torch.ones(hidden_size, hidden_size, device=Z.device, dtype=Z.dtype)\n",
    "            print(\"J.shape:\", J.shape)\n",
    "            # Compute centered covariance matrix of Z\n",
    "            Sigma_token = torch.matmul(torch.matmul(Z, J), Z.t()) # (num_valid_tokens, num_valid_tokens)\n",
    "            # Regularization for stabilization\n",
    "            Sigma_token = Sigma_token + alpha * torch.eye(Sigma_token.shape[0], device=Z.device, dtype=Z.dtype)\n",
    "            print(\"1) Sigma_token stats:\", Sigma_token.mean(), Sigma_token.min(), Sigma_token.max())\n",
    "\n",
    "            # 2. Token covariance\n",
    "            Z_token_centered = Z - Z.mean(dim=1, keepdim=True)\n",
    "            Sigma_token = (Z_token_centered @ Z_token_centered.t()) / max(1, Z.shape[1] - 1)\n",
    "            Sigma_token += alpha * torch.eye(Z.shape[0], device=Z.device, dtype=Z.dtype)\n",
    "            print(\"2) Sigma_token stats:\", Sigma_token.mean(), Sigma_token.min(), Sigma_token.max())\n",
    "\n",
    "\n",
    "    \n",
    "            # Compute covariance matrix on features : Sigma_feat\n",
    "            # ---------------------------------------\n",
    "            # Center the features of Z (subtract the average of each column) and compute covariance on features.\n",
    "            J = torch.eye(num_valid_tokens, device=Z.device, dtype=Z.dtype) - (1 / num_valid_tokens) * torch.ones(num_valid_tokens, num_valid_tokens, device=Z.device, dtype=Z.dtype)\n",
    "            # Compute centered covariance matrix of Z\n",
    "            Sigma_feat = torch.matmul(torch.matmul(Z.t(), J), Z) # (hidden_size, hidden_size)\n",
    "            # Regularization for stabilization\n",
    "            Sigma_feat = Sigma_feat + alpha * torch.eye(Sigma_feat.shape[0], device=Z.device, dtype=Z.dtype)\n",
    "            \n",
    "            # Statistics of the token covariance matrix\n",
    "            # ---------------------------------------\n",
    "            if Sigma_token.dtype != torch.float32:\n",
    "                Sigma_token = Sigma_token.to(torch.float32)\n",
    "            \n",
    "            Sigma_token_diag = Sigma_token.diag()\n",
    "            token_stats = [\n",
    "                Sigma_token_diag.mean().item(),\n",
    "                Sigma_token_diag.std().item(),\n",
    "                Sigma_token_diag.min().item(),\n",
    "                Sigma_token_diag.max().item(),\n",
    "            ]\n",
    "            token_cov_stats.append(token_stats)\n",
    "\n",
    "            # Singular value decomposition (SVD) of the token covariance matrix\n",
    "            # ---------------------------------------\n",
    "            token_svdvals = torch.linalg.svdvals(Sigma_token) # Singular Value Decomposition\n",
    "            token_eigscore = torch.log(token_svdvals).mean()  # mult by 2 missing from the paper? \n",
    "            token_cov_svd.append(token_eigscore)\n",
    "\n",
    "            # Statistics of the feature covariance matrix\n",
    "            # ---------------------------------------\n",
    "            Sigma_feat_diag = Sigma_feat.diag()\n",
    "            \n",
    "            if Sigma_feat_diag.dtype != torch.float32:\n",
    "                Sigma_feat_diag = Sigma_feat_diag.to(torch.float32)\n",
    "\n",
    "            feat_stats = [\n",
    "                Sigma_feat_diag.mean().item(),\n",
    "                Sigma_feat_diag.std().item(),\n",
    "                Sigma_feat_diag.min().item(),\n",
    "                Sigma_feat_diag.max().item()\n",
    "            ]\n",
    "            feat_cov_var.append(Sigma_feat_diag)\n",
    "            feat_cov_stats.append(feat_stats)\n",
    "            \n",
    "            # Singular value decomposition (SVD) of the feature covariance matrix\n",
    "            # ---------------------------------------\n",
    "            feat_svdvals = torch.linalg.svdvals(Sigma_feat) # Singular Value Decomposition\n",
    "            feat_eigscore = torch.log(feat_svdvals).mean() \n",
    "            feat_cov_svd.append(feat_eigscore)\n",
    "\n",
    "        # Return scores\n",
    "        # ---------------------------------------\n",
    "        if \"token_cov_svd\" in modes:\n",
    "            aggregated_tokens[\"token_cov_svd\"] = torch.stack(token_cov_svd) # (batch_size,) \n",
    "        if \"feat_cov_svd\" in modes:\n",
    "            aggregated_tokens[\"feat_cov_svd\"] = torch.stack(feat_cov_svd) # (batch_size,) \n",
    "        if \"token_cov_stats\" in modes:\n",
    "            aggregated_tokens[\"token_cov_stats\"] = torch.tensor(token_cov_stats) # (batch_size, 4) \n",
    "        if \"feat_cov_stats\" in modes:\n",
    "            aggregated_tokens[\"feat_cov_stats\"] = torch.tensor(feat_cov_stats) # (batch_size, 4) \n",
    "        if \"feat_cov_var\" in modes:\n",
    "            aggregated_tokens[\"feat_cov_var\"] = torch.stack(feat_cov_var, dim=0) # (batch_size, hidden_size) \n",
    "\n",
    "    print(\"====================\")\n",
    "    print(aggregated_tokens)\n",
    "    return aggregated_tokens\n",
    "\n",
    "# Mettre tous les aggregated tokens sur le CPU ? \n",
    "# non mais nettoyer la mémoire à chaque fois: torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizer, PreTrainedModel, BatchEncoding\n",
    "import torch\n",
    "from datasets import  Dataset\n",
    "from tqdm import tqdm\n",
    "from typing import List, Callable, Union, Literal, Dict\n",
    "\n",
    "from src.inference.activation_utils import (\n",
    "    compute_offset_attention_mask,\n",
    ")\n",
    "from src.inference.inference_utils import (\n",
    "    build_prompt,\n",
    "    extract_batch, \n",
    "    align_generation_hidden_states,\n",
    "    align_prompt_hidden_states,\n",
    "    build_generation_attention_mask)\n",
    "\n",
    "def generate(\n",
    "    model: PreTrainedModel,\n",
    "    inputs: BatchEncoding,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    max_new_tokens: int = 50,\n",
    "    k_beams: int = 1,\n",
    "    **generate_kwargs\n",
    ") -> Union[torch.Tensor, Dict[str, torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Generate sequences from the model with optional beam search.\n",
    "    Supports advanced options via **generate_kwargs (e.g., output_attentions).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : PreTrainedModel\n",
    "        The language model to use for generation.\n",
    "    inputs : BatchEncoding\n",
    "        Tokenized input prompts.\n",
    "    tokenizer : PreTrainedTokenizer\n",
    "        Tokenizer providing eos and pad token IDs.\n",
    "    max_new_tokens : int, optional\n",
    "        Maximum number of new tokens to generate.\n",
    "    k_beams : int, optional\n",
    "        Number of beams to use. If 1, uses sampling. If >1, beam search is enabled.\n",
    "    **generate_kwargs : dict\n",
    "        Additional keyword arguments passed to `model.generate()`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Union[torch.Tensor, Dict[str, torch.Tensor]]\n",
    "        - If k_beams == 1:\n",
    "            Returns a tensor of generated token IDs: shape (batch_size, prompt_len + gen_len)\n",
    "        - If k_beams > 1:\n",
    "            Returns a dictionary with keys:\n",
    "                - \"sequences\": the generated token IDs\n",
    "                - \"beam_indices\": the beam path for each token\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=1, #max_new_tokens,\n",
    "            do_sample=True    if k_beams == 1 else False,\n",
    "            temperature=0.6   if k_beams == 1 else None,\n",
    "            top_p=0.9         if k_beams == 1 else None,\n",
    "            top_k=50          if k_beams == 1 else None,\n",
    "            num_beams=k_beams,\n",
    "            use_cache=True, \n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id, # Ensures clean padding (right padding)\n",
    "            output_hidden_states=False,      # We rely on the hook to extract hidden states instead (more memory efficient)\n",
    "            output_attentions=False,         # We rely on the hook to extract attention map instead (more memory efficient)\n",
    "            return_dict_in_generate=True,    # Needed for access to beam_indices when num_beams > 1\n",
    "            early_stopping=False if k_beams == 1 else True, #Generation stops as soon as any sequence hits EOS, even if other candidates have not yet finished.\n",
    "            **generate_kwargs                # For future flexibility (e.g., output_attentions, output_scores)\n",
    "        )\n",
    "        return outputs \n",
    "\n",
    "\n",
    "def register_generation_activation_hook(\n",
    "    model: PreTrainedModel,\n",
    "    captured_hidden_list: List[torch.Tensor],\n",
    "    layer_idx: int = -1\n",
    ") -> Tuple[RemovableHandle, dict]:\n",
    "    \"\"\"\n",
    "    Attaches a forward hook to a specific transformer layer to capture hidden states\n",
    "    during autoregressive text generation i.e., at each decoding step.\n",
    "    (more memory-efficient than using output_hidden_states=True).\n",
    "    Transformer layer = self-attention + FFN + normalization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : PreTrainedModel\n",
    "        The Hugging Face causal language model (e.g., GPT, LLaMA).\n",
    "    captured_hidden_list : List[torch.Tensor]\n",
    "        A list that will be filled with hidden states for each generation step. \n",
    "        Each tensor has shape (batch_size * num_beams, seq_len, hidden_size).\n",
    "    layer_idx : int\n",
    "        Index of the transformer block to hook. Defaults to -1 (the last layer).\n",
    "        Use a positive integer if you want to hook an intermediate layer instead.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    RemovableHandle : A handle object\n",
    "        Call `handle.remove()` after generation to remove the hook.\n",
    "    call_counter : int \n",
    "        Stores the number of times the hook is activated.\n",
    "    \"\"\"\n",
    "    # Raise error if layer_idx not in correct range\n",
    "    num_layers = len(model.model.layers)\n",
    "    if not (layer_idx == -1 or 0 <= layer_idx < num_layers):\n",
    "        raise ValueError(\n",
    "            f\"`layer_idx` must be -1 or in [0, {num_layers - 1}], but got {layer_idx}.\"\n",
    "        )\n",
    "    \n",
    "    call_counter = {\"count\": 0} # count how many times the hook is triggered\n",
    "\n",
    "    def hook_fn(module, input, output):\n",
    "        \"\"\"Function called automatically by PyTorch just after\n",
    "            the layer has produced its output during the forward pass.\"\"\"\n",
    "        \n",
    "        call_counter[\"count\"] += 1 \n",
    "\n",
    "        # output is a tuple (hidden_states,) → keep [0]\n",
    "        if layer_idx == -1:\n",
    "            # Capture the final normalized output \n",
    "            captured_hidden_list.append(model.model.norm(output[0]).detach())  # post RMSNorm!\n",
    "        else:\n",
    "            # Capture raw hidden states before layer normalization\n",
    "            captured_hidden_list.append(output[0].detach()) #### TEST #### \n",
    "    \n",
    "    # Register hook on the transformer block\n",
    "    # When Pytorch pass through this layer during forward pass, it also execute hook_fn.\n",
    "    handle = model.model.layers[layer_idx].register_forward_hook(hook_fn)\n",
    "    \n",
    "    return handle, call_counter\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "def run_prompt_and_generation_activation_extraction(\n",
    "    model: PreTrainedModel,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    dataset: Dataset,\n",
    "    batch_size: int = 4,\n",
    "    idx_start_sample: int = 0,\n",
    "    max_samples: int = 1000,\n",
    "    save_to_pkl: bool = False,\n",
    "    output_path: str = \"outputs/all_batch_results.pkl\",\n",
    "    build_prompt_fn: Callable[[str, str], str] = None,\n",
    "    register_generation_activation_hook_fn: Callable = None,\n",
    "    layer_idx: int = -1,  \n",
    "    extract_token_activations_fn: Callable = None,\n",
    "    activation_source: Literal[\"prompt\", \"generation\", \"promptGeneration\"] = \"generation\",\n",
    "    k_beams : int = 1,\n",
    "    start_offset : int = 0,\n",
    "    end_offset : int = 0,\n",
    ") -> Union[Dict[str, List[np.ndarray]], None]:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    batch_activations = {}  # Chosen token activation vectors\n",
    "\n",
    "    for i in tqdm(range(idx_start_sample, idx_start_sample + max_samples, batch_size)):\n",
    "        \n",
    "        # ==============================\n",
    "        # Prepare input batch\n",
    "        # ==============================\n",
    "        batch = extract_batch(dataset, i, batch_size)\n",
    "        print(\"batch[0]: \", batch[0])\n",
    "        \n",
    "        prompts = [build_prompt_fn(s[\"context\"], s[\"question\"]) for s in batch]\n",
    "        #prompts = [build_prompt_fn(s[\"context\"], s[\"question\"]) for s in [batch[1], batch[1]]]\n",
    "        #prompts = [build_prompt_fn(batch[0][\"context\"][:10], batch[0][\"question\"][:5]), \\\n",
    "        #           build_prompt_fn(batch[1][\"context\"][:15], batch[1][\"question\"][:10]) ]\n",
    "        #prompts = [s[\"context\"] + s[\"question\"] for s in batch]\n",
    "        #prompts = [batch[0][\"context\"][:5], batch[0][\"question\"][:5], \\\n",
    "        #           batch[1][\"context\"][:20], batch[1][\"question\"][:20]]\n",
    "        #prompts = [\"ceci est un test\", \"Je ne sais pas pourquoi il y a un probleme\"]\n",
    "\n",
    "        inputs = tokenizer(prompts, padding=True, truncation=True, return_tensors=\"pt\").to(model.device)\n",
    "        prompt_len = inputs[\"input_ids\"].shape[1] # Assumes prompts are padded to same length\n",
    "\n",
    "        for i in range(inputs['input_ids'].shape[0]):\n",
    "            print(\"--- Example\", i)\n",
    "            print(\"prompt:\", prompts[i])\n",
    "            print(\"input_ids:\", inputs[\"input_ids\"][i])\n",
    "            print(\"len(input_ids): \", len(inputs[\"input_ids\"][i]))\n",
    "            print(\"attention_mask:\", inputs[\"attention_mask\"][i])\n",
    "            print(\"pad positions:\", (inputs['input_ids'][i] == tokenizer.pad_token_id).nonzero())\n",
    "            print(\"EOS positions:\", (inputs['input_ids'][i] == tokenizer.eos_token_id).nonzero())\n",
    "\n",
    "\n",
    "        max_id = inputs['input_ids'].max().item()\n",
    "        print(\"max input id:\", max_id)\n",
    "        print(\"embedding matrix size:\", model.get_input_embeddings().weight.size(0))\n",
    "        assert max_id < model.get_input_embeddings().weight.size(0)\n",
    "\n",
    "        # ==============================\n",
    "        # Register forward hook to capture layer output\n",
    "        # ==============================\n",
    "        # This hook collects the hidden states at each decoding step\n",
    "        # activations = [prompt] + [gen_step_1, gen_step_2, ..., gen_step_49], len(activations)=50, if max_new_tokens=50.\n",
    "        activations = [] # activations[k] of Shape: (batch_size * k_beams, seq_len, hidden_size)\n",
    "        #handle, call_counter_act = register_generation_activation_hook_fn(model, activations, layer_idx=layer_idx)\n",
    "        \n",
    "        attentions = [] # attentions[k] of Shape: (batch  *k_beams, n_heads, tgt_seq_len, src_seq_len)\n",
    "        #handle_attn, call_counter_attn = register_generation_attention_hook(model, attentions, layer_idx=layer_idx) # PUT function in params\n",
    "\n",
    "        # ==============================\n",
    "        # Run model forward pass (hook captures activations)\n",
    "        # ==============================\n",
    "        # Generate text from prompts using beam search or sampling. \n",
    "        outputs = generate(model, inputs, tokenizer, max_new_tokens=50, k_beams=k_beams)\n",
    "\n",
    "        print(\"attentions: \", attentions)\n",
    "        \n",
    "    \n",
    "        print(\"len(outputs.attentions): \", len(outputs.attentions))\n",
    "        print(\"outputs.attentions[0].shape: \", outputs.attentions[0].shape)\n",
    "        \n",
    "    \n",
    "        print(\"outputs.sequences.device: \", outputs.sequences.device)\n",
    "    \n",
    "        # Retrieve text of generated answers\n",
    "        gen_answers = tokenizer.batch_decode(\n",
    "            outputs.sequences[:, prompt_len:], \n",
    "            skip_special_tokens=True\n",
    "        ) # Shape: [batch_size,]\n",
    "        \n",
    "        # Define prompt and generation hidden states \n",
    "        #   outputs.hidden_states: list of hidden states at each generation step.\n",
    "        #   - outputs.hidden_states[0][layer_idx]: hidden states for the entire prompt\n",
    "        #   Shape: (batch_size * k_beams, prompt_len, hidden_size)\n",
    "        #   - outputs.hidden_states[1 + i][layer_idx]: hidden states after generating the i-th new token\n",
    "        #   Shape: (batch_size * k_beams, 1, hidden_size), for i = 0, ..., max_new_tokens-1\n",
    "        #   So, for max_new_tokens=50 in generate(): len(outputs.hidden_states)=50\n",
    "        #   and outputs.hidden_states: [prompt] + [gen_step_1] + [gen_step_2] + ... + [gen_step_49]\n",
    "        prompt_activations=outputs.hidden_states[0][layer_idx] \n",
    "        generation_activations=[outputs.hidden_states[k][layer_idx] for k in range(1, len(outputs.hidden_states))] \n",
    "        \n",
    "        \n",
    "        print(\"len(prompt_activations): \", len(prompt_activations))\n",
    "        print(\"len(generation_activations): \", len(generation_activations))\n",
    "\n",
    "        print(\"generation_activations[0].device: \", generation_activations[0].device)\n",
    "        print(\"prompt_activations.device:\", prompt_activations.device)\n",
    "\n",
    "        print(\"Layer L = \", layer_idx)\n",
    "        print(\"=======================================\")\n",
    "        print(\"outputs.sequences.shape[1]: \", outputs.sequences.shape[1])\n",
    "        if k_beams > 1:\n",
    "            print(\"outputs.beam_indices:\", outputs.beam_indices)\n",
    "            print(\"outputs.beam_indices.shape[1] :\", outputs.beam_indices.shape[1]) \n",
    "        print(\"outputs.sequences[:,prompt_len:] :\", outputs.sequences[: , prompt_len:])\n",
    "        print(\"outputs.sequences[: , prompt_len:].shape[1]:\", outputs.sequences[: , prompt_len:].shape[1])\n",
    "        print(\"gen_answers :\", gen_answers)\n",
    "        print(\"=======================================\")\n",
    "        print(\"Length of activations:\", len(activations))\n",
    "        print(\"Shape of activations[0]:\", activations[0].shape)\n",
    "        print(\"Shape of activations[1]:\", activations[1].shape)\n",
    "        print(\"Shape of activations[-1]:\", activations[-1].shape)\n",
    "        print(\"============\")\n",
    "        print(\"Length of outputs.hidden_states: \", len(outputs.hidden_states))\n",
    "        print(\"Shape of outputs.hidden_states[0][L]:\", outputs.hidden_states[0][layer_idx].shape)\n",
    "        print(\"Shape of outputs.hidden_states[1][L]:\", outputs.hidden_states[1][layer_idx].shape)\n",
    "        print(\"Shape of outputs.hidden_states[-1][L]:\", outputs.hidden_states[-1][layer_idx].shape)\n",
    "        print(\"=======================================\")\n",
    "\n",
    "        print(\"activations[0] stats\")\n",
    "        print(\"mean\", activations[0].mean().item(),\n",
    "              \"min\", activations[0].min().item(), \n",
    "              \"max\", activations[0].max().item(), \n",
    "              \"has inf?\", torch.isinf(activations[0]).any().item(),\n",
    "              \"has nan?\", torch.isnan(activations[0]).any().item())\n",
    "        print(\"outputs.hidden_states[0][layer_idx] stats\")\n",
    "        print(\"mean\", outputs.hidden_states[0][layer_idx].mean().item(),\n",
    "              \"min\", outputs.hidden_states[0][layer_idx].min().item(), \n",
    "              \"max\", outputs.hidden_states[0][layer_idx].max().item(), \n",
    "              \"has inf?\", torch.isinf(outputs.hidden_states[0][layer_idx]).any().item(),\n",
    "              \"has nan?\", torch.isnan(outputs.hidden_states[0][layer_idx]).any().item())\n",
    "        \n",
    "        print(\"activations[1:][-1] stats\")\n",
    "        print(\"mean\", activations[1:][-1].mean().item(),\n",
    "              \"min\", activations[1:][-1].min().item(), \n",
    "              \"max\", activations[1:][-1].max().item(), \n",
    "              \"has inf?\", torch.isinf(activations[1:][-1]).any().item(),\n",
    "            \"has nan?\", torch.isnan(activations[1:][-1]).any().item())\n",
    "        print(\"outputs.hidden_states[-1][layer_idx] stats\")\n",
    "        print(\"mean\", outputs.hidden_states[-1][layer_idx].mean().item(),\n",
    "              \"min\", outputs.hidden_states[-1][layer_idx].min().item(), \n",
    "              \"max\", outputs.hidden_states[-1][layer_idx].max().item(), \n",
    "              \"has inf?\", torch.isinf(outputs.hidden_states[-1][layer_idx]).any().item(),\n",
    "            \"has nan?\", torch.isnan(outputs.hidden_states[-1][layer_idx]).any().item())\n",
    "        print(\"=======================================\")\n",
    "\n",
    "        # ===============================\n",
    "        # Truncate activations to match real generation steps (cf. Understanding Note #1)\n",
    "        # ===============================\n",
    "        # During generation, the model may run extra forward passes (especially with beam search)\n",
    "        # beyond the number of tokens in the final output. This results in activations being longer\n",
    "        # than needed — we need to truncate them accordingly.\n",
    "        # (see Understanding Note #1).\n",
    "        if k_beams > 1:\n",
    "            # In beam search, we use beam_indices.shape[1] to determine the actual number of generation steps\n",
    "            gen_len = outputs.beam_indices.shape[1]\n",
    "        else:\n",
    "            # In greedy/top-k sampling, gen_len is simply the number of new tokens beyond the prompt\n",
    "            gen_len = outputs.sequences.shape[1] - prompt_len\n",
    "\n",
    "        # Sometimes, activations may include extra \"ghost\" steps (e.g., due to internal padding/sync in beam search)\n",
    "        bool_truncate_activations = (len(generation_activations) >= gen_len) \n",
    " \n",
    "        if bool_truncate_activations:\n",
    "            # Truncate extra steps to ensure alignment with generated tokens\n",
    "            generation_activations = generation_activations[:gen_len]\n",
    "\n",
    "        \"\"\"\n",
    "        ==================================\n",
    "        Understanding Note #1:\n",
    "        ==================================\n",
    "        When using beam search in Hugging Face Transformers, the number of decoder hidden states\n",
    "        (len(outputs.hidden_states)) can be greater than the number of tokens in the final generated \n",
    "        sequence (outputs.sequences[:,prompt_len:].shape[1] = outputs.beam_indices.shape[1]). \n",
    "        This happens because, during beam search, the model explores multiple candidate sequences \n",
    "        (beams) at each generation step and continues generating until a stopping condition is met \n",
    "        (such as all beams reaching EOS or the maximum number of tokens). But because beams can \n",
    "        finish at different steps (some hitting EOS early, others continuing), the model must keep\n",
    "        generating for the remaining active beams. \n",
    "        *Note* that in our code, outputs.hidden_states and activations are the same. \n",
    "      \n",
    "        Explanation from Hugging Face, January 2023: \n",
    "        (https://github.com/huggingface/transformers/issues/21374)\n",
    "        \"Beam Search: Here it's trickier. In essence, beam search looks for candidate outputs until it hits \n",
    "        a stopping condition. The candidate outputs can have fewer tokens than the total number of generation \n",
    "        steps -- for instance, in an encoder-decoder text model, if your input is How much is 2 + 2? and the \n",
    "        model generates as candidates <BOS>4<EOS> (3 tokens) and <BOS>The answer is potato<EOS> \n",
    "        (for argument's sake, 6 tokens) before deciding to stop, you should see sequences with shape [1, 3] \n",
    "        and decoder_hidden_states with length 5, because 5 tokens were generated internally before settling \n",
    "        on the 1st candidate.\"    \n",
    "        \"\"\"\n",
    "\n",
    "        # ===============================\n",
    "        # Truncate generated token IDs to match activations (cf. Understanding Note #2) \n",
    "        # ===============================\n",
    "        # - When N tokens are generated, only the first N-1 tokens have corresponding hidden states.\n",
    "        #   So activations[1:] covers only the first N-1 steps (cf. Understanding Note #2).\n",
    "        #   Therefore, we exclude the last generated token from outputs.sequences and beam_indices\n",
    "        #   to match activations[1:]\n",
    "        # - Exception: if activations were truncated earlier (bool_truncate_activations = True),\n",
    "        #   then we already lost activations of the final decoding step(s), and our activations[1:]\n",
    "        #   only cover the available tokens. In that case, we keep the full `gen_len` to match.\n",
    "        # (see Understanding Note #2)\n",
    "        if bool_truncate_activations:\n",
    "            expected_gen_len = gen_len  # All generated tokens have hidden states\n",
    "        else: \n",
    "            expected_gen_len  = gen_len - 1 # Drop final token to match activations[1:]\n",
    "\n",
    "        # Truncate generated sequences and beam paths accordingly\n",
    "        truncated_gen_sequences = outputs.sequences[:, prompt_len : prompt_len + expected_gen_len]\n",
    "        if k_beams > 1:\n",
    "            truncated_beam_indices = outputs.beam_indices[:, :expected_gen_len] \n",
    "\n",
    "        \"\"\"\n",
    "        ==================================\n",
    "        Understanding Note #2:\n",
    "        ==================================\n",
    "        When using model.generate() with output_hidden_states=True (what we are replicating here with the hook),\n",
    "        use_cache=True and max_new_tokens=30, there is always an offset between the length of the \n",
    "        generated sequence (outputs.sequences.shape[1][prompt_len:]) and the length of len(outputs.hidden_states) : \n",
    "        * outputs.sequences.shape[1] = prompt_len (17) + max_new_tokens (30) = 47\n",
    "        * len(outputs.hidden_states) = max_new_tokens (30)\n",
    "            With : \n",
    "            * outputs.hidden_states[0][layer_idx].shape = (batch_size, prompt_len, hidden_size)           --> includes the prompt ! \n",
    "            * outputs.hidden_states[i][layer_idx].shape = (batch_size, 1, hidden_size) with 1 <= i <= 29  --> stops at 29 ! \n",
    "        *Note* that in our code, outputs.hidden_states and activations are the same. \n",
    "            \n",
    "        Explanation from Hugging Face, April 2024 \n",
    "        (https://github.com/huggingface/transformers/issues/30036):\n",
    "        \"If you have 30 tokens at the end of generation, you'll always have 29 hidden states.\n",
    "        The token with index N is used to produce hidden states with index N, which is then used \n",
    "        to get the token with index N+1. The generation ends as soon as the target number of \n",
    "        tokens is obtained so, when we obtain the 30th token, we don't spend compute to get the 30th \n",
    "        set of hidden states. You can, however, manually run an additional forward pass to obtain the \n",
    "        30th set of hidden states, corresponding to the 30th token and used to obtain the 31st token.\n",
    "        \"\"\"\n",
    "        # ===============================\n",
    "        # Align generated and prompt hidden states\n",
    "        # ===============================\n",
    "        # Extract the hidden states that correspond to the generated sequence\n",
    "        # selected by the beam search (or top-k sampling if k_beams = 1)\n",
    "        aligned_generation_hidden_states = align_generation_hidden_states(\n",
    "            generation_activations=generation_activations, \n",
    "            beam_indices=truncated_beam_indices if k_beams > 1 else None,\n",
    "            k_beams=k_beams\n",
    "        ) # Shape: (batch_size, gen_len, hidden_size)\n",
    "\n",
    "        # Extract the hidden states that correspond to the prompt\n",
    "        aligned_prompt_hidden_states = align_prompt_hidden_states(\n",
    "            prompt_activations=prompt_activations, \n",
    "            k_beams=k_beams\n",
    "        ) # Shape: (batch_size, prompt_len, hidden_size)\n",
    "\n",
    "        # Concatenate the prompt and generation aligned hidden states  \n",
    "        aligned_prompt_and_gen_hidden_states = torch.cat(\n",
    "            [aligned_prompt_hidden_states, \n",
    "             aligned_generation_hidden_states], \n",
    "             dim=1\n",
    "        ) # Shape: (batch_size, prompt_len + gen_len, hidden_size)\n",
    "\n",
    "        \n",
    "        print(\"=======================================\")\n",
    "        print(\"aligned_prompt_hidden_states stats\")\n",
    "        print(\" min\", aligned_prompt_hidden_states.min().item(), \n",
    "              \"max\", aligned_prompt_hidden_states.max().item(), \n",
    "              \"has inf?\", torch.isinf(aligned_prompt_hidden_states).any().item(),\n",
    "            \"has nan?\", torch.isnan(aligned_prompt_hidden_states).any().item())\n",
    "\n",
    "        print(\"aligned_generation_hidden_states stats\")\n",
    "        print(\" min\", aligned_generation_hidden_states.min().item(), \n",
    "              \"max\", aligned_generation_hidden_states.max().item(), \n",
    "              \"has inf?\", torch.isinf(aligned_generation_hidden_states).any().item(),\n",
    "            \"has nan?\", torch.isnan(aligned_generation_hidden_states).any().item())\n",
    "        print(\"=======================================\")\n",
    "\n",
    "\n",
    "        # ===============================\n",
    "        # Build generation and prompt attention mask\n",
    "        # ===============================\n",
    "        # This mask marks which generated tokens are valid (i.e., not padding).\n",
    "        # Positions are marked True up to and including the first eos_token_id\n",
    "        generation_attention_mask = build_generation_attention_mask(\n",
    "            gen_ids=truncated_gen_sequences, \n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        ) # Shape (batch_size, gen_len)\n",
    "\n",
    "        # Prompt attention mask\n",
    "        prompt_attention_mask = inputs[\"attention_mask\"] \n",
    "        # Shape (batch_size, prompt_len)\n",
    "        \n",
    "        # ===============================\n",
    "        # Modify prompt attention mask with offsets\n",
    "        # ===============================\n",
    "        if start_offset !=0 or end_offset !=0:\n",
    "            prompt_attention_mask, start_indices, end_indices = compute_offset_attention_mask(\n",
    "                attention_mask=prompt_attention_mask, \n",
    "                start_offset=start_offset, \n",
    "                end_offset=end_offset\n",
    "            ) # Shape (batch_size, prompt_len), (batch_size,), (batch_size,)\n",
    "\n",
    "        # Concatenate the prompt and generation attention mask\n",
    "        prompt_and_gen_attention_mask = torch.cat(\n",
    "            [prompt_attention_mask,\n",
    "            generation_attention_mask],\n",
    "            dim=1\n",
    "        ) # Shape (batch_size, prompt_len + gen_len)\n",
    "\n",
    "        # ==============================\n",
    "        # Extract token activations from captured layer, based on source\n",
    "        # ==============================\n",
    "        if activation_source == \"generation\":\n",
    "            # Return only the token activations from the generated answer \n",
    "            selected_token_vecs = extract_token_activations_fn(\n",
    "                    selected_layer=aligned_generation_hidden_states, \n",
    "                    attention_mask=generation_attention_mask, \n",
    "                    device=aligned_generation_hidden_states.device,\n",
    "                ) # Shape (batch_size, hidden_size)\n",
    "            \n",
    "        elif activation_source == \"prompt\":    \n",
    "            # Return only the token activations from the prompt\n",
    "            selected_token_vecs = extract_token_activations_fn(\n",
    "                    selected_layer=aligned_prompt_hidden_states, \n",
    "                    attention_mask=prompt_attention_mask, \n",
    "                    device=aligned_prompt_hidden_states.device,\n",
    "                ) # Shape (batch_size, hidden_size)\n",
    "            \n",
    "        elif activation_source == \"promptGeneration\":\n",
    "            # Return token activations from the concatenated prompt + generated answer \n",
    "            selected_token_vecs = extract_token_activations_fn(\n",
    "                    selected_layer=aligned_prompt_and_gen_hidden_states, \n",
    "                    attention_mask=prompt_and_gen_attention_mask, \n",
    "                    device=aligned_prompt_and_gen_hidden_states.device,\n",
    "                    skip_length=prompt_len \n",
    "                    # skip_length: exclude prompt from computation if \n",
    "                    # mode=='first_generated' in `extract_token_activations_fn`\n",
    "                ) # Shape (batch_size, hidden_size)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Invalid value for `activation_source`: '{activation_source}'. \"\n",
    "                f\"Expected one of: ['prompt', 'generation', 'promptGeneration'].\"\n",
    "            )    \n",
    "        \n",
    "        print(\"selected_token_vecs:\", selected_token_vecs)\n",
    "        # ==============================\n",
    "        # Store results (to file or memory)\n",
    "        # ==============================\n",
    "        activations = {}\n",
    "        for mode, tensor in selected_token_vecs.items():\n",
    "            activations[mode] = [tensor[j].unsqueeze(0).cpu().numpy() for j in range(tensor.size(0))]\n",
    "\n",
    "        batch_dataset_ids = []; batch_dataset_original_idx = []; batch_context = []\n",
    "        batch_question = []; batch_gt_answers = []; batch_title = []\n",
    "        for s in batch:\n",
    "            batch_dataset_ids.append(s['id'])\n",
    "            batch_dataset_original_idx.append(s['original_index'])\n",
    "            batch_context.append(s['context'])\n",
    "            batch_question.append(s['question'])\n",
    "            batch_gt_answers.append(s['answers'])\n",
    "            batch_title.append(s['title'])\n",
    "        \n",
    "        batch_results = {\n",
    "            \"id\": batch_dataset_ids,\n",
    "            \"original_indices\": batch_dataset_original_idx,\n",
    "            \"activations\": activations, # Dict\n",
    "            \"gen_answers\": gen_answers,\n",
    "            \"gt_answers\": batch_gt_answers,\n",
    "            \"context\": batch_context,\n",
    "            \"question\": batch_question,\n",
    "            \"title\": batch_title,\n",
    "        }\n",
    "\n",
    "        if save_to_pkl:\n",
    "            #append_to_pickle(output_path, batch_results)\n",
    "            #save_batch_pickle(batch_data=batch_results, output_dir=output_path, batch_idx=i)\n",
    "            pass\n",
    "        else:\n",
    "            for mode, acts in activations.items():\n",
    "                if mode not in batch_activations:\n",
    "                    batch_activations[mode] = []\n",
    "                if isinstance(acts, list):\n",
    "                    batch_activations[mode].extend(acts)\n",
    "                else:\n",
    "                    batch_activations[mode].extend([a for a in acts])\n",
    "\n",
    "            #batch_activations.extend(activations)\n",
    "        \n",
    "    if not save_to_pkl:\n",
    "        return batch_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "def load_llama(model_name: str = \"meta-llama/Llama-2-7b-chat-hf\"):\n",
    "    model_name = model_name  # fine-tuned version of LLaMA for conversational uses\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')\n",
    "    tokenizer.model_max_length = 1024  # LLaMA-2’s max length tokens is 4096\n",
    "    \n",
    "    # For llama, pad_token is not defined by default:\n",
    "    # The convention is to use tokenizer.pad_token = tokenizer.eos_token \n",
    "    # However, this causes an issue when outputting attention maps during model.generate()\n",
    "    if tokenizer.pad_token is None:\n",
    "        # add \"<pad>\" to vocab as a special token\n",
    "        tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
    "        print(\"<pad> token not defined by default, add it to vocabulary.\")\n",
    "       \n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16, \n",
    "            device_map= \"auto\",          # load model to device \n",
    "            low_cpu_mem_usage=True,     # reduce RAM usage during loading\n",
    "            attn_implementation=\"eager\",\n",
    "            #output_hidden_states=True, # to hidden activations -> memory overload since we access ALL hidden states \n",
    "            #force_download=True        # redo complete download \n",
    "        )\n",
    "    \n",
    "        # required for the model to accept the new vocabulary.\n",
    "        model.resize_token_embeddings(len(tokenizer)) \n",
    "\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16, \n",
    "            device_map=\"auto\",          # load model to device \n",
    "            low_cpu_mem_usage=True,     # reduce RAM usage during loading\n",
    "        )\n",
    "\n",
    "    # Ensures that during generation all sequences are aligned with the PAD token, and not with random tokens. \n",
    "    model.config.pad_token_id = tokenizer.pad_token_id \n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "def load_llama(model_name: str = \"meta-llama/Llama-2-7b-chat-hf\"):\n",
    "    model_name = model_name  # fine-tuned version of LLaMA for conversational uses\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')\n",
    "    tokenizer.model_max_length = 1024  # LLaMA-2’s max length tokens is 4096\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # pad_token not defined by default: reuse the EOS token (</s>) as the padding token.\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16, \n",
    "        device_map=\"cuda:0\", #,\"auto\",          # load model to device \n",
    "        low_cpu_mem_usage=True,     # reduce RAM usage during loading\n",
    "        attn_implementation=\"eager\", # \"\" \"flex_attention\"\n",
    "        #output_hidden_states=True, # to hidden activations -> memory overload since we access ALL hidden states \n",
    "        #force_download=True        # redo complete download \n",
    "    )\n",
    "    print(\"attn_implementation changed\")\n",
    "    model.config.pad_token_id = model.config.eos_token_id # ensures that during generation all sequences are aligned with the EOS token, and not with random tokens. \n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for layer in model.model.layers:\n",
    "    layer.self_attn.forward = patched_LlamaAttention_forward.__get__(layer.self_attn, layer.self_attn.__class__)\n",
    "\n",
    "'''\n",
    "attention_module = model.model.layers[-1].self_attn\n",
    "print(\"attention_module._attn_implementation:\", attention_module.config._attn_implementation)\n",
    "\n",
    "# attention_module.type <class 'transformers.models.llama.modeling_llama.LlamaAttention'>\n",
    "\n",
    "for layer in model.model.layers:\n",
    "    layer.self_attn._attn_implementation = \"eager\"\n",
    "\n",
    "model.model.layers[-1].self_attn\n",
    "\n",
    "\n",
    "\n",
    "'''from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "model_name = \"gpt2\"  # ou \"gpt2-medium\", \"gpt2-large\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# GPT-2 a un pad_token natif\n",
    "tokenizer.pad_token = tokenizer.eos_token'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n",
    "        attention_mask: Optional[torch.Tensor],\n",
    "        past_key_value: Optional[Cache] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        **kwargs: Unpack[FlashAttentionKwargs],\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "        \n",
    "        print(\"=== DEBUG ATTENTION FORWARD ===\")\n",
    "    \n",
    "        # 1. Vérifier les inputs\n",
    "\n",
    "        print(f\"   INPUT hidden_states:\")\n",
    "        print(f\"   Shape: {hidden_states.shape}\")\n",
    "        print(f\"   Dtype: {hidden_states.dtype}\")\n",
    "        print(f\"   Has NaN: {torch.isnan(hidden_states).any()}\")\n",
    "        print(f\"   Has Inf: {torch.isinf(hidden_states).any()}\")\n",
    "        print(f\"   Range: [{hidden_states.min():.4f}, {hidden_states.max():.4f}]\")\n",
    "        num_nans = torch.isnan(hidden_states).sum().item()\n",
    "        print(f\"Nombre de NaNs dans hidden_states : {num_nans}\")\n",
    "        # Masque des NaNs (True là où c'est NaN)\n",
    "        nan_mask = torch.isnan(hidden_states)\n",
    "        # Affichage d'exemples de positions NaN\n",
    "        print(\"Positions des premiers NaNs :\", nan_mask.nonzero(as_tuple=False)[:10])\n",
    "        # Nombre de NaNs par dimension (par exemple par token ou par batch)\n",
    "        nans_par_token = nan_mask.view(hidden_states.size(0), -1).sum(dim=1)\n",
    "        print(\"Nombre de NaNs par exemple du batch :\", nans_par_token)\n",
    "        if num_nans > 0:\n",
    "            return  \n",
    "\n",
    "         # 2. Vérifier les poids des projections\n",
    "        print(f\"\\n  PROJECTION WEIGHTS:\")\n",
    "        for name, param in [(\"q_proj\", self.q_proj.weight), (\"k_proj\", self.k_proj.weight), (\"v_proj\", self.v_proj.weight)]:\n",
    "            print(f\"   {name}.weight:\")\n",
    "            print(f\"     Has NaN: {torch.isnan(param).any()}\")\n",
    "            print(f\"     Has Inf: {torch.isinf(param).any()}\")\n",
    "            print(f\"     Range: [{param.min():.4f}, {param.max():.4f}]\")\n",
    "            print(f\"     Norm: {param.norm().item():.4f}\")\n",
    "        # 3. Vérifier les biais (s'ils existent)\n",
    "        for name, proj in [(\"q_proj\", self.q_proj), (\"k_proj\", self.k_proj), (\"v_proj\", self.v_proj)]:\n",
    "            if hasattr(proj, 'bias') and proj.bias is not None:\n",
    "                print(f\"   {name}.bias:\")\n",
    "                print(f\"     Has NaN: {torch.isnan(proj.bias).any()}\")\n",
    "                print(f\"     Has Inf: {torch.isinf(proj.bias).any()}\")\n",
    "                print(f\"     Range: [{proj.bias.min():.4f}, {proj.bias.max():.4f}]\")\n",
    "\n",
    "        input_shape = hidden_states.shape[:-1]\n",
    "        hidden_shape = (*input_shape, -1, self.head_dim)\n",
    "\n",
    "        print(f\"\\n PROJECTIONS:\")\n",
    "\n",
    "        # Q projection\n",
    "        print(f\"   Computing Q projection...\")\n",
    "        q_raw = self.q_proj(hidden_states)\n",
    "        print(f\"   Q raw - Has NaN: {torch.isnan(q_raw).any()}, Range: [{q_raw.min():.4f}, {q_raw.max():.4f}]\")\n",
    "        \n",
    "        query_states = q_raw.view(hidden_shape).transpose(1, 2)\n",
    "        print(f\"   Q reshaped - Has NaN: {torch.isnan(query_states).any()}\")\n",
    "        \n",
    "        # K projection\n",
    "        print(f\"   Computing K projection...\")\n",
    "        k_raw = self.k_proj(hidden_states)\n",
    "        print(f\"   K raw - Has NaN: {torch.isnan(k_raw).any()}, Range: [{k_raw.min():.4f}, {k_raw.max():.4f}]\")\n",
    "        \n",
    "        key_states = k_raw.view(hidden_shape).transpose(1, 2)\n",
    "        print(f\"   K reshaped - Has NaN: {torch.isnan(key_states).any()}\")\n",
    "        \n",
    "        # V projection\n",
    "        print(f\"   Computing V projection...\")\n",
    "        v_raw = self.v_proj(hidden_states)\n",
    "        print(f\"   V raw - Has NaN: {torch.isnan(v_raw).any()}, Range: [{v_raw.min():.4f}, {v_raw.max():.4f}]\")\n",
    "        \n",
    "        value_states = v_raw.view(hidden_shape).transpose(1, 2)\n",
    "        print(f\"   V reshaped - Has NaN: {torch.isnan(value_states).any()}\")\n",
    "        \n",
    "\n",
    "        \"\"\"\n",
    "        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"Q max\", query_states.max(), \"min\", query_states.min())\n",
    "        print(\"K max\", key_states.max(), \"min\", key_states.min())\n",
    "\n",
    "        cos, sin = position_embeddings\n",
    "\n",
    "        print(f\"\\n  POSITION EMBEDDINGS:\")\n",
    "        print(f\"   cos - Has NaN: {torch.isnan(cos).any()}, Range: [{cos.min():.4f}, {cos.max():.4f}]\")\n",
    "        print(f\"   sin - Has NaN: {torch.isnan(sin).any()}, Range: [{sin.min():.4f}, {sin.max():.4f}]\")\n",
    "\n",
    "        print(f\"\\n  APPLYING RoPE...\")\n",
    "        query_states_before_rope = query_states.clone() ######\n",
    "        key_states_before_rope = key_states.clone() #######\n",
    "\n",
    "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "\n",
    "        print(f\"   Q after RoPE - Has NaN: {torch.isnan(query_states).any()}\")\n",
    "        print(f\"   K after RoPE - Has NaN: {torch.isnan(key_states).any()}\")\n",
    "        \n",
    "        if torch.isnan(query_states).any() or torch.isnan(key_states).any():\n",
    "            print(\"  NaN detectes après RoPE!\")\n",
    "            print(f\"   Q before RoPE had NaN: {torch.isnan(query_states_before_rope).any()}\")\n",
    "            print(f\"   K before RoPE had NaN: {torch.isnan(key_states_before_rope).any()}\")\n",
    "        \n",
    "\n",
    "        if past_key_value is not None:\n",
    "            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n",
    "            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n",
    "            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n",
    "\n",
    "        attention_interface: Callable = eager_attention_forward\n",
    "        if self.config._attn_implementation != \"eager\":\n",
    "            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n",
    "                logger.warning_once(\n",
    "                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n",
    "                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n",
    "                )\n",
    "            else:  \n",
    "                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n",
    "        \n",
    "        print(\"attention_interface:\", attention_interface)\n",
    "        print(\"attention_mask:\", attention_mask)\n",
    "\n",
    "        attn_output, attn_weights = attention_interface(\n",
    "            self,\n",
    "            query_states,\n",
    "            key_states,\n",
    "            value_states,\n",
    "            attention_mask,\n",
    "            dropout=0.0 if not self.training else self.attention_dropout,\n",
    "            scaling=self.scaling,\n",
    "            **kwargs,\n",
    "        )\n",
    "        \n",
    "        #print(\"attention weights dtype:\", attn_weights.dtype)\n",
    "\n",
    "        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "        return attn_output, attn_weights\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n",
    "        attention_mask: Optional[torch.Tensor],\n",
    "        past_key_value: Optional[Cache] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        **kwargs: Unpack[FlashAttentionKwargs],\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "        input_shape = hidden_states.shape[:-1]\n",
    "        hidden_shape = (*input_shape, -1, self.head_dim)\n",
    "\n",
    "        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "\n",
    "        print(\"Q max\", query_states.max(), \"min\", query_states.min())\n",
    "        print(\"K max\", key_states.max(), \"min\", key_states.min())\n",
    "\n",
    "        cos, sin = position_embeddings\n",
    "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "\n",
    "        if past_key_value is not None:\n",
    "            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n",
    "            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n",
    "            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n",
    "\n",
    "        attention_interface: Callable = eager_attention_forward\n",
    "        \n",
    "        if self.config._attn_implementation != \"eager\":\n",
    "            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n",
    "                logger.warning_once(\n",
    "                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n",
    "                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n",
    "                )\n",
    "            else:  \n",
    "                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n",
    "        \n",
    "        print(\"attention_interface:\", attention_interface)\n",
    "        print(\"attention_mask:\", attention_mask)\n",
    "\n",
    "        attn_output, attn_weights = attention_interface(\n",
    "            self,\n",
    "            query_states,\n",
    "            key_states,\n",
    "            value_states,\n",
    "            attention_mask,\n",
    "            dropout=0.0 if not self.training else self.attention_dropout,\n",
    "            scaling=self.scaling,\n",
    "            **kwargs,\n",
    "        )\n",
    "        \n",
    "        print(\"attention weights dtype:\", attn_weights.dtype)\n",
    "\n",
    "        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "        return attn_output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.llama.modeling_llama import LlamaAttention, eager_attention_forward, sdpa_attention_forward\n",
    "\n",
    "def patched_forward(\n",
    "    self,\n",
    "    hidden_states,\n",
    "    position_embeddings,\n",
    "    attention_mask,\n",
    "    past_key_value=None,\n",
    "    cache_position=None,\n",
    "    **kwargs,\n",
    "):\n",
    "    # Codes classiques (projections etc.)\n",
    "    input_shape = hidden_states.shape[:-1]\n",
    "    hidden_shape = (*input_shape, -1, self.head_dim)\n",
    "\n",
    "    query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "    key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "    value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "\n",
    "    cos, sin = position_embeddings\n",
    "    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "\n",
    "    if past_key_value is not None:\n",
    "        cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n",
    "        key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n",
    "\n",
    "    # --- 1. Passe principale avec SDPA ---\n",
    "    attn_output, _ = sdpa_attention_forward(\n",
    "        self,\n",
    "        query_states,\n",
    "        key_states,\n",
    "        value_states,\n",
    "        attention_mask,\n",
    "        dropout=0.0 if not self.training else self.attention_dropout,\n",
    "        scaling=self.scaling,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    # --- 2. Calcul des attn_weights en EAGER (mais pas utilisés, juste pour logging) ---\n",
    "    # À ce stade, attention: le calcul de attn_weights peut planter si instable ou NaN\n",
    "    try:\n",
    "        _, attn_weights = eager_attention_forward(\n",
    "            self,\n",
    "            query_states, key_states, value_states, attention_mask,\n",
    "            dropout=0.0 if not self.training else self.attention_dropout,\n",
    "            scaling=self.scaling, **kwargs,\n",
    "        )\n",
    "    except Exception as ex:\n",
    "        print(f\"!!! Eager NaN/inf during attn_weights calculation: {ex}\")\n",
    "        attn_weights = None\n",
    "\n",
    "    attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n",
    "    attn_output = self.o_proj(attn_output)\n",
    "    return attn_output, attn_weights  # NB: seule la sortie SDPA est utilisée downstream !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n",
    "import torch\n",
    "import torch\n",
    "import sys\n",
    "import os \n",
    "# Add the path to the src directory\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "set_seed(1234)\n",
    "\n",
    "from src.model_loader.llama_loader import load_llama\n",
    "model, tok = load_llama(MODEL_NAME)\n",
    "model.eval()\n",
    "\n",
    "encoding = tok([\"Hi there, how are you?\"], return_tensors=\"pt\").to(\"cuda:0\")\n",
    "with torch.no_grad():\n",
    "    generation_output = model.generate(**encoding, return_dict_in_generate=True, output_logits=True)\n",
    "\n",
    "sequences = generation_output.sequences\n",
    "sanity_check_logits = generation_output.logits\n",
    "\n",
    "with torch.no_grad():\n",
    "    model_output = model(input_ids=encoding['input_ids'])\n",
    "\n",
    "# Vérification des logits\n",
    "prompt_len = encoding['input_ids'].shape[1]\n",
    "total_len = sequences.shape[1]\n",
    "generated_len = total_len - prompt_len\n",
    "\n",
    "print(f\"Longueur du prompt: {prompt_len}\")\n",
    "print(f\"Longueur totale des sequences: {total_len}\")\n",
    "print(f\"Longueur générée: {generated_len}\")\n",
    "print(f\"Nombre de logits générés: {len(generation_output.logits)}\")\n",
    "\n",
    "# Vérification que sequences = prompt + génération\n",
    "print(f\"\\nPrompt original: {encoding['input_ids']}\")\n",
    "print(f\"Séquence complète: {sequences}\")\n",
    "print(f\"Partie prompt de sequences: {sequences[0, :prompt_len]}\")\n",
    "print(f\"Partie générée de sequences: {sequences[0, prompt_len:]}\")\n",
    "print(f\"Prompt == partie prompt ? {torch.equal(encoding['input_ids'][0], sequences[0, :prompt_len])}\")\n",
    "\n",
    "# Le dernier logit du prompt (à la position prompt_len-1) prédit le premier token généré\n",
    "last_prompt_logit = model_output.logits[:, -1, :].float()  # Dernier logit du prompt\n",
    "\n",
    "# Le premier logit de génération correspond au premier token généré\n",
    "first_gen_logit = generation_output.logits[0].float()  # Premier logit de la génération\n",
    "\n",
    "# Comparaison\n",
    "diff = torch.max(torch.abs(first_gen_logit - last_prompt_logit)).cpu().item()\n",
    "are_close = torch.allclose(first_gen_logit, last_prompt_logit, rtol=1e-5, atol=1e-8)\n",
    "\n",
    "print(f\"\\nLes logits sont-ils identiques ? {are_close}\")\n",
    "print(f\"Différence maximale: {diff:.10f}\")\n",
    "\n",
    "# Affichage pour debug\n",
    "print(f\"\\nShape du dernier logit du prompt: {last_prompt_logit.shape}\")\n",
    "print(f\"Shape du premier logit généré: {first_gen_logit.shape}\")\n",
    "\n",
    "# Vérification que le premier token généré correspond bien\n",
    "first_generated_token_id = sequences[0, prompt_len]  # Premier token après le prompt\n",
    "predicted_token_id = torch.argmax(last_prompt_logit, dim=-1)\n",
    "print(f\"\\nPremier token généré (ID): {first_generated_token_id}\")\n",
    "print(f\"Token prédit par le dernier logit du prompt (ID): {predicted_token_id.item()}\")\n",
    "print(f\"Les tokens correspondent-ils ? {first_generated_token_id == predicted_token_id.item()}\")\n",
    "\n",
    "# Décodage pour visualisation\n",
    "print(f\"\\nPremier token généré: '{tok.decode(first_generated_token_id)}'\")\n",
    "print(f\"Token prédit: '{tok.decode(predicted_token_id)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def compute_perplexity(\n",
    "        prompt_logits: torch.Tensor, \n",
    "        gen_logits: torch.Tensor,\n",
    "        prompt_input_ids: torch.Tensor, \n",
    "        gen_input_ids: torch.Tensor,\n",
    "        prompt_attention_mask: torch.Tensor,\n",
    "        gen_attention_mask: torch.Tensor,\n",
    "        mode: Literal[\"prompt\", \"generation\", \"promptGeneration\"] = \"prompt\",\n",
    "        min_k: float = None\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Computes the per-sample perplexity of language model outputs using logits \n",
    "    and corresponding input token IDs. Logits maked by 0 in the attention mask \n",
    "    are ignored in the computation of the perplexity. \n",
    "\n",
    "    Perplexity is defined as:\n",
    "        Perplexity = exp(- mean(log P(token_i | context)))\n",
    "\n",
    "    NOTE: This implementation is inspired by:\n",
    "    \"LLM-Check: Investigating Detection of Hallucinations in Large Language Models\"\n",
    "    (Sriramanan et al., 2024)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    prompt_logits : torch.Tensor\n",
    "        Tensor of shape (batch_size, prompt_len, vocab_size) \n",
    "        These are the model's output logits obtained from a standard forward pass over the prompt sequence.\n",
    "    gen_logits : torch.Tensor\n",
    "        Tensor of shape (batch_size, gen_len, vocab_size).\n",
    "        These are the logits obtained during autoregressive decoding using `model.generate()`.\n",
    "    prompt_input_ids : torch.Tensor\n",
    "        Tensor of shape (batch_size, prompt_len), containing the input token IDs for the prompt.\n",
    "    gen_input_ids : torch.Tensor\n",
    "        Tensor of shape (batch_size, gen_len), containing the token IDs generated by the model.\n",
    "    prompt_attention_mask: torch.Tensor\n",
    "        Tensor of shape (batch_size, Tensor), 1 where token valid, 0 for padding.\n",
    "    gen_attention_mask: torch.Tensor  \n",
    "        Tensor of shape (batch_size, gen_len), 1 where token valid, 0 for padding.\n",
    "    mode : str, optional\n",
    "        One of {\"prompt\", \"generation\", \"promptGeneration\"}:\n",
    "        - \"prompt\": compute perplexity only over the prompt.\n",
    "        - \"generation\": compute perplexity only over the generated tokens.\n",
    "        - \"promptGeneration\": compute perplexity over both prompt and generation.\n",
    "    min_k : float, optional\n",
    "        Optional value between 0 and 1. If specified, only the bottom-k lowest-probability\n",
    "        tokens are used for perplexity calculation.\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "        np.ndarray: Per-sample perplexity scores of shape (batch_size,)\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    About token shifting in autoregressive models:\n",
    "\n",
    "    In a standard autoregressive forward pass:\n",
    "        - At step *t*, the model predicts the token at position *t* based on the tokens up to *t-1*.\n",
    "        - Thus, the logit at position *t* predicts the token at position *t+1*.\n",
    "        - The first token has no preceding context and is not predicted.\n",
    "        - When computing log-probabilities, we must **shift the targets one position to the left** \n",
    "        to correctly align logits with target tokens.\n",
    "        \n",
    "        Example: Suppose we have a sequence of tokens (with their token IDs):\n",
    "        | Index | Token | ID  |\n",
    "        |-------|-------|-----| - The model produces logits at positions 0, 1, \n",
    "        | 0     | A     | 10  | and 2 to predict the tokens B, C, and D, respectively.\n",
    "        | 1     | B     | 29  |\n",
    "        | 2     | C     | 305 |  - The logits at position 0 are used to predict\n",
    "        | 3     | D     | 24  |  token B (ID 29).\n",
    "\n",
    "    During generation (e.g., using model.generate()):\n",
    "        - The logit at time step *t* predicts the token generated at position *t*.\n",
    "        - Each logit already corresponds to the prediction of the token at this step \n",
    "        - No shifting is needed in this case.\n",
    "\n",
    "    Summary of alignment:\n",
    "        - Prompt: logit at position *t* predicts token at position *t+1* -> shift targets left.\n",
    "        - Generation: logit at position *t* predicts token at position *t* -> no shift.\n",
    "\n",
    "    NOTE: help from issue https://github.com/huggingface/transformers/issues/29664\n",
    "    \"\"\"\n",
    "\n",
    "    softmax = torch.nn.Softmax(dim=-1)\n",
    "\n",
    "    # Apply softmax over vocabulary dimension and take log to get log-probabilities\n",
    "    prompt_log_probs = torch.log(softmax(prompt_logits))  # shape: (batch_size, prompt_len, vocab_size)\n",
    "    gen_log_probs = torch.log(softmax(gen_logits))        # shape: (batch_size, gen_len, vocab_size)\n",
    "\n",
    "    if mode in (\"prompt\", \"promptGeneration\"):\n",
    "        # In prompt: logit at position t predicts token at t+1 (requires shifting)\n",
    "        # Remove first token from target (no context to predict it)\n",
    "        prompt_target_tokens = prompt_input_ids[:, 1:] # (batch_size, prompt_len - 1)\n",
    "        \n",
    "        # Remove last logit position (since it predicts next token)\n",
    "        prompt_pred_log_probs = prompt_log_probs[:, :-1, :] # shape: (batch_size, prompt_len - 1, vocab_size)\n",
    "        \n",
    "        # Retrieves, for each position and each batch, the log-probability corresponding to the next token \n",
    "        # (the one in target_tokens) from all the probas on the vocabulary.\n",
    "        prompt_token_log_probs = prompt_pred_log_probs.gather(\n",
    "            dim=2, index=prompt_target_tokens.unsqueeze(-1)\n",
    "            ).squeeze(-1) # shape: (batch_size, prompt_len - 1)\n",
    "    \n",
    "    if mode in (\"prompt\", \"promptGeneration\"):\n",
    "        # In generation: logit at position t predicts token at position t (no shift needed)\n",
    "        gen_token_log_probs = gen_log_probs.gather(\n",
    "            dim=2, index=gen_input_ids.unsqueeze(-1)\n",
    "            ).squeeze(-1)  # shape: (batch_size, gen_len)\n",
    "        \n",
    "    if mode == \"promptGeneration\":\n",
    "        # Last logit of prompt from the forward pass == first logit of generation from `model.generate()`. \n",
    "        # To compute perplexity over the full sequence:\n",
    "        # - Use prompt_token_log_probs (excluding final prompt token)\n",
    "        # - Use gen_token_log_probs from generation\n",
    "        # Concatenate both to form a complete sequence of predicted log-probs\n",
    "        prompt_gen_token_log_probs = torch.cat(\n",
    "            [prompt_token_log_probs, gen_token_log_probs], dim=1\n",
    "        )  # shape: (batch_size, prompt_len - 1 + gen_len)\n",
    "\n",
    "    # Select the appropriate token log-probabilities based on mode\n",
    "    if mode == \"prompt\":\n",
    "        token_log_probs = prompt_token_log_probs  # (batch_size, prompt_len - 1)\n",
    "    elif mode == \"generation\":\n",
    "        token_log_probs = gen_token_log_probs # (batch_size, gen_len)\n",
    "    elif mode == \"promptGeneration\":\n",
    "        token_log_probs = prompt_gen_token_log_probs # (batch_size, prompt_len - 1 + gen_len)\n",
    "\n",
    "    # Optionally focus only on the k% hardest tokens (lowest log-probs)\n",
    "    if min_k is not None:\n",
    "        # Keep only the min_k fraction of tokens with the lowest log-probs \n",
    "        k = int(min_k * token_log_probs.size(1))  # number of tokens to keep per sample\n",
    "        \n",
    "        # Use topk with largest=False to get the k tokens with the lowest log-probabilities\n",
    "        topk_vals, _ = torch.topk(token_log_probs, k=k, dim=1, largest=False)\n",
    "\n",
    "        # Compute perplexity using only the selected subset\n",
    "        ppls = torch.exp(-topk_vals.mean(dim=1))\n",
    "    else:\n",
    "        # Compute perplexity over all predicted tokens\n",
    "        ppls = torch.exp(-token_log_probs.mean(dim=1))\n",
    "\n",
    "    return ppls.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def compute_attn_eig_prod(\n",
    "    prompt_attentions: torch.Tensor,\n",
    "    generation_attentions: List[torch.Tensor],\n",
    "    attentions: List[torch.Tensor], \n",
    "    mode: Literal[\"prompt\", \"generation\", \"promptGeneration\"] = \"promptGeneration\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute a mean log-diagonal attention score (eigenvalue-inspired) for a single layer's attention map.\n",
    "    NOTE: Implementation inspired from \n",
    "    \"LLM-Check: Investigating Detection of Hallucinations in Large Language Models\"\n",
    "    (Sriramanan et al. 2024)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    attentions : list of torch.Tensor: [attn_prompt, attn_gen1, attn_gen2, ...]\n",
    "        - attentions[0]: Tensor of shape (batch_size, n_heads, prompt_len, prompt_len)\n",
    "            Self-attention over the prompt tokens. \n",
    "        - attentions[1:]: List of tensors of shape(batch_size, n_heads, 1, prompt_len + t)\n",
    "            Self-attention for each generated token at generation step t (t >= 1).\n",
    "     mode : str, optional\n",
    "        Specifies which part of the attention map to use for the score computation.\n",
    "        Must be one of the following:\n",
    "        - \"prompt\":\n",
    "            Only uses the prompt self-attention matrix (attentions[0]).\n",
    "            The diagonal (i.e., self-attention values per token) is extracted,\n",
    "            then the log is taken, followed by a mean over prompt tokens and sum over heads.\n",
    "        - \"generation\":\n",
    "            Only uses the generated self-attention maps (attentions[1:]).\n",
    "            Each tensor in attentions[1:] has shape (batch_size, n_heads, 1, prompt_len + t),\n",
    "            where t is the generation step. \n",
    "            Intead of concatenating these tensors to obtain the generation attention matrix, \n",
    "            for each step, we directly take the last value along the last axis (i.e., the self-attention\n",
    "            of the newly generated token). These values are stacked across time steps, then we take the log,\n",
    "            compute the mean over time, and sum over heads.\n",
    "        - \"prompt+generation\":\n",
    "            Combines the diagonals from both the prompt and generation attention maps as described above\n",
    "            for \"prompt\" and \"generation\" mode. The two diagonals are concatenated along the token/time axis, \n",
    "            then the log is taken, followed by a mean across all tokens and a sum over heads.\n",
    "            Note: we do **not** concatenate the full prompt and generation attention matrices,\n",
    "            since the diagonal of the combined matrix would only include values from the prompt attention\n",
    "            due to mismatched matrix shapes.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        A NumPy array of shape (batch_size,), where each value is the per-sample attention score.\n",
    "        The score is summed across heads and averaged across tokens (in log-space).\n",
    "    \"\"\"\n",
    "    assert mode in (\"prompt\", \"generation\", \"promptGeneration\"), \"Invalid mode.\"\n",
    "    \n",
    "    prompt_attentions = attentions[0]\n",
    "    gen_attentions = attentions[1:]\n",
    "\n",
    "    batch_size, n_heads = prompt_attentions.shape[:2]\n",
    "    n_generated = len(attentions) - 1\n",
    "\n",
    "    diag_blocks = []\n",
    "\n",
    "    if mode in (\"prompt\", \"promptGeneration\"):\n",
    "        prompt_diag = torch.diagonal(prompt_attentions, dim1=-2, dim2=-1) # (batch_size, n_heads, prompt_len)\n",
    "        diag_blocks.append(prompt_diag)\n",
    "        print(\"prompt_diag.shape\", prompt_diag.shape)\n",
    "\n",
    "    if mode in (\"generation\", \"promptGeneration\") and n_generated > 0:\n",
    "        # For each generation step, take the (batch_size, n_heads, 1, prompt_len + t)\n",
    "        # => keep last value of last axis (self-attention of generated token)\n",
    "        gen_diag_steps = [attn[..., -1].squeeze(-1) for attn in gen_attentions]  # list of (batch_size, n_heads)\n",
    "        gen_debug = [attn[..., -1] for attn in gen_attentions] \n",
    "        print(\"len(gen_diag_steps): \", len(gen_diag_steps))\n",
    "        print(\"gen_diag_steps[0].shape:\", gen_diag_steps[0].shape)\n",
    "        print(\"gen_debug[0].shape:\", gen_debug[0].shape)\n",
    "        # Stack along newly generated tokens\n",
    "        gen_diag = torch.stack(gen_diag_steps, dim=-1) if gen_diag_steps else None # (batch_size, n_heads, n_generated)\n",
    "        if gen_diag is not None:\n",
    "            diag_blocks.append(gen_diag)\n",
    "            print(\"gen_diag.shape: \", gen_diag.shape)\n",
    "\n",
    "    # Now concatenate along token axis\n",
    "    all_diags = torch.cat(diag_blocks, dim=-1) # (batch_size, n_heads, N) where N = prompt_len + n_generated (or a subset)\n",
    "    print(\"all_diags.shape:\", all_diags.shape)\n",
    "\n",
    "    # Take log, mean over N tokens, sum over heads\n",
    "    all_diags = all_diags.clamp(min=1e-6)\n",
    "    log_diag = torch.log(all_diags).mean(dim=-1) # (batch_size, n_heads)\n",
    "    print(\"log_diag.shape: \", log_diag.shape)\n",
    "    scores = log_diag.sum(dim=-1).cpu().numpy()  # sum over n_heads\n",
    "    print(\"scores.shape: \", scores.shape)\n",
    "\n",
    "    return scores # (batch_size,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def compute_logit_entropy(\n",
    "    prompt_logits: torch.Tensor,\n",
    "    gen_logits: torch.Tensor,\n",
    "    prompt_attention_mask: torch.Tensor,\n",
    "    gen_attention_mask: torch.Tensor,\n",
    "    mode: str = \"prompt\",\n",
    "    top_k: int = None\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the per-sample entropy of a language model's output distributions\n",
    "    using its logits and attention masks.\n",
    "    For each token position, the function computes the entropy of the softmax distribution\n",
    "    over the vocabulary. Entropy is averaged over the valid tokens (i.e., those marked\n",
    "    as 1 in the attention mask). If `top_k` is specified, the entropy is computed only\n",
    "    over the top-k logits (highest values) for each position.\n",
    "\n",
    "    Entropy is defined as:\n",
    "        Entropy = -Sum_i p_i * log(p_i)\n",
    "        where p_i = softmax(logits)_i\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    prompt_logits : torch.Tensor\n",
    "        Tensor of shape (batch_size, prompt_len, vocab_size).\n",
    "    gen_logits : torch.Tensor\n",
    "        Tensor of shape (batch_size, gen_len, vocab_size).\n",
    "    prompt_attention_mask : torch.Tensor\n",
    "        Tensor of shape (batch_size, prompt_len). Contains 1 where valid token, 0 for padding.\n",
    "    gen_attention_mask : torch.Tensor  \n",
    "        Tensor of shape (batch_size, gen_len). Contains 1 where valid token, 0 for padding.\n",
    "    mode : str, optional\n",
    "        One of {\"prompt\", \"generation\", \"promptGeneration\"}:\n",
    "        - \"prompt\": compute entropy only over the prompt tokens.\n",
    "        - \"generation\": compute entropy only over generated tokens.\n",
    "        - \"promptGeneration\": compute entropy over both prompt and generated tokens.\n",
    "    top_k : int, optional\n",
    "        If specified, compute entropy only over the top-k logits per token.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Per-sample entropy values, shape (batch_size,).\n",
    "    \"\"\"\n",
    "\n",
    "    def entropy_from_logits(logits, attention_mask, top_k=None):\n",
    "        print(f\"[DEBUG] Computing entropy from logits of shape {logits.shape}\")\n",
    "        print(f\"[DEBUG] Attention mask shape: {attention_mask.shape}\")\n",
    "\n",
    "        # Convert float12 -> float32 for better accuracy during computations\n",
    "        logits = logits.float()\n",
    "        attention_mask = attention_mask.float()\n",
    "\n",
    "        # AJOUT : Vérifier les logits d'entrée\n",
    "        print(f\"[DEBUG] Logits sample (batch0, first 5 tokens, first 10 vocab): {logits[0, :5, :10]}\")\n",
    "        print(f\"[DEBUG] Logits min/max: {logits.min()}, {logits.max()}\")\n",
    "        print(f\"[DEBUG] Attention mask sample (batch0, first 10): {attention_mask[0, :10]}\")\n",
    "\n",
    "        if top_k is not None:\n",
    "            topk_vals = torch.topk(logits, k=top_k, dim=-1).values  # (batch_size, seq_len, top_k)\n",
    "            print(f\"[DEBUG] Selected top_k={top_k} logits shape: {topk_vals.shape}\")\n",
    "            probs = F.softmax(topk_vals, dim=-1)  # (batch_size, seq_len, top_k)\n",
    "        else:\n",
    "            probs = F.softmax(logits, dim=-1)  # (batch_size, seq_len, vocab_size)\n",
    "            print(f\"[DEBUG] Softmax probabilities shape: {probs.shape}\")\n",
    "\n",
    "        # AJOUT : Vérifier les probabilités\n",
    "        print(f\"[DEBUG] Probs sample (batch0, first 5 tokens, first 10 vocab): {probs[0, :5, :10]}\")\n",
    "        print(f\"[DEBUG] Probs sum per token (should be ~1): {probs[0, :5].sum(dim=-1)}\")\n",
    "\n",
    "        epsilon = 1e-12  # Plus petit epsilon 1e-9\n",
    "        log_probs = torch.log(probs + epsilon)  # numerical stability\n",
    "        print(f\"[DEBUG] probs sample values (batch0, first 5 tokens): {probs[0, :5]}\")\n",
    "        print(f\"[DEBUG] log_probs sample values (batch0, first 5 tokens): {log_probs[0, :5]}\")\n",
    "        # AJOUT : Vérifier les log_probs\n",
    "        print(f\"[DEBUG] Log_probs sample (batch0, first 5 tokens, first 10 vocab): {log_probs[0, :5, :10]}\")\n",
    "        print(f\"[DEBUG] Log_probs min/max: {log_probs.min()}, {log_probs.max()}\")\n",
    "\n",
    "        product = (probs * log_probs)\n",
    "        print(f\"[DEBUG] product per token shape: {product.shape}\")\n",
    "        print(f\"HERE [DEBUG] product sample values (batch0, first 5 tokens): {product[0, :5]}\")\n",
    "        #entropy = -(probs* log_probs).sum(dim=-1)  # (batch_size, seq_len)\n",
    "\n",
    "        # Use torch.special.entr, which automatically handles edge cases.\n",
    "        # entropy(x) = -x * log(x) with entropy(0) = 0\n",
    "        entropy = torch.special.entr(probs).sum(dim=-1) # (batch_size, seq_len)\n",
    "\n",
    "        print(f\"[DEBUG] Entropy per token shape: {entropy.shape}\")\n",
    "        print(f\"HERE [DEBUG] Entropy sample values (batch0, first 5 tokens): {entropy[0, :5]}\")\n",
    "        # AJOUT : Vérifier l'entropie avant masquage\n",
    "        print(f\"[DEBUG] Entropy before masking (batch0, first 5): {entropy[0, :5]}\")\n",
    "        print(f\"[DEBUG] Entropy min/max: {entropy.min()}, {entropy.max()}\")\n",
    "        print(f\"[DEBUG] NaN count in entropy: {torch.isnan(entropy).sum()}\")\n",
    "    \n",
    "        entropy_masked = entropy * attention_mask  # Zero out padded tokens\n",
    "        print(f\"[DEBUG] entropy_masked per token shape: {entropy_masked.shape}\")\n",
    "        print(f\"[DEBUG] entropy_masked sample values (batch0, first 5 tokens): {entropy_masked[0, :5]}\")\n",
    "        total_entropy = entropy_masked.sum(dim=-1)  # sum over seq_len, (batch_size,)\n",
    "        valid_token_count = attention_mask.sum(dim=-1)  # (batch_size,)\n",
    "        print(f\"[DEBUG] Total entropy per sample: {total_entropy}\")\n",
    "        print(f\"[DEBUG] Valid token counts per sample: {valid_token_count}\")\n",
    "\n",
    "        return total_entropy.cpu().numpy(), valid_token_count.cpu().numpy()\n",
    "\n",
    "    if mode == \"prompt\":\n",
    "        total_entropy, count = entropy_from_logits(prompt_logits, prompt_attention_mask, top_k)\n",
    "        avg_entropy = total_entropy / (count + 1e-9)\n",
    "        print(f\"[INFO] Mode 'prompt': avg_entropy shape {avg_entropy.shape}\")\n",
    "        print(f\"[INFO] Sample avg_entropy: {avg_entropy}\")\n",
    "        return avg_entropy\n",
    "\n",
    "    elif mode == \"generation\":\n",
    "        total_entropy, count = entropy_from_logits(gen_logits, gen_attention_mask, top_k)\n",
    "        avg_entropy = total_entropy / (count + 1e-9)\n",
    "        print(f\"[INFO] Mode 'generation': avg_entropy shape {avg_entropy.shape}\")\n",
    "        print(f\"[INFO] Sample avg_entropy: {avg_entropy}\")\n",
    "        return avg_entropy\n",
    "\n",
    "    elif mode == \"promptGeneration\":\n",
    "        ent_prompt, count_prompt = entropy_from_logits(prompt_logits, prompt_attention_mask, top_k)\n",
    "        ent_gen, count_gen = entropy_from_logits(gen_logits, gen_attention_mask, top_k)\n",
    "        total_ent = ent_prompt + ent_gen\n",
    "        total_count = count_prompt + count_gen\n",
    "        avg_entropy = total_ent / (total_count + 1e-9)\n",
    "        print(f\"[INFO] Mode 'promptGeneration': avg_entropy shape {avg_entropy.shape}\")\n",
    "        print(f\"[INFO] Sample avg_entropy: {avg_entropy}\")\n",
    "        return avg_entropy\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown mode: {mode}. Must be 'prompt', 'generation' or 'promptGeneration'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logit_entropy(\n",
    "    prompt_logits: torch.Tensor,\n",
    "    gen_logits: torch.Tensor,\n",
    "    prompt_attention_mask: torch.Tensor,\n",
    "    gen_attention_mask: torch.Tensor,\n",
    "    mode: str = \"prompt\",\n",
    "    top_k: int = None,\n",
    "    window_size: int = None,\n",
    "    stride: int = None\n",
    ") -> np.ndarray:\n",
    "    def entropy_from_logits(logits, attention_mask, top_k=None):\n",
    "        print(f\"\\n[DEBUG] Input logits shape: {logits.shape}\")\n",
    "        print(f\"[DEBUG] Input logits: {logits}\")\n",
    "        print(f\"[DEBUG] Input attention_mask shape: {attention_mask.shape}\")\n",
    "        print(f\"[DEBUG] Input attention_mask: {attention_mask}\")\n",
    "        \n",
    "        logits = logits.float()\n",
    "        attention_mask = attention_mask.float()\n",
    "        \n",
    "        # >>>>> masquer avant de sélectionner le top k non ? non car c'est sur la dim vocab size \n",
    "\n",
    "        if top_k is not None:\n",
    "            print(f\"[DEBUG] top_k activated: {top_k}\")\n",
    "            topk_vals = torch.topk(logits, k=top_k, dim=-1).values\n",
    "            print(f\"[DEBUG] topk_vals:\\n{topk_vals}\")\n",
    "            probs = F.softmax(topk_vals, dim=-1)\n",
    "        else:\n",
    "            print(f\"[DEBUG] Using full softmax\")\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        entropy = torch.special.entr(probs).sum(dim=-1)\n",
    "        print(f\"[DEBUG] Entropy shape: {entropy.shape}\")\n",
    "        print(f\"[DEBUG] Entropy example:\\n{entropy}\")\n",
    "        return entropy, attention_mask\n",
    "\n",
    "    def average_entropy(entropy, mask):\n",
    "        print(\"\\n[DEBUG] === AVERAGE ENTROPY ===\")\n",
    "        entropy_masked = entropy * mask\n",
    "        print(f\"[DEBUG] entropy_masked: {entropy_masked}\")\n",
    "        total_entropy = entropy_masked.sum(dim=-1)\n",
    "        valid_count = mask.sum(dim=-1)\n",
    "        print(f\"[DEBUG] total_entropy: {total_entropy}\")\n",
    "        print(f\"[DEBUG] valid_count: {valid_count}\")\n",
    "        avg_entropy = total_entropy / (valid_count + 1e-9)\n",
    "        print(f\"[DEBUG] avg_entropy: {avg_entropy}\")\n",
    "        return avg_entropy\n",
    "\n",
    "    def max_sliding_window_entropy(entropy, mask, w, stride):\n",
    "        print(\"\\n[DEBUG] === SLIDING WINDOW ENTROPY ===\")\n",
    "        entropy = entropy.unsqueeze(1)  # (B, 1, T)\n",
    "        mask = mask.unsqueeze(1)        # (B, 1, T)\n",
    "        print(f\"[DEBUG] entropy: \\n{entropy}\")\n",
    "        print(f\"[DEBUG] mask: \\n{mask}\")\n",
    "        kernel = torch.ones(1, 1, w, device=entropy.device) / w\n",
    "        print(f\"[DEBUG] kernel: \\n{kernel}\")\n",
    "        moving_avg = F.conv1d(entropy, kernel, stride=stride, padding=0)\n",
    "        valid_counts = F.conv1d(mask, kernel, stride=stride, padding=0)\n",
    "        valid_mask = (valid_counts == 1.0)\n",
    "\n",
    "        print(f\"[DEBUG] moving_avg shape: {moving_avg.shape}\")\n",
    "        print(f\"[DEBUG] moving_avg :\\n{moving_avg}\")\n",
    "        print(f\"[DEBUG] valid_counts:\\n{valid_counts}\")\n",
    "        print(f\"[DEBUG] valid_mask:\\n{valid_mask}\")\n",
    "        print(f\"[DEBUG] moving_avg (before masking):\\n{moving_avg}\")\n",
    "        \n",
    "        moving_avg = moving_avg.masked_fill(~valid_mask, float('-inf'))\n",
    "\n",
    "        print(f\"[DEBUG] moving_avg (after masking):\\n{moving_avg}\")\n",
    "        max_avg_entropy, _ = moving_avg.max(dim=-1)\n",
    "        return max_avg_entropy.squeeze(1)\n",
    "\n",
    "    print(\"\\n[DEBUG] compute_logit_entropy called\")\n",
    "    print(f\"[DEBUG] mode: {mode}, top_k: {top_k}, window_size: {window_size}, stride: {stride}\")\n",
    "    \n",
    "    if top_k is not None:\n",
    "        top_k = int(top_k)\n",
    "        if top_k <= 0 or top_k > prompt_logits.shape[2]:\n",
    "            raise ValueError(\"top_k must be a positive integer less or equal to vocab size\")\n",
    "\n",
    "    if window_size is not None:\n",
    "        if stride is None:\n",
    "            stride = window_size\n",
    "        else:\n",
    "            stride = int(stride)\n",
    "            if stride <= 0 or stride > window_size:\n",
    "                raise ValueError(\"stride must be a positive integer <= window_size.\")\n",
    "    else:\n",
    "        stride = None\n",
    "\n",
    "    if mode == \"prompt\":\n",
    "        entropy, mask = entropy_from_logits(prompt_logits, prompt_attention_mask, top_k)\n",
    "    elif mode == \"generation\":\n",
    "        entropy, mask = entropy_from_logits(gen_logits, gen_attention_mask, top_k)\n",
    "    elif mode == \"promptGeneration\":\n",
    "        ent_p, mask_p = entropy_from_logits(prompt_logits, prompt_attention_mask, top_k)\n",
    "        ent_g, mask_g = entropy_from_logits(gen_logits, gen_attention_mask, top_k)\n",
    "        entropy = torch.cat([ent_p, ent_g], dim=1)\n",
    "        mask = torch.cat([mask_p, mask_g], dim=1)\n",
    "        print(f\"[DEBUG] After concat: entropy shape = {entropy.shape}, mask shape = {mask.shape}\")\n",
    "    else:\n",
    "        raise ValueError(\"mode must be in {'prompt','generation','promptGeneration'}\")\n",
    "\n",
    "    if window_size is None:\n",
    "        result = average_entropy(entropy, mask)\n",
    "    else:\n",
    "        if window_size <= 0:\n",
    "            raise ValueError(\"window_size must be a positive integer\")\n",
    "        if window_size > entropy.shape[1]:\n",
    "            raise ValueError(\"window_size greater than sequence length\")\n",
    "        result = max_sliding_window_entropy(entropy, mask, window_size, stride)\n",
    "\n",
    "    print(f\"\\n[DEBUG] Final entropy per sample: {result}\")\n",
    "    return result.cpu().numpy()\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Dimensions\n",
    "batch_size = 2\n",
    "prompt_len = 5\n",
    "gen_len = 4\n",
    "vocab_size = 3\n",
    "\n",
    "# Logits aléatoires\n",
    "torch.manual_seed(42)\n",
    "prompt_logits = torch.randn(batch_size, prompt_len, vocab_size)\n",
    "gen_logits = torch.randn(batch_size, gen_len, vocab_size)\n",
    "\n",
    "# Masques d’attention avec padding (0 = padding)\n",
    "prompt_mask = torch.tensor([\n",
    "    [0, 0, 1, 1, 1],  # 3 tokens valides\n",
    "    [1, 1, 1, 1, 1],  # tous valides\n",
    "], dtype=torch.float32)\n",
    "\n",
    "gen_mask = torch.tensor([\n",
    "    [1, 1, 0, 0],     # 2 tokens valides\n",
    "    [1, 1, 1, 1],     # 3 valides\n",
    "], dtype=torch.float32)\n",
    "\n",
    "# Appel avec top_k et fenêtre\n",
    "result = compute_logit_entropy(\n",
    "    prompt_logits=prompt_logits,\n",
    "    gen_logits=gen_logits,\n",
    "    prompt_attention_mask=prompt_mask,\n",
    "    gen_attention_mask=gen_mask,\n",
    "    mode=\"promptGeneration\",\n",
    "    top_k=2,\n",
    "    window_size=1,\n",
    "    stride=1\n",
    ")\n",
    "\n",
    "print(\"\\n[TEST RESULT] Entropy scores:\\n\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def compute_logit_entropy(\n",
    "    prompt_logits: torch.Tensor,\n",
    "    gen_logits: torch.Tensor,\n",
    "    prompt_attention_mask: torch.Tensor,\n",
    "    gen_attention_mask: torch.Tensor,\n",
    "    mode: str = \"prompt\",\n",
    "    top_k: int = None\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the per-sample entropy of a language model's output distributions\n",
    "    using its logits and attention masks.\n",
    "    For each token position, the function computes the entropy of the softmax distribution\n",
    "    over the vocabulary. Entropy is averaged over the valid tokens (i.e., those marked\n",
    "    as 1 in the attention mask). If `top_k` is specified, the entropy is computed only\n",
    "    over the top-k logits (highest values) for each position.\n",
    "\n",
    "    Entropy is defined as:\n",
    "        Entropy = -Sum_i p_i * log(p_i)\n",
    "        where p_i = softmax(logits)_i\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    prompt_logits : torch.Tensor\n",
    "        Tensor of shape (batch_size, prompt_len, vocab_size).\n",
    "       These are the model's output logits obtained from a standard forward pass over the prompt sequence.\n",
    "    gen_logits : torch.Tensor\n",
    "        Tensor of shape (batch_size, gen_len, vocab_size).\n",
    "        These are the logits obtained during autoregressive decoding using `model.generate()`.\n",
    "    prompt_attention_mask : torch.Tensor\n",
    "        Tensor of shape (batch_size, prompt_len). Contains 1 where the token is valid and 0 for padding.\n",
    "    gen_attention_mask : torch.Tensor  \n",
    "        Tensor of shape (batch_size, gen_len). Contains 1 where the token is valid and 0 for padding.\n",
    "    mode : str, optional\n",
    "        One of {\"prompt\", \"generation\", \"promptGeneration\"}:\n",
    "        - \"prompt\": compute entropy only over the prompt tokens.\n",
    "        - \"generation\": compute entropy only over the generated tokens.\n",
    "        - \"promptGeneration\": compute entropy over both prompt and generated tokens.\n",
    "          In this case, entropies are summed over all valid tokens and averaged globally.\n",
    "    top_k : int, optional\n",
    "        If specified, compute entropy only over the top-k logits per token.\n",
    "        Useful for estimating uncertainty in the most likely predictions.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Per-sample entropy values as a tensor of shape (batch_size,).\n",
    "        Each value is the average entropy over valid tokens for that sample.\n",
    "    \"\"\"\n",
    "\n",
    "    def entropy_from_logits(logits, attention_mask, top_k=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ---------\n",
    "        logits: (batch_size, seq_len, vocab_size)\n",
    "        attention_mask: (batch_size, seq_len)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        total_entropy: (batch_size,)\n",
    "        valid_token_count: (batch_size,)\n",
    "        \"\"\"\n",
    "        # Convert float16 -> float32 for better accuracy during computations\n",
    "        logits = logits.float()\n",
    "        attention_mask = attention_mask.float()\n",
    "\n",
    "        if top_k is not None:\n",
    "            topk_vals = torch.topk(logits, k=top_k, dim=-1).values # (batch_size, seq_len, top_k)\n",
    "            probs = F.softmax(topk_vals, dim=-1) # (batch_size, seq_len, top_k)\n",
    "        else:\n",
    "            probs = F.softmax(logits, dim=-1) # (batch_size, seq_len, vocab_size)\n",
    "\n",
    "        # Use torch.special.entr, which automatically handles edge cases\n",
    "        # entropy(x) = -x * log(x) with entropy(0) = 0\n",
    "        entropy = torch.special.entr(probs).sum(dim=-1) # (batch_size, seq_len)\n",
    "\n",
    "        entropy_masked = entropy * attention_mask       # (batch_size, seq_len)\n",
    "        total_entropy = entropy_masked.sum(dim=-1)      # (batch_size,)\n",
    "        valid_token_count = attention_mask.sum(dim=-1)  # (batch_size,)\n",
    "\n",
    "        return total_entropy.cpu().numpy(), valid_token_count.cpu().numpy()  # both are (batch_size,)\n",
    "    \n",
    "    if top_k is not None:\n",
    "        top_k = int(top_k)\n",
    "        if top_k < 0 or top_k > prompt_logits.shape[2]: raise ValueError(\"top_k must be an integer between 0 and vocab_size\")\n",
    "\n",
    "    if mode == \"prompt\":\n",
    "        total_entropy, count = entropy_from_logits(prompt_logits, prompt_attention_mask, top_k)\n",
    "        return total_entropy / (count + 1e-9) # (batch_size,)\n",
    "\n",
    "    elif mode == \"generation\":\n",
    "        total_entropy, count = entropy_from_logits(gen_logits, gen_attention_mask, top_k)\n",
    "        return total_entropy / (count + 1e-9) # (batch_size,)\n",
    "\n",
    "    elif mode == \"promptGeneration\":\n",
    "        # Combine prompt and gen entropies\n",
    "        ent_prompt, count_prompt = entropy_from_logits(prompt_logits, prompt_attention_mask, top_k)\n",
    "        ent_gen, count_gen = entropy_from_logits(gen_logits, gen_attention_mask, top_k)\n",
    "\n",
    "        total_ent = ent_prompt + ent_gen         # (batch_size,)\n",
    "        total_count = count_prompt + count_gen   # (batch_size,)\n",
    "        return total_ent / (total_count + 1e-9)  # (batch_size,)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown mode: {mode}. Must be 'prompt', 'generation' or 'promptGeneration'\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# argument pour spécifier ce qu'on récupère comme aggrégation de tokens !!! \n",
    "\n",
    "def run_prompt_and_generation_activation_extraction(\n",
    "    model: PreTrainedModel,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    dataset: Dataset,\n",
    "    batch_size: int = 4,\n",
    "    idx_start_sample: int = 0,\n",
    "    max_samples: int = 1000,\n",
    "    save_to_pkl: bool = False,\n",
    "    output_path: str = \"outputs/all_batch_results.pkl\",\n",
    "    build_prompt_fn: Callable[[str, str], str] = None,\n",
    "    register_generation_activation_hook_fn: Callable = None,\n",
    "    layers: List[int] = [-1],  \n",
    "    extract_token_activations_fn: Callable = None,\n",
    "    activation_source: Literal[\"prompt\", \"generation\", \"promptGeneration\"] = \"generation\",\n",
    "    k_beams : int = 1,\n",
    "    start_offset : int = 0,\n",
    "    end_offset : int = 0,\n",
    ") -> Union[List[torch.Tensor], None]:\n",
    "    \"\"\"\n",
    "    Runs batched inference on a dataset using a decoder-only language model.\n",
    "    For each batch, it performs text generation and extracts token-level hidden activations \n",
    "    (both from the prompt and the generated text depending on `activation_source`) \n",
    "    from specified transformer layers.\n",
    "\n",
    "    Hidden states are captured via a forward hook during generation, then aligned and \n",
    "    filtered using attention masks. \n",
    "    These activations are saved as individual batch files in a specified pickle directory, \n",
    "    allowing efficient incremental storage and later aggregation.\n",
    "    Alternatively, the representations can be returned directly.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : PreTrainedModel\n",
    "        The causal language model to evaluate (e.g., LLaMA).\n",
    "    tokenizer : PreTrainedTokenizer\n",
    "        The corresponding tokenizer.\n",
    "    dataset : Dataset\n",
    "        The input dataset.\n",
    "    batch_size : int\n",
    "        Number of samples per batch.\n",
    "    idx_start_sample : int\n",
    "        Index of the first sample to process from the dataset.\n",
    "    max_samples : int\n",
    "        Total number of examples to process from the dataset, starting from idx_start_sample. \n",
    "    save_to_pkl : bool\n",
    "        If True, activations are appended to the pickle file at output_path.\n",
    "        If False, the function returns a list of activations.\n",
    "    output_path : str\n",
    "        Path to the directory where extracted answers will be saved as individual pickle batch files.\n",
    "    build_prompt_fn : Callable\n",
    "        Function to build a prompt from context and question.\n",
    "    register_generation_activation_hook_fn : Callable\n",
    "        Function that registers a forward hook on the model during autoregressive text generation.\n",
    "    layers : List[int]\n",
    "        List of indices of the transformer layers to extract activations from (default: [-1] for last layer).\n",
    "    extract_token_activations_fn : Callable\n",
    "        Function that selects and aggregates token-level activations. \n",
    "    activation_source : {\"prompt\", \"generation\", \"promptGeneration\"}\n",
    "        Which part of the sequence to extract activations from:\n",
    "        - \"prompt\": only from the prompt\n",
    "        - \"generation\": only from the generated answer\n",
    "        - \"promptGeneration\": prompt and generation answer both concatenated\n",
    "    k_beams : int, optional\n",
    "        Number of beams for beam search during generation (default: 1). If 1, uses sampling. \n",
    "    start_offset : int\n",
    "        Offset from the first non-padding token (must be >= 0). \n",
    "    end_offset : int\n",
    "        Offset from the last non-padding token (must be <= 0, e.g., -3 to remove 3 tokens).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Union[\n",
    "        List[torch.Tensor],\n",
    "        None\n",
    "    ]\n",
    "        If save_to_pkl is False \n",
    "            Returns batch_activations: list of length `num_samples`, each element is a tensor \n",
    "            of shape (1, hidden_size), containing the selected and aggragated token activations.\n",
    "        If save_to_pkl is True:\n",
    "            Returns None (activations are saved incrementally to output_path).\n",
    "    \"\"\"\n",
    "\n",
    "    hidden_scores = [\"average\", \"last\", \"max\", \"first_generated\", \"token_svd_score\", \"feat_var\"]\n",
    "    attn_scores  = [\"attn_eig_prod\"]\n",
    "    logit_scores = [\"perplexity\", \"logit_entropy\", \"window_logit_entropy\"] #=> avec la config qui va avec ??\n",
    "\n",
    "    if activation_source not in ('prompt', 'generation', 'promptGeneration'):\n",
    "        raise ValueError(\n",
    "                f\"Invalid value for `activation_source`: '{activation_source}'. \"\n",
    "                f\"Expected one of: ['prompt', 'generation', 'promptGeneration'].\"\n",
    "            )    \n",
    "        \n",
    "    batch_activations = []  # Chosen token activation vectors\n",
    "\n",
    "    # ==============================\n",
    "    # Patch selected layer(s) with custom LlamaAttention Forward function to retrieve attention weights\n",
    "    # ==============================\n",
    "    for idx in layers:  \n",
    "        model.model.layers[idx].self_attn.forward = patched_LlamaAttention_forward.__get__(\n",
    "            model.model.layers[idx].self_attn,\n",
    "            model.model.layers[idx].self_attn.__class__\n",
    "    )\n",
    "\n",
    "    for i in tqdm(range(idx_start_sample, idx_start_sample + max_samples, batch_size)):\n",
    "        \n",
    "        # ==============================\n",
    "        # Prepare input batch\n",
    "        # ==============================\n",
    "        batch = extract_batch(dataset, i, batch_size)\n",
    "        prompts = [build_prompt_fn(s[\"context\"], s[\"question\"]) for s in batch]\n",
    "        inputs = tokenizer(prompts, padding=True, truncation=True, return_tensors=\"pt\").to(model.device)\n",
    "        prompt_ids = inputs[\"input_ids\"] # (batch_size, prompt_len)\n",
    "        prompt_len = prompt_ids.shape[1] # Assumes prompts are padded to same length\n",
    "\n",
    "        print(f\"[INFO] prompt_ids shape: {prompt_ids.shape}, prompt_len: {prompt_len}\")\n",
    "\n",
    "        # ==============================\n",
    "        # Register forward hook to capture layer output\n",
    "        # ==============================\n",
    "        # This hook collects the hidden states at each decoding step. For layer l: \n",
    "        # activations_lists[l] = [act_prompt, act_gen_step1, ..., act_gen_step49] of length 50, if max_new_tokens=50.\n",
    "        # activations_lists[l][k] of shape: (batch_size, seq_len, hidden_size) \n",
    "        activations_lists = [[] for _ in layers]  # one empty list per layer \n",
    "        handle_act, call_counter_act = register_generation_activation_hook(model, activations_lists, layers)\n",
    "\n",
    "        # This hook collects the activations at each decoding step. For layer l: \n",
    "        # attentions_lists[l] = [attn_prompt, attn_gen_step1, ..., attn_gen_step49], of length 50, if max_new_tokens=50.\n",
    "        # activations_lists[l][k] of shape: (batch_size, n_heads, tgt_seq_len, src_seq_len)\n",
    "        #   tgt_seq_len: length of the sequence the model is currently producing (query)\n",
    "        #   src_seq_len: length of the sequence the model is focusing on (key/value)\n",
    "        attentions_lists = [[] for _ in layers]  # one empty list per layer\n",
    "        handle_attn, call_counter_attn = register_generation_attention_hook(model, attentions_lists, layers)\n",
    "\n",
    "        # ==============================\n",
    "        # Forward pass to the model to retrieve prompt logits \n",
    "        # ==============================\n",
    "        if len(logit_scores) > 0:\n",
    "            gen_logits = torch.stack(outputs.logits, dim=1) \n",
    "            with torch.no_grad():\n",
    "                prompt_logits = model(input_ids=inputs[\"input_ids\"]).logits\n",
    "        \n",
    "        # ==============================\n",
    "        # Run model generation (hook captures activations and attentions)\n",
    "        # ==============================\n",
    "        # When target layers are reached, hooks execute and saves their output in activations and attentions\n",
    "        print(\"[INFO] Starting generation...\")\n",
    "        outputs = generate(model, inputs, tokenizer, max_new_tokens=50, k_beams=k_beams)\n",
    "        gen_ids = outputs.sequences[:, prompt_len:]\n",
    "        print(f\"[INFO] gen_ids shape: {gen_ids.shape}\")\n",
    "        print(f\"[INFO] Sample generated tokens: {gen_ids}\")\n",
    "\n",
    "        # Remove hooks to avoid memory leaks or duplicate logging\n",
    "        for h in handle_act: h.remove()\n",
    "        for h in handle_attn: h.remove()\n",
    "        \n",
    "        # Verify that hooks worked properly\n",
    "        verify_call_counters(call_counter_act, name=\"activation hooks\")\n",
    "        verify_call_counters(call_counter_attn, name=\"attention hooks\")\n",
    "\n",
    "        # Retrieve text of generated answers\n",
    "        gen_answers = tokenizer.batch_decode(\n",
    "            outputs.sequences[:, prompt_len:], \n",
    "            skip_special_tokens=True\n",
    "        ) # (batch_size,)\n",
    "        print(f\"[INFO] Sample generated answer: {gen_answers}\")\n",
    "        \n",
    "        # ===============================\n",
    "        # Build generation and prompt attention mask\n",
    "        # ===============================\n",
    "        # This mask marks which generated tokens are valid (i.e., not padding).\n",
    "        # Positions are marked True up to and including the first eos_token_id\n",
    "        generation_attention_mask = build_generation_attention_mask(\n",
    "            gen_ids=gen_ids, \n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        ) # (batch_size, gen_len)\n",
    "\n",
    "        prompt_attention_mask = inputs[\"attention_mask\"] \n",
    "        # (batch_size, prompt_len)\n",
    "\n",
    "        print(f\"[INFO] prompt_attention_mask: {prompt_attention_mask.shape}\")\n",
    "        print(f\"[INFO] generation_attention_mask: {generation_attention_mask.shape}\")\n",
    "        print(f\"[INFO] generation_attention_mask: {generation_attention_mask}\")\n",
    "\n",
    "        # Modify prompt attention mask with offsets\n",
    "        if start_offset !=0 or end_offset !=0:\n",
    "            print(f\"[INFO] Offsetting prompt_attention_mask with start={start_offset}, end={end_offset}\")\n",
    "            prompt_attention_mask, start_indices, end_indices = compute_offset_attention_mask(\n",
    "                attention_mask=prompt_attention_mask, \n",
    "                start_offset=start_offset, \n",
    "                end_offset=end_offset\n",
    "            ) # (batch_size, prompt_len), (batch_size,), (batch_size,)\n",
    "\n",
    "        print(f\"[INFO] New prompt_attention_mask shape: {prompt_attention_mask.shape}\")\n",
    "        print(f\"[INFO] New prompt_attention_mask : {prompt_attention_mask}\")\n",
    "\n",
    "        # Concatenate the prompt and generation attention mask\n",
    "        prompt_and_gen_attention_mask = torch.cat(\n",
    "            [prompt_attention_mask,\n",
    "            generation_attention_mask],\n",
    "            dim=1\n",
    "        ) # (batch_size, prompt_len + gen_len)\n",
    "\n",
    "        print(f\"[INFO] prompt_and_gen_attention_mask shape: {prompt_and_gen_attention_mask.shape}\")\n",
    "        print(f\"[INFO] prompt_and_gen_attention_mask : {prompt_and_gen_attention_mask}\")\n",
    "\n",
    "        # ===============================\n",
    "        # Truncate generated token IDs and mask to match activations and attentions\n",
    "        # ===============================\n",
    "        # When N tokens are generated, only the first N-1 tokens have corresponding hidden states.\n",
    "        # So activations[1:] covers only the first N-1 steps. Therefore, we exclude the last\n",
    "        # generated token from outputs.sequences to match activations[1:]. Same for attentions.\n",
    "        truncated_gen_ids = gen_ids[:,:-1] # (gen_len-1,)\n",
    "        truncated_generation_attention_mask = generation_attention_mask[:,:-1] # (batch_size, gen_len-1)\n",
    "        truncated_prompt_and_gen_attention_mask = prompt_and_gen_attention_mask[:,:-1] # (batch_size, prompt_len + gen_len-1)\n",
    "\n",
    "        print(f\"[INFO] Truncated gen_ids shape: {truncated_gen_ids.shape}\")\n",
    "        print(f\"[INFO] Truncated generation_attention_mask shape: {truncated_generation_attention_mask.shape}\")\n",
    "        print(f\"[INFO] Truncated prompt_and_gen_attention_mask shape: {truncated_prompt_and_gen_attention_mask.shape}\")\n",
    "        print(f\"[INFO] Truncated gen_ids : {truncated_gen_ids}\")\n",
    "        print(f\"[INFO] Truncated generation_attention_mask : {truncated_generation_attention_mask}\")\n",
    "        print(f\"[INFO] Truncated prompt_and_gen_attention_mask : {truncated_prompt_and_gen_attention_mask}\")\n",
    "\n",
    "        # *******************************\n",
    "        # START: loop on layers\n",
    "        # *******************************\n",
    "        for l in range(len(layers)):\n",
    "            layer_idx = layers[l]\n",
    "            print(f\"\\n----- Layer {layer_idx} -----\")\n",
    "\n",
    "            activations = activations_lists[l]\n",
    "            attentions = attentions_lists[l]\n",
    "\n",
    "            print(\"============\")\n",
    "            print(\"[INFO] Length of activations:\", len(activations))\n",
    "            for i in range(len(activations)):\n",
    "                print(f\"[INFO] Shape  of activations[{i}]: {activations[i].shape}\") \n",
    "            print(\"============\")\n",
    "            print(\"[INFO] Length of attentions:\", len(attentions))\n",
    "            for i in range(len(attentions)):\n",
    "                print(f\"[INFO] Shape  of attentions[{i}]: {attentions[i].shape}\") \n",
    "            print(\"============\")\n",
    "\n",
    "            # Define prompt and generation hidden states \n",
    "            prompt_activations=activations[0]       # `[0]` to include only the prompt part \n",
    "            generation_activations=activations[1:]  # `[1:]` to exclude the prompt part \n",
    "            \n",
    "            # Define prompt and generation attention maps\n",
    "            prompt_attentions=attentions[0]         # `[0]` to include only the prompt part \n",
    "            generation_attentions=attentions[1:]    # `[1:]` to exclude the prompt part \n",
    "\n",
    "            print(f\"[DEBUG] prompt_activations shape: {prompt_activations.shape}\")\n",
    "            print(f\"[DEBUG] prompt_attentions shape: {prompt_attentions.shape}\")\n",
    "\n",
    "            # ===============================\n",
    "            # Align generated and prompt hidden states\n",
    "            # ===============================\n",
    "            # For each batch item, take the last generated hidden state at this step\n",
    "            stacked_generation_activations = torch.stack(\n",
    "                [h[:, -1, :] for h in generation_activations], dim=1\n",
    "            ) # (batch_size, gen_len, hidden_size)\n",
    "\n",
    "            print(f\"[DEBUG] stacked_generation_activations shape: {stacked_generation_activations.shape}\")\n",
    "\n",
    "            # Concatenate the prompt and generation aligned hidden states  \n",
    "            prompt_and_gen_activations = torch.cat(\n",
    "                [stacked_generation_activations, # (batch_size, gen_len, hidden_size)\n",
    "                prompt_activations],             # (batch_size, prompt_len, hidden_size)\n",
    "                dim=1\n",
    "            ) # (batch_size, prompt_len + gen_len, hidden_size)\n",
    "            \n",
    "            print(f\"[DEBUG] prompt_and_gen_activations shape: {prompt_and_gen_activations.shape}\")\n",
    "\n",
    "            # ==============================\n",
    "            # Extract token activations from captured layer, based on source\n",
    "            # ==============================\n",
    "            print(f\"[INFO] Activation source: {activation_source}\")\n",
    "\n",
    "            if len(hidden_scores) > 0:\n",
    "                if activation_source == \"generation\":\n",
    "                    # Return only the token activations from the generated answer \n",
    "                    selected_token_vecs = extract_token_activations(                 ##### extract_token_activations_fn \n",
    "                            selected_layer=stacked_generation_activations, \n",
    "                            attention_mask=truncated_generation_attention_mask, \n",
    "                            device=stacked_generation_activations.device,\n",
    "                            modes=hidden_scores,\n",
    "                        ) # (batch_size, hidden_size)\n",
    "                    \n",
    "                elif activation_source == \"prompt\":    \n",
    "                    # Return only the token activations from the prompt\n",
    "                    selected_token_vecs = extract_token_activations(\n",
    "                            selected_layer=prompt_activations, \n",
    "                            attention_mask=prompt_attention_mask, \n",
    "                            device=prompt_activations.device,\n",
    "                            modes=hidden_scores,\n",
    "                        ) # (batch_size, hidden_size)\n",
    "                    \n",
    "                else: # activation_source == \"promptGeneration\"\n",
    "                    # Return token activations from the concatenated prompt + generated answer \n",
    "                    selected_token_vecs = extract_token_activations(\n",
    "                            selected_layer=prompt_and_gen_activations, \n",
    "                            attention_mask=truncated_prompt_and_gen_attention_mask, \n",
    "                            device=prompt_and_gen_activations.device,\n",
    "                            skip_length=prompt_len,\n",
    "                            modes=hidden_scores,\n",
    "                            # skip_length: exclude prompt from computation if \n",
    "                            # mode=='first_generated' in `extract_token_activations_fn`\n",
    "                        ) # (batch_size, hidden_size)\n",
    "                \n",
    "            print(f\"[RESULT] selected_token_vecs sample:\\n{selected_token_vecs}\")\n",
    "\n",
    "            if 'attn_eig_prod' in attn_scores:\n",
    "                attn_eig_prod = compute_attn_eig_prod(\n",
    "                        prompt_attentions=prompt_attentions, \n",
    "                        generation_attentions=generation_attentions,\n",
    "                        prompt_attention_mask=prompt_attention_mask, \n",
    "                        generation_attention_mask=truncated_generation_attention_mask,\n",
    "                        mode=activation_source,\n",
    "                )\n",
    "                print(f\"[RESULT] attn_eig_prod:\\n{attn_eig_prod}\")\n",
    "            \n",
    "            if 'perplexity' in logit_scores:\n",
    "                perplexity = compute_perplexity(\n",
    "                    prompt_logits=prompt_logits, \n",
    "                    gen_logits=gen_logits,\n",
    "                    prompt_ids=prompt_ids, \n",
    "                    gen_ids=gen_ids,\n",
    "                    prompt_attention_mask=prompt_attention_mask,\n",
    "                    gen_attention_mask=generation_attention_mask,\n",
    "                    mode=activation_source,\n",
    "                    min_k=None\n",
    "                )\n",
    "                print(f\"[RESULT] perplexity:\\n{perplexity}\")\n",
    "\n",
    "            if 'logit_entropy' in logit_scores:\n",
    "                logit_entropy = compute_logit_entropy(\n",
    "                    prompt_logits=prompt_logits,\n",
    "                    gen_logits=gen_logits,\n",
    "                    prompt_attention_mask=prompt_attention_mask,\n",
    "                    gen_attention_mask=generation_attention_mask,\n",
    "                    mode=activation_source,\n",
    "                    top_k=50,\n",
    "                    window_size=None,\n",
    "                    stride=None\n",
    "                )\n",
    "                print(f\"[RESULT] logit_entropy:\\n{logit_entropy}\")\n",
    "            \n",
    "            if 'window_logit_entropy' in logit_scores:\n",
    "                window_logit_entropy = compute_logit_entropy(\n",
    "                    prompt_logits=prompt_logits,\n",
    "                    gen_logits=gen_logits,\n",
    "                    prompt_attention_mask=prompt_attention_mask,\n",
    "                    gen_attention_mask=generation_attention_mask,\n",
    "                    mode=activation_source,\n",
    "                    top_k=50,\n",
    "                    window_size=1,\n",
    "                    stride=1\n",
    "                )\n",
    "                print(f\"[RESULT] window_logit_entropy:\\n{window_logit_entropy}\")\n",
    "            \n",
    "\n",
    "        # *******************************\n",
    "        # END: loop on layers\n",
    "        # *******************************\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f12ee5ed731245939307c81aab548795",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt logits shape: torch.Size([1, 7, 32000])\n",
      "Gen logits shape: torch.Size([1, 5, 32000])\n",
      "Vérification que les logits correspondent (avec tolérance numérique) :\n",
      "Logits égaux ? False\n",
      "Token prédit par dernier logit prompt : I\n",
      "Token prédit par premier logit génération : I\n",
      "Tokens prédits identiques ? True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lila.roig/.env/ood_env/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/lila.roig/.env/ood_env/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/lila.roig/.env/ood_env/lib/python3.11/site-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# VERIFIER que dernier logit du prompt avec forward = premier logit avec model.generate()\n",
    "# CONCLUSION: ca fonctionne avec gpt2 mais pas avec LLama. \n",
    "\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Charger modèle et tokenizer\n",
    "'''model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "'''\n",
    "model, tokenizer = load_llama(MODEL_NAME)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Phrase prompt\n",
    "prompt_text = \"Hello, how are you?\"\n",
    "\n",
    "# Encoder prompt\n",
    "inputs = tokenizer(prompt_text, return_tensors=\"pt\")\n",
    "\n",
    "# Forward pass complet sur le prompt pour obtenir les logits\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, return_dict=True)\n",
    "    prompt_logits = outputs.logits  # shape: (1, prompt_len, vocab_size)\n",
    "\n",
    "# Générer la suite à partir du prompt, en conservant le cache pour extraire logits\n",
    "with torch.no_grad():\n",
    "    generate_outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=5,\n",
    "        output_scores=True,\n",
    "        output_logits=True, \n",
    "        return_dict_in_generate=True,\n",
    "        do_sample=False,\n",
    "        use_cache=False\n",
    "    )\n",
    "\n",
    "# Extraire les logits des tokens générés lors de la génération autoregressive\n",
    "gen_logits = torch.stack(generate_outputs.logits, dim=1)  # (1, gen_len, vocab_size)\n",
    "\n",
    "# Vérifier les shapes\n",
    "print(f\"Prompt logits shape: {prompt_logits.shape}\")\n",
    "print(f\"Gen logits shape: {gen_logits.shape}\")\n",
    "\n",
    "# Comparer le dernier logit du prompt avec le premier logit generé par generate()\n",
    "last_prompt_logit = prompt_logits[:, -1, :]\n",
    "first_gen_logit = gen_logits[:, 0, :]\n",
    "\n",
    "last_prompt_logit = last_prompt_logit.float()\n",
    "first_gen_logit = first_gen_logit.float()\n",
    "\n",
    "print(\"Vérification que les logits correspondent (avec tolérance numérique) :\")\n",
    "are_close = torch.allclose(last_prompt_logit, first_gen_logit, atol=1e-5)\n",
    "print(f\"Logits égaux ? {are_close}\")\n",
    "\n",
    "# Vérifier aussi les tokens prédit par ces logits\n",
    "last_prompt_token = last_prompt_logit.argmax(dim=-1)\n",
    "first_gen_token = first_gen_logit.argmax(dim=-1)\n",
    "\n",
    "print(f\"Token prédit par dernier logit prompt : {tokenizer.decode(last_prompt_token)}\")\n",
    "print(f\"Token prédit par premier logit génération : {tokenizer.decode(first_gen_token)}\")\n",
    "print(f\"Tokens prédits identiques ? {torch.equal(last_prompt_token, first_gen_token)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prompt_score_extraction(\n",
    "    model: PreTrainedModel,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    dataset: Dataset,\n",
    "    batch_size: int = 4,\n",
    "    idx_start_sample: int = 0,\n",
    "    max_samples: int = 1000,\n",
    "    save_to_pkl: bool = False,\n",
    "    output_path: str = \"outputs/all_batch_results.pkl\",\n",
    "    build_prompt_fn: Callable[[str, str], str] = None,\n",
    "    layers: List[int] = [-1],  \n",
    "    hidden_scores: List[str] = [\"average\", \"last\", \"max\", \"first_generated\", \"token_svd_score\", \"feat_var\"],\n",
    "    attn_scores: List[str] = [\"attn_eig_prod\"],\n",
    "    logit_scores: List[str] = [\"perplexity\", \"logit_entropy\", \"window_logit_entropy\"],\n",
    "    logit_config: dict = {\"top_k\": 50, \"window_size\": 1, \"stride\": 1},\n",
    "    start_offset : int = 0,\n",
    "    end_offset : int = 0,\n",
    ") -> Union[List[torch.Tensor], None]:\n",
    "    \"\"\"\n",
    "    Runs batched inference on a dataset using a decoder-only language model.\n",
    "    For each batch, it runs a forward pass on the prompt and extracts token-level hidden \n",
    "    activations, attention maps and logit scores from specified transformer layers.\n",
    "\n",
    "    The function supports multiple aggregation modes for the activations (`hidden_scores`), attention-based \n",
    "    scores (`attn_scores`), and logit-based scores (`logit_scores`). The `logit_config` argument provides \n",
    "    configuration parameters for logit-based score functions.\n",
    "    \n",
    "    Hidden states and attention maps are captured via forward hooks, \n",
    "    then aggregated based on token position and attention masks.\n",
    "    \n",
    "    These activations are saved as individual batch files in a specified pickle directory, \n",
    "    allowing efficient incremental storage and later aggregation.\n",
    "    Alternatively, the representations can be returned directly.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : PreTrainedModel\n",
    "        The causal language model to evaluate (e.g., LLaMA).\n",
    "    tokenizer : PreTrainedTokenizer\n",
    "        The corresponding tokenizer.\n",
    "    dataset : Dataset\n",
    "        The input dataset.\n",
    "    batch_size : int\n",
    "        Number of samples per batch.\n",
    "    idx_start_sample : int\n",
    "        Index of the first sample to process from the dataset.\n",
    "    max_samples : int\n",
    "        Total number of examples to process from the dataset, starting from idx_start_sample. \n",
    "    save_to_pkl : bool\n",
    "        If True, activations are appended to the pickle file at output_path.\n",
    "        If False, the function returns a list of activations.\n",
    "    output_path : str\n",
    "        Path to the directory where extracted answers will be saved as individual pickle batch files.\n",
    "    build_prompt_fn : Callable\n",
    "        Function to build a prompt from context and question.\n",
    "    layers : List[int]\n",
    "        List of indices of the transformer layers to extract activations from (default: [-1] for last layer).\n",
    "    hidden_scores : List[str], optional\n",
    "        List of aggregation modes to compute on token activations. Possible modes include:\n",
    "            \"average\", \"last\", \"max\", \"first_generated\", \"token_svd_score\", \"feat_var\".\n",
    "        These modes are passed to `extract_token_activations` for aggregation. Default includes the above.\n",
    "    attn_scores : List[str], optional\n",
    "        List of attention-based scores to compute. Supported: \"attn_eig_prod\".\n",
    "    logit_scores : List[str], optional\n",
    "        List of logit-based scores to compute. Supported:\n",
    "            \"perplexity\", \"logit_entropy\", \"window_logit_entropy\".\n",
    "    logit_config : dict, optional\n",
    "        Configuration dictionary for logit-based scoring functions, with keys such as:\n",
    "            - \"top_k\": int, number of top logits considered (default 50)\n",
    "            - \"window_size\": int, window size for windowed entropy (default 1)\n",
    "            - \"stride\": int, stride for windowed entropy (default 1)\n",
    "    start_offset : int\n",
    "        Offset from the first non-padding token (must be >= 0). \n",
    "    end_offset : int\n",
    "        Offset from the last non-padding token (must be <= 0, e.g., -3 to remove 3 tokens).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Union[List[dict], None]\n",
    "        If `save_to_pkl` is False, returns a list of dictionaries, one per batch, with each element\n",
    "         of the list having the following structure:\n",
    "            {\n",
    "                \"id\": List[str],  # IDs of batch samples\n",
    "                \"original_indices\": List[int],  # Original dataset indices\n",
    "                \"context\": List[str],\n",
    "                \"question\": List[str],\n",
    "                \"gt_answers\": List[str],        # Ground-truth reference answers\n",
    "                \"gen_answers\": List[str],       # Generated model answers\n",
    "                \"scores\": {\n",
    "                    \"layer_{layer_idx}\": {\n",
    "                        \"hidden\": { \n",
    "                            \"{mode}\": np.ndarray[(batch_size, hidden_size), float], \n",
    "                            ... # one entry per mode in hidden_scores\n",
    "                        },\n",
    "                        \"attention\": {\n",
    "                            \"{attn_score}\": np.ndarray[(batch_size,), float],  \n",
    "                            ...\n",
    "                        }\n",
    "                    },\n",
    "                    \"logits\": {\n",
    "                        \"perplexity\": np.ndarray[(batch_size,), float],\n",
    "                        \"logit_entropy\": np.ndarray[(batch_size,), float],\n",
    "                        \"window_logit_entropy\": np.ndarray[(batch_size,), float] \n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "\n",
    "        If `save_to_pkl` is True, saves each batch's dictionary incrementally to disk and returns None.\n",
    "    \"\"\"\n",
    "\n",
    "    # ==============================================================  \n",
    "    # [PATCH] Replace LlamaAttention.forward on target layers by\n",
    "    #  custom module to extract attention weights\n",
    "    # ==============================================================\n",
    "    for idx in layers:  \n",
    "        model.model.layers[idx].self_attn.forward = patched_LlamaAttention_forward.__get__(\n",
    "            model.model.layers[idx].self_attn,\n",
    "            model.model.layers[idx].self_attn.__class__\n",
    "    )\n",
    "        \n",
    "    # ==============================================================  \n",
    "    # [LOOP] Process batches of examples  \n",
    "    # ==============================================================\n",
    "    all_batch_results = []  \n",
    "\n",
    "    for i in tqdm(range(idx_start_sample, idx_start_sample + max_samples, batch_size)):\n",
    "      \n",
    "        # ----------------------------------------------------------\n",
    "        # [BATCH INPUT] Extract and tokenize prompts\n",
    "        # ----------------------------------------------------------\n",
    "        batch = extract_batch(dataset, i, batch_size)\n",
    "        prompts = [build_prompt_fn(s[\"context\"], s[\"question\"]) for s in batch]\n",
    "        inputs = tokenizer(prompts, padding=True, truncation=True, return_tensors=\"pt\").to(model.device)\n",
    "        prompt_ids = inputs[\"input_ids\"] # (batch_size, prompt_len)\n",
    "        prompt_attention_mask = inputs[\"attention_mask\"] \n",
    "\n",
    "        # ----------------------------------------------------------\n",
    "        # [HOOKS] Register hooks to capture hidden states and attentions\n",
    "        # The activations/attention retrieved by the hooks are have similar values \n",
    "        # as the ones from `output_hidden_states=True`/`output_attentions=True` in `model.generate()`\n",
    "        # ----------------------------------------------------------\n",
    "        # This hook collects the hidden states. For layer l: \n",
    "        # activations_lists[l] = [act_prompt], \n",
    "        # activations_lists[l][0] of shape: (batch_size, prompt_len, hidden_size) \n",
    "        activations_lists = [[] for _ in layers]  # one empty list per layer \n",
    "        handle_act, call_counter_act = register_generation_activation_hook(model, activations_lists, layers)\n",
    "\n",
    "        # This hook collects the activations at each decoding step. For layer l: \n",
    "        # attentions_lists[l] = [attn_prompt], \n",
    "        # activations_lists[l][0] of shape: (batch_size, n_heads, prompt_len, prompt_len)\n",
    "        attentions_lists = [[] for _ in layers]  # one empty list per layer\n",
    "        handle_attn, call_counter_attn = register_generation_attention_hook(model, attentions_lists, layers)\n",
    "        \n",
    "        # ----------------------------------------------------------\n",
    "        # [FOWARD PASS] Run model with hooks to capture intermediate states\n",
    "        # ----------------------------------------------------------\n",
    "        # Pass inputs through the model. When the target layer is reached,\n",
    "        # the hook executes and saves its output in captured_hidden.\n",
    "        if logit_scores is not None and len(logit_scores) > 0:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs, return_dict=True, return_logits=True)\n",
    "            prompt_logits = outputs.logits\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs, return_dict=True)\n",
    "        \n",
    "        # Remove hooks to avoid memory leaks or duplicate logging\n",
    "        for h in handle_act: h.remove()\n",
    "        for h in handle_attn: h.remove()\n",
    "        \n",
    "        # Verify that hooks worked properly\n",
    "        verify_call_counters(call_counter_act, name=\"activation hooks\")\n",
    "        verify_call_counters(call_counter_attn, name=\"attention hooks\")\n",
    "\n",
    "\n",
    "        # ----------------------------------------------------------\n",
    "        # [OFFSET] Modify prompt mask with offset, if specified\n",
    "        # ----------------------------------------------------------\n",
    "        if start_offset !=0 or end_offset !=0:\n",
    "            prompt_attention_mask, start_indices, end_indices = compute_offset_attention_mask(\n",
    "                attention_mask=prompt_attention_mask, \n",
    "                start_offset=start_offset, \n",
    "                end_offset=end_offset\n",
    "            ) # (batch_size, prompt_len), (batch_size,), (batch_size,)\n",
    "\n",
    "\n",
    "        # **********************************************************\n",
    "        # [LAYER LOOP] Extract activation and attention-based scores for each specified layer \n",
    "        # **********************************************************\n",
    "        save_layers_scores = {}\n",
    "\n",
    "        for l, layer_idx in enumerate(layers):\n",
    "\n",
    "            activations = activations_lists[l]\n",
    "            attentions = attentions_lists[l]\n",
    "\n",
    "            # Define prompt and generation hidden states \n",
    "            prompt_activations=activations[0]    \n",
    "            \n",
    "            # Define prompt and generation attention maps\n",
    "            prompt_attentions=attentions[0]        \n",
    "\n",
    "            # ------------------------------------------------------\n",
    "            # [HIDDEN SCORES] Extract token-level activations/hidden-states\n",
    "            # ------------------------------------------------------\n",
    "            if hidden_scores is not None and len(hidden_scores) > 0:\n",
    "                # Return only the token activations from the prompt\n",
    "                selected_token_vecs = extract_token_activations(\n",
    "                        selected_layer=prompt_activations, \n",
    "                        attention_mask=prompt_attention_mask, \n",
    "                        device=prompt_activations.device,\n",
    "                        modes=hidden_scores,\n",
    "                    ) # (batch_size, hidden_size)\n",
    " \n",
    "                # Save results to dict\n",
    "                hidden_results = {}\n",
    "                for mode in hidden_scores:\n",
    "                    if mode in selected_token_vecs:\n",
    "                        hidden_results[mode] = selected_token_vecs[mode].cpu().numpy()\n",
    "                save_layers_scores.setdefault(f\"layer_{layer_idx}\", {}).update({\"hidden\": hidden_results})\n",
    "\n",
    "            # ------------------------------------------------------\n",
    "            # [ATTENTION SCORES] Extract attention eigenvalue-based metric\n",
    "            # ------------------------------------------------------\n",
    "            if attn_scores is not None and 'attn_eig_prod' in attn_scores:\n",
    "                attn_eig_prod = compute_attn_eig_prod(\n",
    "                        prompt_attentions=prompt_attentions, \n",
    "                        generation_attentions=None,\n",
    "                        prompt_attention_mask=prompt_attention_mask, \n",
    "                        generation_attention_mask=None,\n",
    "                        mode='prompt',\n",
    "                )\n",
    "                # Save results to dict\n",
    "                save_layers_scores.setdefault(f\"layer_{layer_idx}\", {}).update({\"attention\": {\"attn_eig_prod\": attn_eig_prod}}) \n",
    "        \n",
    "        # **********************************************************\n",
    "        # [END LAYER LOOP] \n",
    "        # **********************************************************\n",
    "\n",
    "        save_logits_scores = {}\n",
    "        # ------------------------------------------------------\n",
    "        # [LOGIT SCORES] Compute metrics from model logits\n",
    "        # ------------------------------------------------------\n",
    "        if logit_scores is not None:\n",
    "            if 'perplexity' in logit_scores:\n",
    "                perplexity = compute_perplexity(\n",
    "                    prompt_logits=prompt_logits, \n",
    "                    gen_logits=None,\n",
    "                    prompt_ids=prompt_ids, \n",
    "                    gen_ids=None,\n",
    "                    prompt_attention_mask=prompt_attention_mask,\n",
    "                    gen_attention_mask=None,\n",
    "                    mode='prompt',\n",
    "                    min_k=None\n",
    "                )\n",
    "                # Save results to dict\n",
    "                save_logits_scores['perplexity'] = perplexity \n",
    "\n",
    "            if 'logit_entropy' in logit_scores:\n",
    "                if logit_config is None:\n",
    "                    raise ValueError(\"logit_entropy is required but logit_config is None\")\n",
    "                logit_entropy = compute_logit_entropy(\n",
    "                    prompt_logits=prompt_logits,\n",
    "                    gen_logits=None,\n",
    "                    prompt_attention_mask=prompt_attention_mask,\n",
    "                    gen_attention_mask=None,\n",
    "                    mode='prompt',\n",
    "                    top_k=logit_config['top_k'], \n",
    "                    window_size=None,\n",
    "                    stride=None\n",
    "                )\n",
    "                # Save results to dict\n",
    "                save_logits_scores['logit_entropy'] = logit_entropy \n",
    "        \n",
    "            if 'window_logit_entropy' in logit_scores:\n",
    "                if logit_config is None:\n",
    "                    raise ValueError(\"window_logit_entropy is required but logit_config is None\")\n",
    "                window_logit_entropy = compute_logit_entropy(\n",
    "                    prompt_logits=prompt_logits,\n",
    "                    gen_logits=None,\n",
    "                    prompt_attention_mask=prompt_attention_mask,\n",
    "                    gen_attention_mask=None,\n",
    "                    mode='prompt',\n",
    "                    top_k=logit_config['top_k'],\n",
    "                    window_size=logit_config['window_size'], \n",
    "                    stride=logit_config['stride'] \n",
    "                )\n",
    "                # Save results to dict\n",
    "                save_logits_scores['window_logit_entropy'] = window_logit_entropy \n",
    "\n",
    "\n",
    "        # ==========================================================\n",
    "        # [OUTPUT] Store extracted results (to memory or file)\n",
    "        # ==========================================================\n",
    "        batch_results = {\n",
    "            \"id\": [s['id'] for s in batch],\n",
    "            \"original_indices\": [s['original_index'] for s in batch],\n",
    "            \"context\": [s['context'] for s in batch],\n",
    "            \"question\": [s['question'] for s in batch],\n",
    "            \"gt_answers\": [s['answers'] for s in batch],\n",
    "            \"scores\": {**save_layers_scores, **({\"logits\": save_logits_scores} if save_logits_scores else {})}\n",
    "        }\n",
    "\n",
    "        from src.data_reader.pickle_io import save_batch_pickle\n",
    "\n",
    "        if save_to_pkl:\n",
    "            save_batch_pickle(batch_data=batch_results, output_dir=output_path, batch_idx=i)\n",
    "        else:\n",
    "            all_batch_results.append(batch_results)\n",
    "\n",
    "    if not save_to_pkl:\n",
    "        return all_batch_results\n",
    "\n",
    "\n",
    "\n",
    "def run_prompt_and_generation_score_extraction(\n",
    "    model: PreTrainedModel,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    dataset: Dataset,\n",
    "    batch_size: int = 4,\n",
    "    idx_start_sample: int = 0,\n",
    "    max_samples: int = 1000,\n",
    "    save_to_pkl: bool = False,\n",
    "    output_path: str = \"outputs/all_batch_results.pkl\",\n",
    "    build_prompt_fn: Callable[[str, str], str] = None,\n",
    "    layers: List[int] = [-1],  \n",
    "    activation_source: Literal[\"prompt\", \"generation\", \"promptGeneration\"] = \"generation\",\n",
    "    hidden_scores: List[str] = [\"average\", \"last\", \"max\", \"first_generated\", \"token_svd_score\", \"feat_var\"],\n",
    "    attn_scores: List[str] = [\"attn_eig_prod\"],\n",
    "    logit_scores: List[str] = [\"perplexity\", \"logit_entropy\", \"window_logit_entropy\"],\n",
    "    logit_config: dict = {\"top_k\": 50, \"window_size\": 1, \"stride\": 1},\n",
    "    start_offset : int = 0,\n",
    "    end_offset : int = 0,\n",
    ") -> Union[List[torch.Tensor], None]:\n",
    "    \"\"\"\n",
    "    Runs batched inference on a dataset using a decoder-only language model.\n",
    "    For each batch, it performs text generation and extracts token-level \n",
    "    hidden activations, attention maps and logit scores from specified transformer layers.\n",
    "    (both from the prompt and the generated text depending on `activation_source`) \n",
    "\n",
    "    The function supports multiple aggregation modes for the activations (`hidden_scores`), attention-based \n",
    "    scores (`attn_scores`), and logit-based scores (`logit_scores`). The `logit_config` argument provides \n",
    "    configuration parameters for logit-based score functions.\n",
    "    \n",
    "    Hidden states and attention maps are captured via forward hooks during generation, \n",
    "    then aggregated based on token position and attention masks.\n",
    "    \n",
    "    These activations are saved as individual batch files in a specified pickle directory, \n",
    "    allowing efficient incremental storage and later aggregation.\n",
    "    Alternatively, the representations can be returned directly.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : PreTrainedModel\n",
    "        The causal language model to evaluate (e.g., LLaMA).\n",
    "    tokenizer : PreTrainedTokenizer\n",
    "        The corresponding tokenizer.\n",
    "    dataset : Dataset\n",
    "        The input dataset.\n",
    "    batch_size : int\n",
    "        Number of samples per batch.\n",
    "    idx_start_sample : int\n",
    "        Index of the first sample to process from the dataset.\n",
    "    max_samples : int\n",
    "        Total number of examples to process from the dataset, starting from idx_start_sample. \n",
    "    save_to_pkl : bool\n",
    "        If True, activations are appended to the pickle file at output_path.\n",
    "        If False, the function returns a list of activations.\n",
    "    output_path : str\n",
    "        Path to the directory where extracted answers will be saved as individual pickle batch files.\n",
    "    build_prompt_fn : Callable\n",
    "        Function to build a prompt from context and question.\n",
    "    layers : List[int]\n",
    "        List of indices of the transformer layers to extract activations from (default: [-1] for last layer).\n",
    "    activation_source : {\"prompt\", \"generation\", \"promptGeneration\"}\n",
    "        Which part of the sequence to extract activations/attentions/logits from:\n",
    "        - \"prompt\": only from the prompt\n",
    "        - \"generation\": only from the generated answer\n",
    "        - \"promptGeneration\": prompt and generation answer both concatenated\n",
    "    hidden_scores : List[str], optional\n",
    "        List of aggregation modes to compute on token activations. Possible modes include:\n",
    "            \"average\", \"last\", \"max\", \"first_generated\", \"token_svd_score\", \"feat_var\".\n",
    "        These modes are passed to `extract_token_activations` for aggregation. Default includes the above.\n",
    "    attn_scores : List[str], optional\n",
    "        List of attention-based scores to compute. Supported: \"attn_eig_prod\".\n",
    "    logit_scores : List[str], optional\n",
    "        List of logit-based scores to compute. Supported:\n",
    "            \"perplexity\", \"logit_entropy\", \"window_logit_entropy\".\n",
    "    logit_config : dict, optional\n",
    "        Configuration dictionary for logit-based scoring functions, with keys such as:\n",
    "            - \"top_k\": int, number of top logits considered (default 50)\n",
    "            - \"window_size\": int, window size for windowed entropy (default 1)\n",
    "            - \"stride\": int, stride for windowed entropy (default 1)\n",
    "    start_offset : int\n",
    "        Offset from the first non-padding token (must be >= 0). \n",
    "    end_offset : int\n",
    "        Offset from the last non-padding token (must be <= 0, e.g., -3 to remove 3 tokens).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Union[List[dict], None]\n",
    "        If `save_to_pkl` is False, returns a list of dictionaries, one per batch, with each element\n",
    "         of the list having the following structure:\n",
    "            {\n",
    "                \"id\": List[str],  # IDs of batch samples\n",
    "                \"original_indices\": List[int],  # Original dataset indices\n",
    "                \"context\": List[str],\n",
    "                \"question\": List[str],\n",
    "                \"gt_answers\": List[str],        # Ground-truth reference answers\n",
    "                \"gen_answers\": List[str],       # Generated model answers\n",
    "                \"scores\": {\n",
    "                    \"layer_{layer_idx}\": {\n",
    "                        \"hidden\": { \n",
    "                            \"{mode}\": np.ndarray[(batch_size, hidden_size), float], \n",
    "                            ... # one entry per mode in hidden_scores\n",
    "                        },\n",
    "                        \"attention\": {\n",
    "                            \"{attn_score}\": np.ndarray[(batch_size,), float],  \n",
    "                            ...\n",
    "                        }\n",
    "                    },\n",
    "                    \"logits\": {\n",
    "                        \"perplexity\": np.ndarray[(batch_size,), float],\n",
    "                        \"logit_entropy\": np.ndarray[(batch_size,), float],\n",
    "                        \"window_logit_entropy\": np.ndarray[(batch_size,), float] \n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "\n",
    "        If `save_to_pkl` is True, saves each batch's dictionary incrementally to disk and returns None.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    When using model.generate() with output_hidden_states=True (what we are replicating here with the ,\n",
    "    activation hook) use_cache=True and max_new_tokens=30, there is always an offset between the length of the \n",
    "    generated sequence (outputs.sequences.shape[1][prompt_len:]) and the length of len(outputs.hidden_states) : \n",
    "    * outputs.sequences.shape[1] = prompt_len (17) + max_new_tokens (30) = 47\n",
    "    * len(outputs.hidden_states) = max_new_tokens (30)\n",
    "        With : \n",
    "        * outputs.hidden_states[0][layer_idx].shape = (batch_size, prompt_len, hidden_size)           --> includes the prompt ! \n",
    "        * outputs.hidden_states[i][layer_idx].shape = (batch_size, 1, hidden_size) with 1 <= i <= 29  --> stops at 29 ! \n",
    "    *Note* that in our code, outputs.hidden_states and activations are the same. \n",
    "        \n",
    "    Explanation from Hugging Face, April 2024 \n",
    "    (https://github.com/huggingface/transformers/issues/30036):\n",
    "    \"\"\"\n",
    "\n",
    "    # ==============================================================\n",
    "    # [VALIDATION] Ensure activation_source is correctly defined\n",
    "    # ==============================================================\n",
    "    if activation_source not in ('prompt', 'generation', 'promptGeneration'):\n",
    "        raise ValueError(\n",
    "                f\"Invalid value for `activation_source`: '{activation_source}'. \"\n",
    "                f\"Expected one of: ['prompt', 'generation', 'promptGeneration'].\"\n",
    "            )    \n",
    "        \n",
    "    # ==============================================================  \n",
    "    # [PATCH] Replace LlamaAttention.forward on target layers by\n",
    "    #  custom module to extract attention weights\n",
    "    # ==============================================================\n",
    "    for idx in layers:  \n",
    "        model.model.layers[idx].self_attn.forward = patched_LlamaAttention_forward.__get__(\n",
    "            model.model.layers[idx].self_attn,\n",
    "            model.model.layers[idx].self_attn.__class__\n",
    "    )\n",
    "        \n",
    "    # ==============================================================  \n",
    "    # [LOOP] Process batches of examples  \n",
    "    # ==============================================================\n",
    "    all_batch_results = []  \n",
    "\n",
    "    for i in tqdm(range(idx_start_sample, idx_start_sample + max_samples, batch_size)):\n",
    "      \n",
    "        # ----------------------------------------------------------\n",
    "        # [BATCH INPUT] Extract and tokenize prompts\n",
    "        # ----------------------------------------------------------\n",
    "        batch = extract_batch(dataset, i, batch_size)\n",
    "        prompts = [build_prompt_fn(s[\"context\"], s[\"question\"]) for s in batch]\n",
    "        inputs = tokenizer(prompts, padding=True, truncation=True, return_tensors=\"pt\").to(model.device)\n",
    "        prompt_ids = inputs[\"input_ids\"] # (batch_size, prompt_len)\n",
    "        prompt_len = prompt_ids.shape[1] # Assumes prompts are padded to same length\n",
    "\n",
    "        # ----------------------------------------------------------\n",
    "        # [HOOKS] Register hooks to capture hidden states and attentions\n",
    "        # ----------------------------------------------------------\n",
    "        # This hook collects the hidden states at each decoding step. For layer l: \n",
    "        # activations_lists[l] = [act_prompt, act_gen_step1, ..., act_gen_step49] of length 50, if max_new_tokens=50.\n",
    "        # activations_lists[l][k] of shape: (batch_size, seq_len, hidden_size) \n",
    "        activations_lists = [[] for _ in layers]  # one empty list per layer \n",
    "        handle_act, call_counter_act = register_generation_activation_hook(model, activations_lists, layers)\n",
    "\n",
    "        # This hook collects the activations at each decoding step. For layer l: \n",
    "        # attentions_lists[l] = [attn_prompt, attn_gen_step1, ..., attn_gen_step49], of length 50, if max_new_tokens=50.\n",
    "        # activations_lists[l][k] of shape: (batch_size, n_heads, tgt_seq_len, src_seq_len)\n",
    "        #   tgt_seq_len: length of the sequence the model is currently producing (query)\n",
    "        #   src_seq_len: length of the sequence the model is focusing on (key/value)\n",
    "        attentions_lists = [[] for _ in layers]  # one empty list per layer\n",
    "        handle_attn, call_counter_attn = register_generation_attention_hook(model, attentions_lists, layers)\n",
    "        \n",
    "        # ----------------------------------------------------------\n",
    "        # [GENERATION] Run model with hooks to capture intermediate states\n",
    "        # ----------------------------------------------------------\n",
    "        # When target layers are reached, hooks execute and saves their output in activations and attentions\n",
    "        outputs = generate(model, inputs, tokenizer, max_new_tokens=50, k_beams=1)\n",
    "        gen_ids = outputs.sequences[:, prompt_len:]\n",
    "    \n",
    "        # Remove hooks to avoid memory leaks or duplicate logging\n",
    "        for h in handle_act: h.remove()\n",
    "        for h in handle_attn: h.remove()\n",
    "        \n",
    "        # Verify that hooks worked properly\n",
    "        verify_call_counters(call_counter_act, name=\"activation hooks\")\n",
    "        verify_call_counters(call_counter_attn, name=\"attention hooks\")\n",
    "\n",
    "        # Retrieve text of generated answers\n",
    "        gen_answers = tokenizer.batch_decode(\n",
    "            outputs.sequences[:, prompt_len:], \n",
    "            skip_special_tokens=True\n",
    "        ) # (batch_size,)\n",
    "\n",
    "        # ----------------------------------------------------------\n",
    "        # [FOWARD PASS] Forward pass to the model to retrieve prompt logits \n",
    "        # ----------------------------------------------------------\n",
    "        if logit_scores is not None and len(logit_scores) > 0:\n",
    "            gen_logits = torch.stack(outputs.logits, dim=1) \n",
    "            with torch.no_grad():\n",
    "                prompt_logits = model(input_ids=inputs[\"input_ids\"]).logits\n",
    "        \n",
    "        # ----------------------------------------------------------\n",
    "        # [MASKING] Build attention masks for prompt and generation\n",
    "        # ----------------------------------------------------------\n",
    "        # This mask marks which generated tokens are valid (i.e., not padding).\n",
    "        # Positions are marked True up to and including the first eos_token_id\n",
    "        generation_attention_mask = build_generation_attention_mask(\n",
    "            gen_ids=gen_ids, \n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        ) # (batch_size, gen_len)\n",
    "\n",
    "        prompt_attention_mask = inputs[\"attention_mask\"] \n",
    "        # (batch_size, prompt_len)\n",
    "\n",
    "        # ----------------------------------------------------------\n",
    "        # [OFFSET] Modify prompt mask with offset, if specified\n",
    "        # ----------------------------------------------------------\n",
    "        if start_offset !=0 or end_offset !=0:\n",
    "            prompt_attention_mask, start_indices, end_indices = compute_offset_attention_mask(\n",
    "                attention_mask=prompt_attention_mask, \n",
    "                start_offset=start_offset, \n",
    "                end_offset=end_offset\n",
    "            ) # (batch_size, prompt_len), (batch_size,), (batch_size,)\n",
    "\n",
    "        # ----------------------------------------------------------\n",
    "        # [MASKING] Concatenate the prompt and generation attention mask\n",
    "        # ----------------------------------------------------------\n",
    "        prompt_and_gen_attention_mask = torch.cat(\n",
    "            [prompt_attention_mask,\n",
    "            generation_attention_mask],\n",
    "            dim=1\n",
    "        ) # (batch_size, prompt_len + gen_len)\n",
    "\n",
    "        # ----------------------------------------------------------\n",
    "        # [TRUNCATE] Remove final token from generated outputs to align with activations/attentions\n",
    "        # ----------------------------------------------------------\n",
    "        # When N tokens are generated, only the first N-1 tokens have corresponding hidden states.\n",
    "        # So activations[1:] covers only the first N-1 steps. Therefore, we exclude the last\n",
    "        # generated token from outputs.sequences to match activations[1:]. Same for attentions.\n",
    "        truncated_gen_ids = gen_ids[:,:-1] # (gen_len-1,)\n",
    "        truncated_generation_attention_mask = generation_attention_mask[:,:-1] # (batch_size, gen_len-1)\n",
    "        truncated_prompt_and_gen_attention_mask = prompt_and_gen_attention_mask[:,:-1] # (batch_size, prompt_len + gen_len-1)\n",
    "\n",
    "        # **********************************************************\n",
    "        # [LAYER LOOP] Extract activation and attention-based scores for each specified layer \n",
    "        # **********************************************************\n",
    "        save_layers_scores = {}\n",
    "\n",
    "        for l, layer_idx in enumerate(layers):\n",
    "\n",
    "            activations = activations_lists[l]\n",
    "            attentions = attentions_lists[l]\n",
    "\n",
    "            # Define prompt and generation hidden states \n",
    "            prompt_activations=activations[0]       # `[0]` to include only the prompt part \n",
    "            generation_activations=activations[1:]  # `[1:]` to exclude the prompt part \n",
    "            \n",
    "            # Define prompt and generation attention maps\n",
    "            prompt_attentions=attentions[0]         # `[0]` to include only the prompt part \n",
    "            generation_attentions=attentions[1:]    # `[1:]` to exclude the prompt part \n",
    "\n",
    "            # ------------------------------------------------------\n",
    "            # [ALIGNMENT] Stack and concatenate prompt + generation activations\n",
    "            # ------------------------------------------------------\n",
    "            # For each batch item, take the last generated hidden state at this step\n",
    "            stacked_generation_activations = torch.stack(\n",
    "                [h[:, -1, :] for h in generation_activations], dim=1\n",
    "            ) # (batch_size, gen_len, hidden_size)\n",
    "\n",
    "            # Concatenate the prompt and generation hidden states  \n",
    "            prompt_and_gen_activations = torch.cat(\n",
    "                [stacked_generation_activations, # (batch_size, gen_len, hidden_size)\n",
    "                prompt_activations],             # (batch_size, prompt_len, hidden_size)\n",
    "                dim=1\n",
    "            ) # (batch_size, prompt_len + gen_len, hidden_size)\n",
    "            \n",
    "            # ------------------------------------------------------\n",
    "            # [HIDDEN SCORES] Extract token-level activations/hidden-states\n",
    "            # ------------------------------------------------------\n",
    "            if hidden_scores is not None and len(hidden_scores) > 0:\n",
    "                if activation_source == \"generation\":\n",
    "                    # Return only the token activations from the generated answer \n",
    "                    selected_token_vecs = extract_token_activations(               \n",
    "                            selected_layer=stacked_generation_activations, \n",
    "                            attention_mask=truncated_generation_attention_mask, \n",
    "                            device=stacked_generation_activations.device,\n",
    "                            modes=hidden_scores,\n",
    "                        ) # (batch_size, hidden_size)\n",
    "                    \n",
    "                elif activation_source == \"prompt\":    \n",
    "                    # Return only the token activations from the prompt\n",
    "                    selected_token_vecs = extract_token_activations(\n",
    "                            selected_layer=prompt_activations, \n",
    "                            attention_mask=prompt_attention_mask, \n",
    "                            device=prompt_activations.device,\n",
    "                            modes=hidden_scores,\n",
    "                        ) # (batch_size, hidden_size)\n",
    "                    \n",
    "                else: # activation_source == \"promptGeneration\"\n",
    "                    # Return token activations from the concatenated prompt + generated answer \n",
    "                    selected_token_vecs = extract_token_activations(\n",
    "                            selected_layer=prompt_and_gen_activations, \n",
    "                            attention_mask=truncated_prompt_and_gen_attention_mask, \n",
    "                            device=prompt_and_gen_activations.device,\n",
    "                            skip_length=prompt_len,\n",
    "                            modes=hidden_scores,\n",
    "                            # skip_length: exclude prompt from computation if \n",
    "                            # mode=='first_generated' in `extract_token_activations_fn`\n",
    "                        ) # (batch_size, hidden_size)\n",
    "\n",
    "                # Save results to dict\n",
    "                hidden_results = {}\n",
    "                for mode in hidden_scores:\n",
    "                    if mode in selected_token_vecs:\n",
    "                        hidden_results[mode] = selected_token_vecs[mode].cpu().numpy()\n",
    "                save_layers_scores.setdefault(f\"layer_{layer_idx}\", {}).update({\"hidden\": hidden_results})\n",
    "\n",
    "            # ------------------------------------------------------\n",
    "            # [ATTENTION SCORES] Extract attention eigenvalue-based metric\n",
    "            # ------------------------------------------------------\n",
    "            if attn_scores is not None and 'attn_eig_prod' in attn_scores:\n",
    "                attn_eig_prod = compute_attn_eig_prod(\n",
    "                        prompt_attentions=prompt_attentions, \n",
    "                        generation_attentions=generation_attentions,\n",
    "                        prompt_attention_mask=prompt_attention_mask, \n",
    "                        generation_attention_mask=truncated_generation_attention_mask,\n",
    "                        mode=activation_source,\n",
    "                )\n",
    "                # Save results to dict\n",
    "                save_layers_scores.setdefault(f\"layer_{layer_idx}\", {}).update({\"attention\": {\"attn_eig_prod\": attn_eig_prod}}) \n",
    "        \n",
    "        # **********************************************************\n",
    "        # [END LAYER LOOP] \n",
    "        # **********************************************************\n",
    "\n",
    "        save_logits_scores = {}\n",
    "        # ------------------------------------------------------\n",
    "        # [LOGIT SCORES] Compute metrics from model logits\n",
    "        # ------------------------------------------------------\n",
    "        if logit_scores is not None:\n",
    "            if 'perplexity' in logit_scores:\n",
    "                perplexity = compute_perplexity(\n",
    "                    prompt_logits=prompt_logits, \n",
    "                    gen_logits=gen_logits,\n",
    "                    prompt_ids=prompt_ids, \n",
    "                    gen_ids=gen_ids,\n",
    "                    prompt_attention_mask=prompt_attention_mask,\n",
    "                    gen_attention_mask=generation_attention_mask,\n",
    "                    mode=activation_source,\n",
    "                    min_k=None\n",
    "                )\n",
    "                # Save results to dict\n",
    "                save_logits_scores['perplexity'] = perplexity \n",
    "\n",
    "            if 'logit_entropy' in logit_scores:\n",
    "                if logit_config is None:\n",
    "                    raise ValueError(\"logit_entropy is required but logit_config is None\")\n",
    "                logit_entropy = compute_logit_entropy(\n",
    "                    prompt_logits=prompt_logits,\n",
    "                    gen_logits=gen_logits,\n",
    "                    prompt_attention_mask=prompt_attention_mask,\n",
    "                    gen_attention_mask=generation_attention_mask,\n",
    "                    mode=activation_source,\n",
    "                    top_k=logit_config['top_k'], \n",
    "                    window_size=None,\n",
    "                    stride=None\n",
    "                )\n",
    "                # Save results to dict\n",
    "                save_logits_scores['logit_entropy'] = logit_entropy \n",
    "        \n",
    "            if 'window_logit_entropy' in logit_scores:\n",
    "                if logit_config is None:\n",
    "                    raise ValueError(\"window_logit_entropy is required but logit_config is None\")\n",
    "                window_logit_entropy = compute_logit_entropy(\n",
    "                    prompt_logits=prompt_logits,\n",
    "                    gen_logits=gen_logits,\n",
    "                    prompt_attention_mask=prompt_attention_mask,\n",
    "                    gen_attention_mask=generation_attention_mask,\n",
    "                    mode=activation_source,\n",
    "                    top_k=logit_config['top_k'],\n",
    "                    window_size=logit_config['window_size'], \n",
    "                    stride=logit_config['stride'] \n",
    "                )\n",
    "                # Save results to dict\n",
    "                save_logits_scores['window_logit_entropy'] = window_logit_entropy \n",
    "\n",
    "\n",
    "        # ==========================================================\n",
    "        # [OUTPUT] Store extracted results (to memory or file)\n",
    "        # ==========================================================\n",
    "        batch_results = {\n",
    "            \"id\": [s['id'] for s in batch],\n",
    "            \"original_indices\": [s['original_index'] for s in batch],\n",
    "            \"context\": [s['context'] for s in batch],\n",
    "            \"question\": [s['question'] for s in batch],\n",
    "            \"gt_answers\": [s['answers'] for s in batch],\n",
    "            \"gen_answers\": gen_answers,\n",
    "            \"scores\": {**save_layers_scores, **({\"logits\": save_logits_scores} if save_logits_scores else {})}\n",
    "        }\n",
    "\n",
    "        from src.data_reader.pickle_io import save_batch_pickle\n",
    "\n",
    "        if save_to_pkl:\n",
    "            save_batch_pickle(batch_data=batch_results, output_dir=output_path, batch_idx=i)\n",
    "        else:\n",
    "            all_batch_results.append(batch_results)\n",
    "\n",
    "    if not save_to_pkl:\n",
    "        return all_batch_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compare activations from different sources\n",
      "forward vs hook close? True, diff: 0.0\n",
      "forward vs gen close? True, diff: 0.0\n",
      "hook vs gen close? True, diff: 0.0\n",
      "Compare Logits from different sources\n",
      "forward vs logitLens forward close? True, diff: 0.0\n",
      "forward vs logitLens hook close? True, diff: 0.0\n",
      "forward vs logitLens gen close? True, diff: 0.0\n",
      "logitLens forward vs logitLens hook close? True, diff: 0.0\n",
      "logitLens forward vs logitLens gen close? True, diff: 0.0\n",
      "logitLens hook vs logitLens gen close? True, diff: 0.0\n",
      "Compare predicted tokens from computed logits\n",
      "\n",
      "[Exemple 0] Prompt : The cat sat on the\n",
      "→ forward   : Unterscheidung  is on the windows\n",
      "→ logitLens : Unterscheidung  is on the windows\n",
      "→ hook      : Unterscheidung  is on the windows\n",
      "→ gen       : Unterscheidung  is on the windows\n",
      "  Différences (fwd vs hook): 0\n",
      "  Différences (fwd vs gen) : 0\n",
      "  Différences (hook vs gen): 0\n",
      "=============================\n",
      "Flatten to compare logits only with non padded part\n",
      "Compare Logits from different sources\n",
      "forward vs logitLens forward close? True, diff: 0.0\n",
      "forward vs logitLens hook close? True, diff: 0.0\n",
      "forward vs logitLens gen close? True, diff: 0.0\n",
      "logitLens forward vs logitLens hook close? True, diff: 0.0\n",
      "logitLens forward vs logitLens gen close? True, diff: 0.0\n",
      "logitLens hook vs logitLens gen close? True, diff: 0.0\n",
      "\n",
      "===== Version aplatie (tous les tokens valides du batch) =====\n",
      "→ forward   : Unterscheidung  is on the windows\n",
      "→ logitLens : Unterscheidung  is on the windows\n",
      "→ hook      : Unterscheidung  is on the windows\n",
      "→ gen       : Unterscheidung  is on the windows\n",
      "  Différences (fwd vs hook): 0\n",
      "  Différences (fwd vs gen) : 0\n",
      "  Différences (hook vs gen): 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, PreTrainedModel, PreTrainedTokenizer, BatchEncoding\n",
    "from typing import Union, Dict\n",
    "\n",
    "def generate2(\n",
    "    model: PreTrainedModel,\n",
    "    inputs: BatchEncoding,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    max_new_tokens: int = 50,\n",
    "    k_beams: int = 1,\n",
    "    **generate_kwargs\n",
    ") -> Union[torch.Tensor, Dict[str, torch.Tensor]]:\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True    if k_beams == 1 else False,\n",
    "            temperature=0.6   if k_beams == 1 else None,\n",
    "            top_p=0.9         if k_beams == 1 else None,\n",
    "            top_k=50          if k_beams == 1 else None,\n",
    "            num_beams=k_beams,\n",
    "            use_cache=True, \n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.eos_token_id, # Ensures clean padding (right padding)\n",
    "            output_hidden_states=True,      # We rely on the hook to extract hidden states instead (more memory efficient)\n",
    "            output_attentions=False,         # We rely on the hook to extract attention map instead (more memory efficient)\n",
    "            output_logits=True,              # Logits not filtered/truncated by top-k/top-p sampling. Note: `output_scores=True` returns filtered logits. \n",
    "            return_dict_in_generate=True,    # Needed for access to beam_indices when num_beams > 1\n",
    "            early_stopping=False if k_beams == 1 else True, #Generation stops as soon as any sequence hits EOS, even if other candidates have not yet finished.\n",
    "            **generate_kwargs                # For future flexibility (e.g., output_attentions, output_scores)\n",
    "        )\n",
    "        return outputs \n",
    "\n",
    "def compare(l1, l2, name1, name2):\n",
    "    diff = torch.norm(l1-l2)\n",
    "    print(f\"{name1} vs {name2} close? {torch.allclose(l1, l2, rtol=1e-5, atol=1e-8)}, diff: {diff}\")\n",
    "\n",
    "# Charger modèle et tokenizer (remplacez par votre chemin ou nom HuggingFace)\n",
    "#model, tokenizer = load_llama(MODEL_NAME)\n",
    "\n",
    "'''\n",
    "Setting:\n",
    "-------\n",
    "Each time we retrieve the LAST layer hidden state of the PROMPT.\n",
    "\n",
    "In this code, we compute:\n",
    "- case 1) last layer prompt hidden state retrieved from output_hidden_states=True in the forward pass, denoted as hidden_forward\n",
    "- case 2) last layer prompt hidden state retrieved from output_hidden_states=True in model.generate(), denoted as hidden_gen\n",
    "- case 3) last layer prompt hidden state retrieved from a hook on model.generate(), denoted as hidden_hook\n",
    "\n",
    "We also compute the logits:\n",
    "- case a) logits obtained with the built-in argument outputs_forward.logits where outputs_forward is obtained in case 1)\n",
    "- case b) logits obtained using the logit lens on the activations hidden_forward obtained in case 1)\n",
    "- case c) logits obtained using the logit lens on the activations hidden_gen obtained in case 2)\n",
    "- case d) logits obtained using the logit lens on the activations hidden_hook obtained in case 3)\n",
    "\n",
    "Note: we cannot compare with logits obtained from the built-in argument outputs_gen.logits where outputs_gen\n",
    "is obtained in case 2), because it only contains logits for the generated prompt tokens and not the logits \n",
    "of the original prompt.\n",
    "\n",
    "Results:\n",
    "-------\n",
    ">>> When batch_size = 1:\n",
    "\n",
    "case 1) = case 3) : forward vs hook close? True\n",
    "case 1) = case 2) : forward vs gen close? True\n",
    "case 2) = case 3) : hook vs gen close? True\n",
    "\n",
    "case a) = case b) : forward vs logitLens forward close? True\n",
    "case a) = case d) : forward vs logitLens hook close? True\n",
    "case a) = case c) : forward vs logitLens gen close? True\n",
    "case b) = case d) : logitLens forward vs logitLens hook close? True\n",
    "case b) = case c) : logitLens forward vs logitLens gen close? True\n",
    "case d) = case c) : logitLens hook vs logitLens gen close? True\n",
    "\n",
    ">>> When batch_size > 1:\n",
    "\n",
    "case 1) != case 3) : forward vs hook close? False  => differences due to batching (I d'ont know why)\n",
    "case 1) != case 2) : forward vs gen close? False   =>  \n",
    "case 2)  = case 3) : hook vs gen close? True       => my hook works properly\n",
    "\n",
    "case a)  = case b) : forward vs logitLens forward close? True   => my logitLens function works properly\n",
    "case a) != case d) : forward vs logitLens hook close? False     => differences due to differences in activations \n",
    "case a) != case c) : forward vs logitLens gen close? False      \n",
    "case b) != case d) : logitLens forward vs logitLens hook close? False  \n",
    "case b) != case c) : logitLens forward vs logitLens gen close? False\n",
    "case d)  = case c) : logitLens hook vs logitLens gen close? True \n",
    "\n",
    "Even when I only compare the non-padded tokens, I see a difference. \n",
    "'''\n",
    "\n",
    "# --- Prompt et tokenisation ---\n",
    "prompt_text = [\"The cat sat on the\"]#, \"What color is the sky?\"]\n",
    "inputs = tokenizer(prompt_text, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "prompt_mask = inputs['attention_mask']\n",
    "\n",
    "# --- Forward classique ---\n",
    "with torch.no_grad():\n",
    "    outputs_forward = model(inputs['input_ids'], output_hidden_states=True, output_logits=True)\n",
    "\n",
    "hidden_forward = outputs_forward.hidden_states[-1] # [1, promp_len, hidden_size]\n",
    "\n",
    "# --- Hook pendant la génération ---\n",
    "activations = [[]]\n",
    "handles, _ = register_generation_activation_hook(model, activations, layers_idx_list=[-1])\n",
    "outputs_gen = generate2(model, inputs, tokenizer)\n",
    "for h in handles: h.remove()\n",
    "\n",
    "hidden_hook = activations[0][0].to(hidden_forward.device) # [1, promp_len, hidden_size]\n",
    "hidden_gen = outputs_gen.hidden_states[0][-1].to(hidden_forward.device) # [1, promp_len, hidden_size]\n",
    "\n",
    "# --- Compare activations ---\n",
    "print(\"Compare activations from different sources\")\n",
    "compare(hidden_forward, hidden_hook, \"forward\", \"hook\")\n",
    "compare(hidden_forward, hidden_gen, \"forward\", \"gen\")\n",
    "compare(hidden_hook, hidden_gen, \"hook\", \"gen\")\n",
    "\n",
    "# --- Logit lens ---\n",
    "logits_forward = outputs_forward.logits\n",
    "logits_lens_forward = apply_logit_lens(model, hidden_forward).to(hidden_forward.device)\n",
    "logits_lens_hook = apply_logit_lens(model, hidden_hook).to(hidden_forward.device)\n",
    "logits_lens_gen = apply_logit_lens(model, hidden_gen).to(hidden_forward.device)\n",
    "\n",
    "# --- Comparaison globale ---\n",
    "print(\"Compare Logits from different sources\")\n",
    "compare(logits_forward, logits_lens_forward, \"forward\", \"logitLens forward\")\n",
    "compare(logits_forward, logits_lens_hook, \"forward\", \"logitLens hook\")\n",
    "compare(logits_forward, logits_lens_gen, \"forward\", \"logitLens gen\")\n",
    "compare(logits_lens_forward, logits_lens_hook, \"logitLens forward\", \"logitLens hook\")\n",
    "compare(logits_lens_forward, logits_lens_gen, \"logitLens forward\", \"logitLens gen\")\n",
    "compare(logits_lens_hook, logits_lens_gen, \"logitLens hook\", \"logitLens gen\")\n",
    "\n",
    "\n",
    "# --- Comparaison par token pour chaque exemple (sans padding) ---\n",
    "print(\"Compare predicted tokens from computed logits\")\n",
    "batch_size = inputs['input_ids'].shape[0]\n",
    "valid_positions = prompt_mask.bool()\n",
    "\n",
    "for i in range(batch_size):\n",
    "    valids = valid_positions[i]\n",
    "    \n",
    "    logits_f = logits_forward[i][valids]\n",
    "    logits_l = logits_lens_forward[i][valids]\n",
    "    logits_h = logits_lens_hook[i][valids]\n",
    "    logits_g = logits_lens_gen[i][valids]\n",
    "\n",
    "    toks_f = torch.argmax(logits_f, dim=-1)\n",
    "    toks_l = torch.argmax(logits_l, dim=-1)\n",
    "    toks_h = torch.argmax(logits_h, dim=-1)\n",
    "    toks_g = torch.argmax(logits_g, dim=-1)\n",
    "\n",
    "    dec_f = tokenizer.decode(toks_f, skip_special_tokens=True)\n",
    "    dec_l = tokenizer.decode(toks_l, skip_special_tokens=True)\n",
    "    dec_h = tokenizer.decode(toks_h, skip_special_tokens=True)\n",
    "    dec_g = tokenizer.decode(toks_g, skip_special_tokens=True)\n",
    "\n",
    "    print(f\"\\n[Exemple {i}] Prompt :\", tokenizer.decode(inputs['input_ids'][i], skip_special_tokens=True))\n",
    "    print(\"→ forward   :\", dec_f)\n",
    "    print(\"→ logitLens :\", dec_l)\n",
    "    print(\"→ hook      :\", dec_h)\n",
    "    print(\"→ gen       :\", dec_g)\n",
    "\n",
    "    def diff_count(t1, t2): return (t1 != t2).sum().item()\n",
    "    print(\"  Différences (fwd vs hook):\", diff_count(toks_f, toks_h))\n",
    "    print(\"  Différences (fwd vs gen) :\", diff_count(toks_f, toks_g))\n",
    "    print(\"  Différences (hook vs gen):\", diff_count(toks_h, toks_g))\n",
    "\n",
    "\n",
    "# --- Comparaison aplatie ---\n",
    "print(\"=============================\")\n",
    "print(\"Flatten to compare logits only with non padded part\")\n",
    "valid_flat = prompt_mask.view(-1).bool()\n",
    "\n",
    "logits_f_flat = logits_forward.view(-1, logits_forward.shape[-1])[valid_flat]\n",
    "logits_l_flat = logits_lens_forward.view(-1, logits_forward.shape[-1])[valid_flat]\n",
    "logits_h_flat = logits_lens_hook.view(-1, logits_forward.shape[-1])[valid_flat]\n",
    "logits_g_flat = logits_lens_gen.view(-1, logits_forward.shape[-1])[valid_flat]\n",
    "\n",
    "\n",
    "# Compare logits \n",
    "print(\"Compare Logits from different sources\")\n",
    "compare(logits_f_flat, logits_l_flat, \"forward\", \"logitLens forward\")\n",
    "compare(logits_f_flat, logits_h_flat, \"forward\", \"logitLens hook\")\n",
    "compare(logits_f_flat, logits_g_flat, \"forward\", \"logitLens gen\")\n",
    "compare(logits_l_flat, logits_h_flat, \"logitLens forward\", \"logitLens hook\")\n",
    "compare(logits_l_flat, logits_g_flat, \"logitLens forward\", \"logitLens gen\")\n",
    "compare(logits_h_flat, logits_g_flat, \"logitLens hook\", \"logitLens gen\")\n",
    "\n",
    "\n",
    "toks_f_flat = torch.argmax(logits_f_flat, dim=-1)\n",
    "toks_l_flat = torch.argmax(logits_l_flat, dim=-1)\n",
    "toks_h_flat = torch.argmax(logits_h_flat, dim=-1)\n",
    "toks_g_flat = torch.argmax(logits_g_flat, dim=-1)\n",
    "\n",
    "print(\"\\n===== Version aplatie (tous les tokens valides du batch) =====\")\n",
    "print(\"→ forward   :\", tokenizer.decode(toks_f_flat, skip_special_tokens=True))\n",
    "print(\"→ logitLens :\", tokenizer.decode(toks_l_flat, skip_special_tokens=True))\n",
    "print(\"→ hook      :\", tokenizer.decode(toks_h_flat, skip_special_tokens=True))\n",
    "print(\"→ gen       :\", tokenizer.decode(toks_g_flat, skip_special_tokens=True))\n",
    "\n",
    "print(\"  Différences (fwd vs hook):\", (toks_f_flat != toks_h_flat).sum().item())\n",
    "print(\"  Différences (fwd vs gen) :\", (toks_f_flat != toks_g_flat).sum().item())\n",
    "print(\"  Différences (hook vs gen):\", (toks_h_flat != toks_g_flat).sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compare activations from different sources\n",
      "hook vs gen close? True, diff: 0.0\n",
      "Compare Logits from different sources\n",
      "gen vs logitLens hook close? False, diff: 0.21297432482242584\n",
      "gen vs logitLens gen close? False, diff: 0.21297432482242584\n",
      "logitLens hook vs logitLens gen close? True, diff: 0.0\n",
      "torch.Size([1, 50, 32000])\n",
      "torch.Size([1, 49, 32000])\n",
      "torch.Size([1, 49, 32000])\n",
      "torch.Size([1, 49, 32000])\n",
      "\n",
      "last_prompt_logit\n",
      " tensor([[-4.6992, -5.0938,  3.2773,  ..., -4.9961, -4.6602, -3.6621]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "logits_gen\n",
      " tensor([[[-4.6992, -5.0938,  3.2773,  ..., -4.9961, -4.6602, -3.6621],\n",
      "         [-2.0820, -2.6055,  6.6562,  ..., -1.3359, -4.8594, -1.1240],\n",
      "         [-2.6113, -2.0156,  7.5156,  ..., -0.7070, -2.8418, -1.8438],\n",
      "         ...,\n",
      "         [-4.6562, -6.9102,  2.6953,  ..., -4.2930, -2.2969, -5.2305],\n",
      "         [-2.8398, -3.9766,  6.6953,  ..., -1.3516, -5.1016, -2.3145],\n",
      "         [-4.4023, -5.5742,  7.2500,  ..., -4.0039, -5.2695, -5.5352]]],\n",
      "       device='cuda:0')\n",
      "\n",
      "logits_gen_truncated\n",
      " tensor([[[-2.0820, -2.6055,  6.6562,  ..., -1.3359, -4.8594, -1.1240],\n",
      "         [-2.6113, -2.0156,  7.5156,  ..., -0.7070, -2.8418, -1.8438],\n",
      "         [-5.8203, -6.7266,  4.3867,  ..., -2.6445, -5.0625, -2.3457],\n",
      "         ...,\n",
      "         [-4.6562, -6.9102,  2.6953,  ..., -4.2930, -2.2969, -5.2305],\n",
      "         [-2.8398, -3.9766,  6.6953,  ..., -1.3516, -5.1016, -2.3145],\n",
      "         [-4.4023, -5.5742,  7.2500,  ..., -4.0039, -5.2695, -5.5352]]],\n",
      "       device='cuda:0')\n",
      "\n",
      "logits_lens_hook\n",
      " tensor([[[-2.0820, -2.6055,  6.6562,  ..., -1.3350, -4.8594, -1.1240],\n",
      "         [-2.6113, -2.0156,  7.5156,  ..., -0.7070, -2.8418, -1.8438],\n",
      "         [-5.8203, -6.7266,  4.3867,  ..., -2.6445, -5.0625, -2.3457],\n",
      "         ...,\n",
      "         [-4.6562, -6.9102,  2.6953,  ..., -4.2930, -2.2969, -5.2305],\n",
      "         [-2.8398, -3.9766,  6.6953,  ..., -1.3516, -5.1016, -2.3145],\n",
      "         [-4.4023, -5.5742,  7.2500,  ..., -4.0039, -5.2695, -5.5352]]],\n",
      "       device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "\n",
      "logits_lens_gen\n",
      " tensor([[[-2.0820, -2.6055,  6.6562,  ..., -1.3350, -4.8594, -1.1240],\n",
      "         [-2.6113, -2.0156,  7.5156,  ..., -0.7070, -2.8418, -1.8438],\n",
      "         [-5.8203, -6.7266,  4.3867,  ..., -2.6445, -5.0625, -2.3457],\n",
      "         ...,\n",
      "         [-4.6562, -6.9102,  2.6953,  ..., -4.2930, -2.2969, -5.2305],\n",
      "         [-2.8398, -3.9766,  6.6953,  ..., -1.3516, -5.1016, -2.3145],\n",
      "         [-4.4023, -5.5742,  7.2500,  ..., -4.0039, -5.2695, -5.5352]]],\n",
      "       device='cuda:0', grad_fn=<ToCopyBackward0>)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Setting:\n",
    "-------\n",
    "Each time we retrieve the LAST layer hidden state of the GENERATED TOKENS.\n",
    "We can see that last prompt logit = first generated logit. \n",
    "'''\n",
    "\n",
    "hidden_hook = torch.stack(activations[0][1:], dim=0).squeeze(2).transpose(0, 1).to(hidden_forward.device) # [1, gen_len-1, hidden_size]\n",
    "\n",
    "last_layer_activations = [outputs_gen.hidden_states[i][-1] for i in range(1,len(outputs_gen.hidden_states))]\n",
    "last_layer_activations = [h.squeeze(1) if h.dim() == 3 and h.size(1) == 1 else h for h in last_layer_activations]\n",
    "hidden_gen = torch.stack(last_layer_activations, dim=1)# [1, gen_len-1, hidden_size]\n",
    "\n",
    "# --- Compare activations ---\n",
    "print(\"Compare activations from different sources\")\n",
    "compare(hidden_hook, hidden_gen, \"hook\", \"gen\")\n",
    "\n",
    "# --- Logit lens ---\n",
    "logits_gen = torch.stack(outputs_gen.logits, dim=0).transpose(0, 1)[:,:,:].float() \n",
    "logits_gen_truncated = logits_gen[:,1:,:] # remove first token\n",
    "logits_lens_hook = apply_logit_lens(model, hidden_hook).float().to(hidden_forward.device)\n",
    "logits_lens_gen = apply_logit_lens(model, hidden_gen).float().to(hidden_forward.device)\n",
    "\n",
    "# --- Comparaison globale ---\n",
    "print(\"Compare Logits from different sources\")\n",
    "compare(logits_gen_truncated, logits_lens_hook, \"gen\", \"logitLens hook\")\n",
    "compare(logits_gen_truncated, logits_lens_gen, \"gen\", \"logitLens gen\")\n",
    "compare(logits_lens_hook, logits_lens_gen, \"logitLens hook\", \"logitLens gen\")\n",
    "\n",
    "print(logits_gen.shape)\n",
    "print(logits_gen_truncated.shape)\n",
    "print(logits_lens_hook.shape)\n",
    "print(logits_lens_gen.shape)\n",
    "\n",
    "last_prompt_logit = logits_forward[:,-1,:] #we can see that last prompt logit = first generated logit\n",
    "print('\\nlast_prompt_logit\\n', last_prompt_logit)\n",
    "print('\\nlogits_gen\\n', logits_gen)\n",
    "print('\\nlogits_gen_truncated\\n', logits_gen_truncated)\n",
    "print('\\nlogits_lens_hook\\n', logits_lens_hook)\n",
    "print('\\nlogits_lens_gen\\n', logits_lens_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ood_env",
   "language": "python",
   "name": "ood_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
