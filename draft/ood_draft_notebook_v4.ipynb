{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OOD detection applied to Hallucination Detection\n",
    "\n",
    " The goal is to predict if an INPUT prompt  is going to produce an hallucination or not (using OOD detection methods). For now, we don’t look at the output generated by the model, we may consider this in a second time. Retrieve ID samples:  To do this, take a general (easy) QA dataset containing questions along with their true hallucination-free answers. Feed the questions to the model. Let the model generate responses and check if the a given generated response is the same as the real hallucination-free answer. All the correct generated responses will be considered ID. More precisely, the ID dataset will consist of the embeddings of the last token of the last layer of the input (or maybe middle layer) of the correct generated responses.  Test a new sample to see if this is going to be OOD=hallucination: Take another dataset containing questions susceptible to trigger hallucinations along with the true hallucination-free answers (or no answer if the model cannot know the answer by any way and all response that the model might produce will necessarily be hallucinated). Feed a question to the model and let it generate a response. Check by comparing to the hallucination-free answer is that generated response is hallucinated or not. At the same time, apply an OOD detection method on the input question (at the last token last layer) and see if there is a correspondence between a high OOD score and a generated hallucination. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#/home/lila.roig/.env/ood_env/bin/python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Embedding Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "# -----------------------------------\n",
    "import torch\n",
    "import sys\n",
    "import time \n",
    "import os \n",
    "import pickle\n",
    "from functools import partial\n",
    "# Add the path to the src directory\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define global variables\n",
    "# -----------------------------------\n",
    "SEED = 777 #44\n",
    "BATCH_SIZE = 16 #32\n",
    "MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "OUTPUT_DIR = \"../results/raw/TEST/\" \n",
    "PLOT_DIR   = \"../results/figures/\" \n",
    "LAYER = -1      # (integer) - Layer from witch retrieve the embeddings \n",
    "TOKENS = \"-1\"  # (string) - How to retrieve the embeddings \n",
    "K_BEAMS = 1 #3\n",
    "ACTIVATION_SOURCE = \"generation\" # can be 'generation', 'PromptGeneration'\n",
    " \n",
    "if TOKENS==\"0\":\n",
    "    EXTRACTION_MODE = \"first_generated\"\n",
    "elif TOKENS==\"-1\":\n",
    "    EXTRACTION_MODE = \"last\"\n",
    "elif TOKENS==\"Avg\":\n",
    "    EXTRACTION_MODE = \"average\"\n",
    "elif TOKENS==\"Max\":\n",
    "    EXTRACTION_MODE = \"max\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear memory to avoid \"CUDA out of memory\"\n",
    "# -----------------------------------\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\n",
      "Cuda version: 12.6\n",
      "Number of available de GPU : 2\n",
      "GPU 1 : NVIDIA GeForce RTX 4090\n",
      "GPU 2 : NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "# Visualize setup \n",
    "# -----------------------------------\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Cuda version: {torch.version.cuda}\")\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Number of available de GPU : {num_gpus}\")\n",
    "for i in range(num_gpus):\n",
    "    print(f\"GPU {i + 1} : {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed everything\n",
    "# -----------------------------------\n",
    "from src.utils.general import seed_all\n",
    "seed_all(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "209628938f73445aaba13066d2215996",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model\n",
    "# -----------------------------------\n",
    "from src.model_loader.llama_loader import load_llama\n",
    "\n",
    "model, tokenizer = load_llama(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sdpa\n",
      "sdpa\n"
     ]
    }
   ],
   "source": [
    "print(model.config._attn_implementation )\n",
    "#model.config._attn_implementation = 'eager'\n",
    "print(model.config._attn_implementation )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load ID dataset\n",
    "\n",
    "For the ID general dataset, we are going to use the SQUAD 1.1 dataset: \n",
    "\n",
    "***SQuAD 1.1:** Comprises over 100,000 question-answer pairs derived from more than 500 Wikipedia articles. Each question is paired with a specific segment of text (a span) from the corresponding article that serves as the answer.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Dataset Information =====\n",
      "Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers', 'original_index', 'is_impossible'],\n",
      "    num_rows: 1000\n",
      "})\n",
      "Mean ground-truth answer length: 2.25, Max length: 14\n",
      "Mean context + question length: 146.50, Max length: 342\n"
     ]
    }
   ],
   "source": [
    "# Load ID dataset\n",
    "# -----------------------------------\n",
    "from src.data_reader.squad_loader import load_id_fit_dataset \n",
    "# Total number of samples in squad v1.1: 87599, squad v2.0: 86821\n",
    "\n",
    "id_fit_dataset = load_id_fit_dataset()\n",
    "#id_fit_dataset = id_fit_dataset.shuffle(SEED) \n",
    "id_fit_dataset = id_fit_dataset.slice(idx_start=0, idx_end=1_000) # 10_000\n",
    "id_fit_dataset.print_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development of new solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify extrat_token_activations to add features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizer, PreTrainedModel\n",
    "import torch\n",
    "from torch.utils.hooks import RemovableHandle\n",
    "from typing import Tuple, Literal, List, Optional, Dict\n",
    "\n",
    "def extract_token_activations(\n",
    "    selected_layer: torch.Tensor,\n",
    "    attention_mask: torch.Tensor,\n",
    "    device: torch.device,\n",
    "    modes: List[Literal[\n",
    "        \"average\", \"last\", \"max\", \"first_generated\", \n",
    "        \"token_svd_score\", \"feat_cov_svd\", \n",
    "        \"token_cov_stats\", \"feat_cov_stats\", \"feat_cov_var\"\n",
    "    ]] = [\"average\"],\n",
    "    skip_length: Optional[int] = None,\n",
    "    alpha: int = 0.001,\n",
    ") -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"   \n",
    "    Aggregate token-level activations over a specified span for each sequence in a batch,\n",
    "    using various aggregation modes and attention mask.\n",
    "\n",
    "    This function takes as input:\n",
    "      - The layer activations (selected_layer) for each token in a batch of sequences,\n",
    "      - An attention mask (attention_mask) of the same shape, where 1 indicates tokens to include\n",
    "        in the aggregation and 0 marks tokens to ignore.\n",
    "\n",
    "    The attention mask may be the original model mask, or a custom mask generated using\n",
    "    `compute_offset_attention_mask` to dynamically select a sub-span of tokens.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    selected_layer : torch.Tensor\n",
    "        Tensor of shape (batch_size, seq_len, hidden_size) containing model activations for each token.\n",
    "    attention_mask : torch.Tensor\n",
    "        Attention mask of shape (batch_size, seq_len),  1 for real tokens, 0 for padding.\n",
    "    device : torch.device\n",
    "        Device for computation.\n",
    "\n",
    "    modes : List[str]\n",
    "        List of aggregation modes to compute. Computed using only valid tokens where attention_mask == 1.\n",
    "        Supported:\n",
    "        - \"average\": Mean activation vector across valid tokens. Shape: (batch_size, hidden_size)\n",
    "        - \"max\": Element-wise max activation across valid tokens. Shape: (batch_size, hidden_size)\n",
    "        - \"last\": Activation vector of last valid token in each sequence. Shape: (batch_size, hidden_size)\n",
    "        - \"first_generated\": Activation of the first generated valid token in each sequence. Shape: (batch_size, hidden_size)\n",
    "             If skip_length is provided, selects the token starting from that offset. \n",
    "        - \"token_svd_score\": Mean log singular value of the centered Gram matrix over tokens. Shape (batch_size,)\n",
    "            The Gram matrix is computed as Gram_token = Z·J·Z^T, where J is the centering matrix on features.\n",
    "            It quantifies the pairwise similarity between token representations after removing the mean value \n",
    "            of each feature across tokens. Note: This is not a classical covariance matrix.\n",
    "            The log singular values quantifies the effective dimensionality or diversity of the token \n",
    "            activations: higher values reflect more diverse (less redundant) token representations, lower values \n",
    "            indicate more redundancy or alignment.\n",
    "            NOTE: Implementation from \"LLM-Check: Investigating Detection of Hallucinations in Large Language Models\"\n",
    "            (Sriramanan et al. 2024)\n",
    "        - \"feat_var\": Diagonal of the centered feature covariance matrix (variances). Shape: (batch_size, hidden_size)\n",
    "\n",
    "    skip_length : Optional[int]\n",
    "        If provided, used to explicitly select the first generated token (useful for \"first_generated\" mode).\n",
    "    alpha : float\n",
    "        Regularization parameter added to the covariance matrix.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, torch.Tensor or np.ndarray]\n",
    "        Dictionary mapping each mode to its result:\n",
    "            - (batch_size, hidden_size) for \"average\", \"max\", \"last\", \"first_generated\", \"feat_var\"\n",
    "            - (batch_size,) for \"token_svd_score\"\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size, seq_len, hidden_size = selected_layer.shape\n",
    "    print(f\"batch_size: {batch_size}, hidden_size: {hidden_size}\")\n",
    "    aggregated_tokens = {}\n",
    "    \n",
    "    # Move to device \n",
    "    attention_mask = attention_mask.to(selected_layer.device)\n",
    "    print(\"selected_layer.device\", selected_layer.device)\n",
    "    print(\"attention_mask.devce\", attention_mask.device)\n",
    "\n",
    "    # =======================================\n",
    "    # Select the first token with optional offset `skip_length`\n",
    "    # =======================================\n",
    "    if \"first_generated\" in modes:\n",
    "        batch_indices = torch.arange(batch_size, device=device)\n",
    "        if skip_length is not None:\n",
    "            first_indices = torch.full((batch_size,), skip_length, device=device, dtype=torch.long)\n",
    "        else:\n",
    "            first_indices = (attention_mask == 1).float().argmax(dim=1)\n",
    "        first = selected_layer[batch_indices, first_indices] # Shape: (batch_size, hidden_size)\n",
    "        aggregated_tokens[\"first_generated\"] = first\n",
    "\n",
    "    # =======================================\n",
    "    # Select the last token \n",
    "    # =======================================\n",
    "    if \"last\" in modes:\n",
    "        last_indices = attention_mask.shape[1] - 1 - attention_mask.flip(dims=[1]).float().argmax(dim=1)\n",
    "        batch_indices = torch.arange(batch_size, device=device)\n",
    "        last = selected_layer[batch_indices, last_indices]  # Shape: (batch_size, hidden_size)\n",
    "        aggregated_tokens[\"last\"] = last\n",
    "\n",
    "    # =======================================\n",
    "    # Apply mask and compute aggregation \n",
    "    # =======================================\n",
    "    if \"average\" in modes or \"max\" in modes:\n",
    "        # Add one dimension for the broadcast on hidden_size\n",
    "        mask_float = attention_mask.float().unsqueeze(-1)  # (batch_size, num_valid_tokens, 1)\n",
    "        # Apply the mask to the activations: zero out tokens outside the target interval\n",
    "        masked = selected_layer * mask_float\n",
    "        #  Count the number of selected tokens for each sequence (avoid division by zero with clamp)\n",
    "        counts = mask_float.sum(dim=1).clamp(min=1e-6)\n",
    "        if \"average\" in modes:\n",
    "            # Compute the mean activation vector for each sequence over the selected interval\n",
    "            avg = masked.sum(dim=1) / counts # Shape: (batch_size, hidden_size)\n",
    "            aggregated_tokens[\"average\"] = avg\n",
    "        if \"max\" in modes:\n",
    "            # Replace padding with -inf to exclude from max calculation\n",
    "            masked_max = masked.masked_fill(mask_float.logical_not(), float('-inf'))\n",
    "            # Extract maximum values across sequence dimension\n",
    "            max_vals, _ = masked_max.max(dim=1) # Shape: (batch_size, hidden_size)\n",
    "            aggregated_tokens[\"max\"] = max_vals\n",
    "\n",
    "    # =======================================\n",
    "    # Covariance-based metrics\n",
    "    # =======================================\n",
    "    if any(m in modes for m in [\"token_svd_score\", \"feat_var\"]):\n",
    "        token_svd_score = [] \n",
    "        feat_var = []\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            # Select valid tokens \n",
    "            mask = attention_mask[i].bool()\n",
    "            Z = selected_layer[i][mask]  # (num_valid_tokens, hidden_size)\n",
    "            \n",
    "            if Z.shape[0] == 0:\n",
    "                feat_var.append(torch.full((hidden_size,), float('nan')))\n",
    "                token_svd_score.append(float('nan'))\n",
    "                continue\n",
    "            \n",
    "            if Z.dtype != torch.float32:\n",
    "                Z = Z.to(torch.float32)\n",
    "\n",
    "            if \"token_svd_score\" in modes:\n",
    "                # Compute Gram matrix on tokens : Gram_token = Z·J·Z^T\n",
    "                # ---------------------------------------\n",
    "                # Assumes Z is in full precision\n",
    "                # Center the features of Z (i.e., subtract the mean value of each feature across tokens)\n",
    "                J = torch.eye(hidden_size, device=Z.device, dtype=Z.dtype) - (1 / hidden_size) * torch.ones(hidden_size, hidden_size, device=Z.device, dtype=Z.dtype)\n",
    "                # The Gram matrix Gram_token reflects the inner products (similarities) between tokens\n",
    "                Gram_token = torch.matmul(torch.matmul(Z, J), Z.t()) # (num_valid_tokens, num_valid_tokens)\n",
    "                # Regularization for stabilization\n",
    "                Gram_token = Gram_token + alpha * torch.eye(Gram_token.shape[0], device=Z.device, dtype=Z.dtype)\n",
    "            \n",
    "                # Singular value decomposition (SVD) of the token Gram matrix\n",
    "                # ---------------------------------------\n",
    "                if Gram_token.dtype != torch.float32:\n",
    "                    Gram_token = Gram_token.to(torch.float32)\n",
    "                token_svdvals = torch.linalg.svdvals(Gram_token) # Singular Value Decomposition\n",
    "                token_eigscore = torch.log(token_svdvals).mean()  # mult by 2 missing from the paper? \n",
    "                token_svd_score.append(token_eigscore)\n",
    "\n",
    "            if \"feat_var\" in modes:\n",
    "                # Compute covariance matrix on features \n",
    "                # ---------------------------------------\n",
    "                Z_feat_centered = Z - Z.mean(dim=0, keepdim=True) # (num_valid_tokens, hidden_size)\n",
    "                Cov_feat = (Z_feat_centered.t() @ Z_feat_centered) / max(1, Z.shape[0] - 1) # (hidden_size, idden_size)\n",
    "                Cov_feat += alpha * torch.eye(Z.shape[1], device=Z.device, dtype=Z.dtype)\n",
    "                feat_var.append(Cov_feat.diag())\n",
    "            \n",
    "        # Return scores\n",
    "        # ---------------------------------------\n",
    "        if \"feat_var\" in modes:\n",
    "            aggregated_tokens[\"feat_var\"] = torch.stack(feat_var, dim=0) # (batch_size, hidden_size) \n",
    "        if \"token_svd_score\" in modes:\n",
    "            aggregated_tokens[\"token_svd_score\"] = torch.stack(token_svd_score) # (batch_size,) \n",
    "        \n",
    "        # Put everything on CPU\n",
    "        # ---------------------------------------\n",
    "        for key in aggregated_tokens:\n",
    "            aggregated_tokens[key] = aggregated_tokens[key].detach().cpu()\n",
    "\n",
    "    print(\"====================\")\n",
    "    print(aggregated_tokens)\n",
    "    return aggregated_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import List, Literal\n",
    "\n",
    "def compute_attn_eig_prod(\n",
    "    prompt_attentions: torch.Tensor,\n",
    "    generation_attentions: List[torch.Tensor],\n",
    "    prompt_attention_mask: torch.Tensor,\n",
    "    generation_attention_mask: torch.Tensor,\n",
    "    mode: Literal[\"prompt\", \"generation\", \"promptGeneration\"] = \"prompt\"\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute a mean log-diagonal attention score (eigenvalue-inspired) for a single layer's \n",
    "    attention map, using attention mask. \n",
    "    \n",
    "    NOTE: Implementation inspired by \n",
    "    \"LLM-Check: Investigating Detection of Hallucinations in Large Language Models\"\n",
    "    (Sriramanan et al. 2024)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    prompt_attentions: torch.Tensor\n",
    "        Tensor of shape (batch_size, n_heads, prompt_len, prompt_len)\n",
    "        Self-attention over the prompt tokens. \n",
    "    generation_attentions: list of torch.Tensor\n",
    "        List of tensors of shape (batch_size, n_heads, 1, prompt_len + t)\n",
    "        Self-attention for each generated token at generation step t (t >= 1).\n",
    "    prompt_attention_mask: torch.Tensor\n",
    "        Tensor of shape (batch_size, prompt_len), 1 where token valid, 0 for padding.\n",
    "    generation_attention_mask: torch.Tensor  \n",
    "        Tensor of shape (batch_size, gen_len), 1 where token valid, 0 for padding.\n",
    "    mode : str, optional\n",
    "        Specifies which part of the attention map to use for the score computation.\n",
    "        Must be one of the following:\n",
    "        - \"prompt\":\n",
    "            Only uses the prompt self-attention map (prompt_attentions). \n",
    "            It is a matrix of shape (batch_size, n_heads, prompt_len, prompt_len).\n",
    "            The diagonal (i.e., self-attention values per token) is extracted,\n",
    "            then the log is taken, followed by a mean over prompt tokens and sum over heads.\n",
    "        - \"generation\":\n",
    "            Only uses the generated self-attention maps (generation_attentions).\n",
    "            Each tensor in generation_attentions has shape (batch_size, n_heads, 1, prompt_len + t),\n",
    "            where t is the generation step. \n",
    "            Instead of concatenating these tensors to obtain the generation attention matrix, \n",
    "            for each step, we directly take the last value along the last axis (i.e., the self-attention\n",
    "            of the newly generated token). These values are stacked across time steps, then we take the log,\n",
    "            compute the mean over time, and sum over heads.\n",
    "        - \"promptGeneration\":\n",
    "            Combines the diagonals from both the prompt and generation attention maps as described above\n",
    "            for \"prompt\" and \"generation\" mode. The two diagonals are concatenated along the token/time axis, \n",
    "            then the log is taken, followed by a mean across all tokens and a sum over heads.\n",
    "            Note: we do **not** concatenate the full prompt and generation attention matrices,\n",
    "            since the diagonal of the combined matrix would only include values from the prompt attention\n",
    "            due to mismatched matrix shapes.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        A NumPy array of shape (batch_size,), where each value is the per-sample attention score.\n",
    "        The score is summed across heads and averaged across tokens (in log-space).\n",
    "    \"\"\"\n",
    "    if mode not in (\"prompt\", \"generation\", \"promptGeneration\"):\n",
    "        raise ValueError(f\"Invalid mode: {mode}. Must be 'prompt', 'generation' or 'promptGeneration'.\")\n",
    "\n",
    "    batch_size, n_heads = prompt_attentions.shape[:2]\n",
    "    gen_len = len(generation_attentions)    \n",
    "    diag_blocks = []\n",
    "\n",
    "    # Move to device\n",
    "    device = prompt_attentions.device\n",
    "    prompt_attention_mask = prompt_attention_mask.to(device)\n",
    "    generation_attention_mask = generation_attention_mask.to(device)\n",
    "\n",
    "    # ==============================\n",
    "    # Prompt mode or combined\n",
    "    # ==============================\n",
    "    if mode in (\"prompt\", \"promptGeneration\"):\n",
    "        # Extract diagonal of prompt attentions\n",
    "        prompt_diag = torch.diagonal(prompt_attentions, dim1=-2, dim2=-1) # (batch_size, n_heads, prompt_len)\n",
    "        # Expand prompt mask to (batch_size, n_heads, prompt_len)\n",
    "        p_mask_ext = prompt_attention_mask.unsqueeze(1).expand(-1, n_heads, -1)\n",
    "        diag_blocks.append(prompt_diag)\n",
    "\n",
    "    # ==============================\n",
    "    # Generation mode or combined\n",
    "    # ==============================\n",
    "    if mode in (\"generation\", \"promptGeneration\") and gen_len > 0:\n",
    "        # For each generation step, take the last value along last dim.\n",
    "        gen_diag_steps = [attn[..., -1].squeeze(-1) for attn in generation_attentions] # list of (batch_size, n_heads)\n",
    "        # Stack along time axis (= newly generated tokens)\n",
    "        gen_diag = torch.stack(gen_diag_steps, dim=-1) if gen_diag_steps else None # (batch_size, n_heads, gen_len)\n",
    "        # Expand generation mask to (batch_size, n_heads, gen_len)\n",
    "        g_mask_ext = generation_attention_mask.unsqueeze(1).expand(-1, n_heads, -1)\n",
    "        if gen_diag is not None:\n",
    "            diag_blocks.append(gen_diag)\n",
    "\n",
    "\n",
    "    # Concatenate diagonals along tokens/time dim\n",
    "    all_diags = torch.cat(diag_blocks, dim=-1) # (batch_size, n_heads, N) where N = prompt_len + n_generated (or a subset)\n",
    "    # Build full mask concatenated similarly: (batch_size, n_heads, N)\n",
    "    if mode == \"prompt\":\n",
    "        full_mask = p_mask_ext # (batch_size, n_heads, prompt_len)\n",
    "    elif mode == \"generation\":\n",
    "        full_mask = g_mask_ext  # (batch_size, n_heads, gen_len)\n",
    "    else:  # \"promptGeneration\"\n",
    "        full_mask = torch.cat([p_mask_ext, g_mask_ext], dim=-1)  # (batch_size, n_heads, total_len)\n",
    "\n",
    "    # ==============================\n",
    "    # Compute attention eigen product, ignoring padding tokens \n",
    "    # ==============================\n",
    "    # Clamp very small values to avoid log(0)\n",
    "    all_diags = all_diags.clamp(min=1e-6)\n",
    "    # Compute log\n",
    "    log_all_diags = torch.log(all_diags) # (batch_size, n_heads, N)\n",
    "    # Mask out padding tokens by zeroing out their logs\n",
    "    masked_log_all_diags = log_all_diags * full_mask # (batch_size, n_heads, N)\n",
    "    # Count valid tokens per batch and head to compute mean properly (avoid div by zero)\n",
    "    valid_token_counts = full_mask.sum(dim=-1).clamp(min=1) # (batch_size, n_heads)\n",
    "    # Mean log diag over valid tokens dimension (N)\n",
    "    mean_log_diag = masked_log_all_diags.sum(dim=-1) / valid_token_counts  # (batch_size, n_heads)\n",
    "    # Sum over heads to get final per-sample scores\n",
    "    scores = mean_log_diag.sum(dim=-1).cpu().numpy() # (batch_size,)\n",
    "\n",
    "    return scores  # shape: (batch_size,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "from typing import Callable, Optional, Tuple, Unpack\n",
    "from transformers.models.llama.modeling_llama import apply_rotary_pos_emb, eager_attention_forward, repeat_kv\n",
    "from transformers.cache_utils import Cache\n",
    "from transformers.modeling_flash_attention_utils import FlashAttentionKwargs\n",
    "from transformers.utils import logging\n",
    "logger = logging.get_logger(__name__)\n",
    "from transformers.modeling_utils import ALL_ATTENTION_FUNCTIONS\n",
    "\n",
    "\"\"\"\n",
    "**Problem**\n",
    "When working with Hugging Face's Llama models, I needed to extract attention weights for analysis purposes. \n",
    "However, the default \"eager\" attention implementation, which exposes attention weights, caused instability when running. \n",
    "Specifically, using the \"eager\" backend resulted in hidden states containing NaN values—sometimes even before the \n",
    "computation of Q, K, V. This led to CUDA crashes or completely invalid results.\n",
    "\n",
    "Switching to the \"sdpa\" attention backend resolved the numerical instability: with sdpa, there were no NaNs in \n",
    "hidden states, and the model ran stably, even in challenging configurations. However, sdpa computes attention \n",
    "weights inside a fused, highly optimized kernel and does not expose them—making it impossible to retrieve attention\n",
    " maps for analysis.\n",
    "\n",
    "Trying to \"fix\" eager by forcing float32 on hidden states did not resolve the core issue, since the rest of the model\n",
    "(and its layers) expects float16—leading to incompatibilities and further errors. Thus, neither backend offered both\n",
    " stability and transparency.\n",
    "\n",
    "**Solution**\n",
    "Implement a custom patch for the LlamaAttention forward method, but only on the specific layers where we wanted to \n",
    "access attention weights. The main computation of hidden states uses the stable backend (sdpa by default). \n",
    "This ensures the forward pass and generated sequences remain numerically stable.\n",
    "In parallel, the patch computes attention weights using the \"eager\" mechanism, but solely to extract and return them \n",
    "for inspection. These weights are not used in the model's forward pass and do not affect generation, so any instabilities \n",
    "or NaN handling for these analytical values do not impact the model's outputs.\n",
    "\n",
    "Thanks to this solution, we can now reliably run generation using Llama and access the true attention weights for \n",
    "chosen layers, benefiting both from the stability of \"sdpa\" and the interpretability of the \"eager\" backend, without \n",
    "compromising model correctness.\n",
    "\"\"\"\n",
    "\n",
    "def custom_eager_attention_forward(\n",
    "    module: nn.Module,\n",
    "    query: torch.Tensor,\n",
    "    key: torch.Tensor,\n",
    "    attention_mask: Optional[torch.Tensor],\n",
    "    scaling: float,\n",
    "    dropout: float = 0.0,\n",
    "    **kwargs,\n",
    "):\n",
    "    key_states = repeat_kv(key, module.num_key_value_groups)\n",
    "\n",
    "    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n",
    "    if attention_mask is not None:\n",
    "        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n",
    "        attn_weights = attn_weights + causal_mask\n",
    "\n",
    "    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n",
    "    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n",
    "\n",
    "    return attn_weights\n",
    "\n",
    "def patched_LlamaAttention_forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n",
    "        attention_mask: Optional[torch.Tensor],\n",
    "        past_key_value: Optional[Cache] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        **kwargs: Unpack[FlashAttentionKwargs],\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "        input_shape = hidden_states.shape[:-1]\n",
    "        hidden_shape = (*input_shape, -1, self.head_dim)\n",
    "\n",
    "        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "\n",
    "        cos, sin = position_embeddings\n",
    "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "\n",
    "        if past_key_value is not None:\n",
    "            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n",
    "            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n",
    "            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n",
    "\n",
    "        attention_interface: Callable = eager_attention_forward\n",
    "        if self.config._attn_implementation != \"eager\":\n",
    "            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n",
    "                logger.warning_once(\n",
    "                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n",
    "                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n",
    "                )\n",
    "            else:  \n",
    "                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n",
    "        \n",
    "        # ========================================================\n",
    "        # [1] Forward pass using main attention backend (sdpa / flash)\n",
    "        # This is the output used in the model's autoregressive loop.\n",
    "        # These implementations are optimized (for memory + stability).\n",
    "        # Does not compute attn_weights.\n",
    "        # ========================================================\n",
    "        attn_output, _ = attention_interface(\n",
    "                self,\n",
    "                query_states,\n",
    "                key_states,\n",
    "                value_states,\n",
    "                attention_mask,\n",
    "                dropout=0.0 if not self.training else self.attention_dropout,\n",
    "                scaling=self.scaling,\n",
    "                **kwargs,\n",
    "            )\n",
    "\n",
    "        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "\n",
    "        # ========================================================\n",
    "        # [2] Parallel computation of attention weights using eager attention\n",
    "        # This is to retrieve attention weights only (not used in forward loop)\n",
    "        # It is more numerically unstable (NaN possible with fp16)\n",
    "        # ========================================================\n",
    "        try:\n",
    "            attn_weights = custom_eager_attention_forward(\n",
    "                self,\n",
    "                query_states, \n",
    "                key_states, \n",
    "                attention_mask,\n",
    "                dropout=0.0 if not self.training else self.attention_dropout,\n",
    "                scaling=self.scaling, \n",
    "                **kwargs,\n",
    "            )\n",
    "\n",
    "            # Replace NaNs (if any) by 0.0 (no attention)\n",
    "            if torch.isnan(attn_weights).any():\n",
    "                print(\"[WARN] NaNs detected in attn_weights — replacing with 0.0 (no renormalization)\")\n",
    "                attn_weights = attn_weights.masked_fill(torch.isnan(attn_weights), 0.0)\n",
    "\n",
    "        except Exception as ex:\n",
    "            print(f\"[ERROR] Exception in custom_eager_attention_forward: {ex}\")\n",
    "            attn_weights = None\n",
    "\n",
    "        return attn_output, attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizer, PreTrainedModel\n",
    "import torch\n",
    "from torch.utils.hooks import RemovableHandle\n",
    "from typing import Tuple, Literal, List, Optional, Dict\n",
    "\n",
    "from typing import List, Tuple, Dict\n",
    "from torch.utils.hooks import RemovableHandle\n",
    "from transformers import PreTrainedModel\n",
    "\n",
    "\n",
    "def register_generation_attention_hook(\n",
    "    model: PreTrainedModel,\n",
    "    captured_attn_list: List[torch.Tensor],\n",
    "    layer_idx: int = -1\n",
    ") -> Tuple[RemovableHandle, dict]:\n",
    "    \"\"\"\n",
    "    Attaches a forward hook to a specific Llama layer's self-attention module \n",
    "    to capture attention maps (weights) during autoregressive text generation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : PreTrainedModel\n",
    "        The Hugging Face causal language model (e.g., Llama 2).\n",
    "    captured_attn_list : List[torch.Tensor]\n",
    "        List which will receive attention tensors after each decoding step.\n",
    "        Each tensor: (batch_size * num_beams, n_heads, tgt_seq_len, src_seq_len)\n",
    "    layer_idx : int\n",
    "        Index of the layer to hook. Defaults to -1 (last layer).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    RemovableHandle\n",
    "        Call handle.remove() after generation to cleanly remove the hook.\n",
    "    call_counter : dict\n",
    "        Counts how many times the hook fired.\n",
    "    \"\"\"\n",
    "    # Raise error if layer_idx not in correct range\n",
    "    num_layers = len(model.model.layers)\n",
    "    \n",
    "    call_counter = {\"count\": 0}\n",
    "    \n",
    "    # Pick correct index if -1 given\n",
    "    idx = layer_idx if layer_idx != -1 else num_layers - 1\n",
    "\n",
    "    def attn_hook_fn(module, input, output):\n",
    "        \"\"\"\n",
    "        Hook: captures the attention weights after the forward pass.\n",
    "        For Llama (transformers >=4.31/hf), output is a tuple:\n",
    "        (attn_output, attn_weights)\n",
    "        \"\"\"\n",
    "        call_counter[\"count\"] += 1\n",
    "        # HuggingFace Llama2 attention forward: output[1] are attn weights\n",
    "        attn_weights = output[1]  # (batch * num_beams, n_heads, tgt_seq_len, src_seq_len)\n",
    "        captured_attn_list.append(attn_weights) #.detach()\n",
    "\n",
    "    # The attention submodule for Llama: \n",
    "    attention_module = model.model.layers[idx].self_attn\n",
    "\n",
    "    # Register hook on the Attention block\n",
    "    # When Pytorch pass through this layer during forward pass, it also execute attn_hook_fn.\n",
    "    handle = attention_module.register_forward_hook(attn_hook_fn)\n",
    "    return handle, call_counter\n",
    "\n",
    "\n",
    "# classe avec: call counter et captured_attn_list comme variables globales \n",
    "# dans le init: mettre register_forward_hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def compute_perplexity(\n",
    "        prompt_logits: torch.Tensor, \n",
    "        gen_logits: torch.Tensor,\n",
    "        prompt_ids: torch.Tensor, \n",
    "        gen_ids: torch.Tensor,\n",
    "        prompt_attention_mask: torch.Tensor,\n",
    "        gen_attention_mask: torch.Tensor,\n",
    "        mode: Literal[\"prompt\", \"generation\", \"promptGeneration\"] = \"prompt\",\n",
    "        min_k: float = None\n",
    "    ) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the per-sample perplexity of language model outputs using logits \n",
    "    and corresponding input token IDs. Logits maked by 0 in the attention mask \n",
    "    are ignored in the computation of the perplexity. \n",
    "    If `min_k` is provided,\n",
    "    it filters the lowest probabilities to compute a restricted perplexity.\n",
    "\n",
    "    Perplexity is defined as:\n",
    "        Perplexity = exp(- mean(log P(token_i | context))) \n",
    "        where token_i is the next token actually predicted\n",
    "\n",
    "    NOTE: This implementation is inspired by:\n",
    "    \"LLM-Check: Investigating Detection of Hallucinations in Large Language Models\"\n",
    "    (Sriramanan et al., 2024)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    prompt_logits : torch.Tensor\n",
    "        Tensor of shape (batch_size, prompt_len, vocab_size) \n",
    "        These are the model's output logits obtained from a standard forward pass over the prompt sequence.\n",
    "    gen_logits : torch.Tensor\n",
    "        Tensor of shape (batch_size, gen_len, vocab_size).\n",
    "        These are the logits obtained during autoregressive decoding using `model.generate()`.\n",
    "    prompt_ids : torch.Tensor\n",
    "        Tensor of shape (batch_size, prompt_len), containing the input token IDs for the prompt.\n",
    "    gen_ids : torch.Tensor\n",
    "        Tensor of shape (batch_size, gen_len), containing the token IDs generated by the model.\n",
    "    prompt_attention_mask: torch.Tensor\n",
    "        Tensor of shape (batch_size, prompt_len), 1 where token valid, 0 for padding.\n",
    "    gen_attention_mask: torch.Tensor  \n",
    "        Tensor of shape (batch_size, gen_len), 1 where token valid, 0 for padding.\n",
    "    mode : str, optional\n",
    "        One of {\"prompt\", \"generation\", \"promptGeneration\"}:\n",
    "        - \"prompt\": compute perplexity only over the prompt.\n",
    "        - \"generation\": compute perplexity only over the generated tokens.\n",
    "        - \"promptGeneration\": compute perplexity over both prompt and generation.\n",
    "    min_k : float, optional\n",
    "        Optional value between 0 and 1. If specified, only the bottom-k lowest-probability\n",
    "        tokens are used for perplexity calculation.\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "        np.ndarray: Per-sample perplexity scores of shape (batch_size,)\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    1) This function computes a \"Pseudo Perplexity\".\n",
    "\n",
    "        The Standard Perplexity requires ground truth tokens:\n",
    "            PPL = exp(-1/N ∑_{t=1}^N log p(w_real_t | w_real_{<t})) where w_real_t are the true next tokens\n",
    "\n",
    "        In our case, we are in pure generation mode (equivalent to teacher forcing on the generated text)\n",
    "        and we don't have acces the real tokens. We thefore compute the Pseudo Perplexity:\n",
    "            PPL_gen = exp(-1/N ∑_{t=1}^N log p(w_gen_t | w_gen_{<t})) where w_gen_t are the generated next tokens\n",
    "        This measures the internal consistency of the model, and how well the model finds its own generation probable. \n",
    "    \n",
    "    2) About token shifting in autoregressive models:\n",
    "\n",
    "        In a standard autoregressive forward pass:\n",
    "            - At step *t*, the model predicts the token at position *t* based on the tokens up to *t-1*.\n",
    "            - Thus, the logit at position *t* predicts the token at position *t+1*.\n",
    "            - The first token has no preceding context and is not predicted.\n",
    "            - When computing log-probabilities, we must **shift the targets one position to the left** \n",
    "            to correctly align logits with target tokens.\n",
    "            \n",
    "            Example: Suppose we have a sequence of tokens (with their token IDs):\n",
    "            | Index | Token | ID  |\n",
    "            |-------|-------|-----| - The model produces logits at positions 0, 1, \n",
    "            | 0     | A     | 10  | and 2 to predict the tokens B, C, and D, respectively.\n",
    "            | 1     | B     | 29  |\n",
    "            | 2     | C     | 305 |  - The logits at position 0 are used to predict\n",
    "            | 3     | D     | 24  |  token B (ID 29).\n",
    "\n",
    "        During generation (e.g., using model.generate()):\n",
    "            - The logit at time step *t* predicts the token generated at position *t*.\n",
    "            - Each logit already corresponds to the prediction of the token at this step \n",
    "            - No shifting is needed in this case.\n",
    "\n",
    "        Summary of alignment:\n",
    "            - Prompt: logit at position *t* predicts token at position *t+1* -> shift targets left.\n",
    "            - Generation: logit at position *t* predicts token at position *t* -> no shift.\n",
    "\n",
    "        NOTE: help from issue https://github.com/huggingface/transformers/issues/29664\n",
    "    \"\"\"  \n",
    "    if min_k is not None:\n",
    "        if min_k < 0 or min_k > 1: raise ValueError(\"min_k must be between 0 and 1\")\n",
    "\n",
    "    if mode not in ('prompt','generation','promptGeneration'):\n",
    "        raise ValueError(\"mode must be in {'prompt','generation','promptGeneration'}\")\n",
    "\n",
    "    # ==============================\n",
    "    # Apply softmax over vocabulary dimension and take log to get log-probabilities\n",
    "    # ==============================\n",
    "    prompt_log_probs = torch.log_softmax(prompt_logits, dim=-1)  # (batch_size, prompt_len, vocab_size)\n",
    "    gen_log_probs = torch.log_softmax(gen_logits, dim=-1)        # (batch_size, gen_len, vocab_size)\n",
    "    \n",
    "    # ==============================\n",
    "    # Extraction of prompt log-probs\n",
    "    # ==============================\n",
    "    if mode in (\"prompt\", \"promptGeneration\"):\n",
    "        # In prompt: logit at position t predicts token at t+1 (requires shifting)\n",
    "        # Remove first token from target (no context to predict it)\n",
    "        prompt_target_tokens = prompt_ids[:, 1:] # (batch_size, prompt_len - 1)\n",
    "        prompt_attention_mask_shifted = prompt_attention_mask[:, 1:]  # (batch_size, prompt_len - 1)\n",
    "\n",
    "    \n",
    "        # Remove last logit position (since it predicts next token)\n",
    "        prompt_pred_log_probs = prompt_log_probs[:, :-1, :] # shape: (batch_size, prompt_len - 1, vocab_size)\n",
    "\n",
    "        # Retrieves, for each position and each batch, the log-probability corresponding to the next token \n",
    "        # (the one in target_tokens) from all the probas on the vocabulary.\n",
    "        prompt_token_log_probs = prompt_pred_log_probs.gather(\n",
    "            dim=2, index=prompt_target_tokens.unsqueeze(-1)\n",
    "            ).squeeze(-1) # shape: (batch_size, prompt_len - 1)\n",
    "      \n",
    "        # Mask paddings\n",
    "        prompt_token_log_probs = prompt_token_log_probs * prompt_attention_mask_shifted\n",
    "        \n",
    "    # ==============================\n",
    "    # Extraction of generation log-probs\n",
    "    # ==============================\n",
    "    if mode in (\"generation\", \"promptGeneration\"):\n",
    "        # In generation: logit at position t predicts token at position t (no shift needed)\n",
    "        gen_token_log_probs = gen_log_probs.gather(\n",
    "            dim=2, index=gen_ids.unsqueeze(-1)\n",
    "            ).squeeze(-1)  # shape: (batch_size, gen_len)\n",
    "        \n",
    "        # Mask paddings\n",
    "        gen_token_log_probs = gen_token_log_probs * gen_attention_mask\n",
    "\n",
    "    # ==============================\n",
    "    # Select log-probs according to mode\n",
    "    # ==============================\n",
    "    if mode == \"promptGeneration\":\n",
    "        # Last logit of prompt from the forward pass == first logit of generation from `model.generate()`. \n",
    "        # To compute perplexity over the full sequence:\n",
    "        # - Use prompt_token_log_probs (excluding final prompt token)\n",
    "        # - Use gen_token_log_probs from generation\n",
    "        # Concatenate both to form a complete sequence of predicted log-probs\n",
    "        token_log_probs = torch.cat(\n",
    "            [prompt_token_log_probs, gen_token_log_probs],  \n",
    "            dim=1) # (batch_size, prompt_len - 1 + gen_len)\n",
    "        total_mask = torch.cat(\n",
    "            [prompt_attention_mask_shifted, gen_attention_mask],\n",
    "            dim=1) # (batch_size, prompt_len - 1 + gen_len)\n",
    "    \n",
    "    elif mode == \"prompt\":\n",
    "        token_log_probs = prompt_token_log_probs    # (batch_size, prompt_len - 1)\n",
    "        total_mask = prompt_attention_mask_shifted  # (batch_size, prompt_len - 1)\n",
    "    \n",
    "    elif mode == \"generation\":\n",
    "        token_log_probs = gen_token_log_probs  # (batch_size, gen_len)\n",
    "        total_mask = gen_attention_mask        # (batch_size, gen_len)\n",
    "\n",
    "    # ==============================\n",
    "    # Compute Perplexity ignoring padded tokens\n",
    "    # ==============================\n",
    "    eps = 1e-12  # to avoid division by zero\n",
    "\n",
    "    # Optionally focus only on the k% hardest tokens (lowest log-probs)\n",
    "    if min_k is not None:\n",
    "        # Keep only the min_k fraction of tokens with the lowest log-probs \n",
    "        k = int(min_k * token_log_probs.size(1))  # number of tokens to keep per sample\n",
    "        \n",
    "        # Exclude padding tokens from topk selection\n",
    "        masked_log_probs = token_log_probs.clone()\n",
    "        masked_log_probs[total_mask == 0] = 1e6  \n",
    "\n",
    "        # Use topk with largest=False to get the k tokens with the lowest log-probabilities\n",
    "        topk_vals, _ = torch.topk(masked_log_probs, k=k, dim=1, largest=False)\n",
    "\n",
    "        # Compute perplexity using only the selected subset\n",
    "        ppls = torch.exp(-topk_vals.mean(dim=1))\n",
    "\n",
    "    else:\n",
    "        # Compute perplexity over all predicted tokens\n",
    "        sum_log_probs = (token_log_probs * total_mask).sum(dim=1)\n",
    "        count = total_mask.sum(dim=1).clamp(min=eps)\n",
    "        mean_log_prob = sum_log_probs / count\n",
    "        ppls = torch.exp(-mean_log_prob)\n",
    "\n",
    "    return ppls.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def compute_logit_entropy(\n",
    "    prompt_logits: torch.Tensor,\n",
    "    gen_logits: torch.Tensor,\n",
    "    prompt_attention_mask: torch.Tensor,\n",
    "    gen_attention_mask: torch.Tensor,\n",
    "    mode: str = \"prompt\",\n",
    "    top_k: int = None,\n",
    "    window_size: int = None,\n",
    "    stride: int = None\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the per-sample entropy of a language model's output distributions\n",
    "    using its logits and attention masks.\n",
    "    For each token position, the function computes the entropy of the softmax distribution\n",
    "    over the vocabulary. Entropy is averaged over the valid tokens (i.e., those marked\n",
    "    as 1 in the attention mask). If `top_k` is specified, the entropy is computed only\n",
    "    over the top-k logits (highest values) for each position.\n",
    "\n",
    "    Entropy is defined as:\n",
    "        Entropy = -Sum_i p_i * log(p_i)\n",
    "        where p_i = softmax(logits)_i\n",
    "    \n",
    "    There are two main usage patterns:\n",
    "      - Classic token-level average entropy (if window_size is None): computes the per-token entropy over the\n",
    "        sequence, averages over all valid tokens per sample (optionally using top_k).\n",
    "      - Windowed maximum mean entropy (if window_size is specified): slides a window of width `window_size`\n",
    "        and stride `stride` (default equals window_size: non-overlapping windows, else user-specified) across\n",
    "        the sequence of token entropies, and returns the maximum mean entropy observed in any window for each sample.\n",
    "\n",
    "    Padding tokens are always ignored (via the provided attention masks); only windows where all tokens are valid\n",
    "    are considered in the windowed mode.\n",
    "\n",
    "    NOTE: This implementation is inspired by:\n",
    "    \"LLM-Check: Investigating Detection of Hallucinations in Large Language Models\"\n",
    "    (Sriramanan et al., 2024)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    prompt_logits : torch.Tensor\n",
    "        Tensor of shape (batch_size, prompt_len, vocab_size).\n",
    "        These are the model's output logits obtained from a standard forward pass over the prompt sequence.\n",
    "    gen_logits : torch.Tensor\n",
    "        Tensor of shape (batch_size, gen_len, vocab_size).\n",
    "        These are the logits obtained during autoregressive decoding using `model.generate()`.\n",
    "    prompt_attention_mask : torch.Tensor\n",
    "        Tensor of shape (batch_size, prompt_len). Contains 1 where the token is valid and 0 for padding.\n",
    "    gen_attention_mask : torch.Tensor  \n",
    "        Tensor of shape (batch_size, gen_len). Contains 1 where the token is valid and 0 for padding.\n",
    "    mode : str, optional\n",
    "        Which tokens to use for entropy computation:\n",
    "        - \"prompt\": compute entropy only over the prompt logits/mask.\n",
    "        - \"generation\": compute entropy only over the generated logits/mask.\n",
    "        - \"promptGeneration\": compute entropy over both concatenated prompt and generated logits/mask.\n",
    "    top_k : int, optional\n",
    "        If specified, only the top_k logits (per token) are used to compute the entropy.\n",
    "        If None, use all logits.\n",
    "    window_size : int, optional\n",
    "        If not None, apply a sliding window of this size across the (valid) sequence of token entropies,\n",
    "        and return the maximum mean entropy over any complete window, for each sample.\n",
    "        If None, simply average the per-token entropies over all valid tokens.\n",
    "    stride : int, optional\n",
    "        Sliding window stride. Only used if window_size is specified.\n",
    "        - If None, defaults to window_size (non-overlapping windows).\n",
    "        - If set, must be a positive integer <= window_size.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Array of shape (batch_size,). For each batch sample, either the average logit entropy\n",
    "        over valid tokens (if window_size is None) or the maximum windowed mean entropy (if window_size is given).\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Padding tokens are always ignored, both in classic and windowed entropy.\n",
    "    - In windowed mode, only windows where all tokens in the window are valid are considered.\n",
    "    - Uses torch.special.entr for numerically stable entropy calculation.\n",
    "    \"\"\"\n",
    "    def entropy_from_logits(logits, attention_mask, top_k=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        logits: (batch_size, seq_len, vocab_size)\n",
    "        attention_mask: (batch_size, seq_len)\n",
    "        top_k: int > 0\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        entropy: (batch_size, seq_len)\n",
    "        attention_mask: (batch_size, seq_len)\n",
    "        \"\"\"\n",
    "\n",
    "        # Convert float16 -> float32 for better accuracy during computations\n",
    "        logits = logits.float()\n",
    "        attention_mask = attention_mask.float()\n",
    "\n",
    "        if top_k is not None:\n",
    "            topk_vals = torch.topk(logits, k=top_k, dim=-1).values  # (batch_size, seq_len, top_k)\n",
    "            probs = F.softmax(topk_vals, dim=-1) # (batch_size, seq_len, top_k)\n",
    "        else:\n",
    "            probs = F.softmax(logits, dim=-1) # (batch_size, seq_len, vocab_size)\n",
    "\n",
    "        # Use torch.special.entr, which automatically handles edge cases\n",
    "        # entropy(x) = -x * log(x) with entropy(0) = 0\n",
    "        entropy = torch.special.entr(probs).sum(dim=-1)  # (batch_size, seq_len)\n",
    "        return entropy, attention_mask # both are (batch_size, seq_len)\n",
    "\n",
    "    def average_entropy(entropy, mask):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        entropy: (batch_size, seq_len)\n",
    "        mask: (batch_size, seq_len)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        avg_entropy: (batch_size,)\n",
    "        \"\"\"\n",
    "        entropy_masked = entropy * mask                    # (batch_size, seq_len)\n",
    "        total_entropy = entropy_masked.sum(dim=-1)         # (batch_size,)\n",
    "        valid_count = mask.sum(dim=-1)                     # (batch_size,)\n",
    "        avg_entropy = total_entropy / (valid_count + 1e-9) # (batch_size,)\n",
    "        return avg_entropy\n",
    "\n",
    "    def max_sliding_window_entropy(entropy, mask, w, stride):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        entropy: (batch_size, seq_len)\n",
    "        mask: (batch_size, seq_len)\n",
    "        w: int > 0\n",
    "        stride: int > 0\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        max_avg_entropy: (batch_size,)\n",
    "        \"\"\"\n",
    "        # Add one dummy channel dimension since conv1d requires 3D tensors\n",
    "        entropy = entropy.unsqueeze(1)  # (batch_size, 1, seq_len)\n",
    "        mask = mask.unsqueeze(1)        # (batch_size, 1, seq_len)\n",
    "\n",
    "        kernel = torch.ones(1, 1, w, device=entropy.device) / w  # shape: (1,1,w)\n",
    "\n",
    "        # padding=0 to avoid artificial values and distorting the calculation\n",
    "        # Ignore windows for which there are not enough elements to form a complete window.\n",
    "        moving_avg = F.conv1d(entropy, kernel, stride=stride, padding=0)  # sliding mean entropy\n",
    "        \n",
    "        # All windows where there is at least one padding token will be ignored with valid_mask\n",
    "        valid_counts = F.conv1d(mask, kernel, stride=stride, padding=0)   # sliding mean mask (valid token ratio)\n",
    "        valid_mask = (valid_counts == 1.0)  # full valid windows only\n",
    "\n",
    "        moving_avg = moving_avg.masked_fill(~valid_mask, float('-inf')) # put -inf where valid_mask==0\n",
    "\n",
    "        max_avg_entropy, _ = moving_avg.max(dim=-1)  # (batch_size, 1)\n",
    "        \n",
    "        return max_avg_entropy.squeeze(1) # (batch_size,)\n",
    "\n",
    "    if top_k is not None:\n",
    "        top_k = int(top_k)\n",
    "        if top_k <= 0 or top_k > prompt_logits.shape[2]:\n",
    "            raise ValueError(\"top_k must be a positive integer less or equal to vocab size\")\n",
    "        \n",
    "    if window_size is not None:\n",
    "        if stride is None:\n",
    "            stride = window_size\n",
    "        else:\n",
    "            stride = int(stride)\n",
    "            if stride <= 0 or stride > window_size:\n",
    "                raise ValueError(\"stride must be a positive integer less or equal to window_size.\")\n",
    "    else:\n",
    "        stride = None\n",
    "\n",
    "    if mode == \"prompt\":\n",
    "        entropy, mask = entropy_from_logits(prompt_logits, prompt_attention_mask, top_k) # both are (batch_size, prompt_len)\n",
    "    elif mode == \"generation\":\n",
    "        entropy, mask = entropy_from_logits(gen_logits, gen_attention_mask, top_k)       # both are (batch_size, gen_len)\n",
    "    elif mode == \"promptGeneration\":\n",
    "        ent_p, mask_p = entropy_from_logits(prompt_logits, prompt_attention_mask, top_k) # both are (batch_size, prompt_len)\n",
    "        ent_g, mask_g = entropy_from_logits(gen_logits, gen_attention_mask, top_k)       # both are (batch_size, gen_len)\n",
    "        entropy = torch.cat([ent_p, ent_g], dim=1) # (batch_size, prompt_len + gen_len)\n",
    "        mask = torch.cat([mask_p, mask_g], dim=1)  # (batch_size, prompt_len + gen_len)\n",
    "    else:\n",
    "        raise ValueError(\"mode must be in {'prompt','generation','promptGeneration'}\")\n",
    "\n",
    "    if window_size is None:\n",
    "        result = average_entropy(entropy, mask)\n",
    "    \n",
    "    else:\n",
    "        if window_size <= 0:\n",
    "            raise ValueError(\"window_size must be a positive integer\")\n",
    "        if window_size > entropy.shape[1]:\n",
    "            raise ValueError(\"window_size greater than sequence length\")\n",
    "        if stride is None:\n",
    "            stride = window_size\n",
    "        else:\n",
    "            stride = int(stride)\n",
    "            if stride <= 0 or stride > window_size:\n",
    "                raise ValueError(\"stride must be a positive integer less or equal to window_size.\")\n",
    "        \n",
    "        window_size = int(window_size)\n",
    "        result = max_sliding_window_entropy(entropy, mask, window_size, stride)\n",
    "    return result.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizer, PreTrainedModel, BatchEncoding\n",
    "import torch\n",
    "from datasets import  Dataset\n",
    "from tqdm import tqdm\n",
    "from typing import List, Callable, Union, Literal, Dict, Tuple\n",
    "from torch.utils.hooks import RemovableHandle\n",
    "\n",
    "from src.inference.activation_utils import (\n",
    "    compute_offset_attention_mask,\n",
    ")\n",
    "from src.inference.inference_utils import (\n",
    "    build_prompt,\n",
    "    extract_batch, \n",
    "    align_generation_hidden_states,\n",
    "    align_prompt_hidden_states,\n",
    "    build_generation_attention_mask)\n",
    "\n",
    "def generate(\n",
    "    model: PreTrainedModel,\n",
    "    inputs: BatchEncoding,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    max_new_tokens: int = 50,\n",
    "    k_beams: int = 1,\n",
    "    **generate_kwargs\n",
    ") -> Union[torch.Tensor, Dict[str, torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Generate sequences from the model with optional beam search.\n",
    "    Supports advanced options via **generate_kwargs (e.g., output_attentions).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : PreTrainedModel\n",
    "        The language model to use for generation.\n",
    "    inputs : BatchEncoding\n",
    "        Tokenized input prompts.\n",
    "    tokenizer : PreTrainedTokenizer\n",
    "        Tokenizer providing eos and pad token IDs.\n",
    "    max_new_tokens : int, optional\n",
    "        Maximum number of new tokens to generate.\n",
    "    k_beams : int, optional\n",
    "        Number of beams to use. If 1, uses sampling. If >1, beam search is enabled.\n",
    "    **generate_kwargs : dict\n",
    "        Additional keyword arguments passed to `model.generate()`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Union[torch.Tensor, Dict[str, torch.Tensor]]\n",
    "        - If k_beams == 1:\n",
    "            Returns a tensor of generated token IDs: shape (batch_size, prompt_len + gen_len)\n",
    "        - If k_beams > 1:\n",
    "            Returns a dictionary with keys:\n",
    "                - \"sequences\": the generated token IDs\n",
    "                - \"beam_indices\": the beam path for each token\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True    if k_beams == 1 else False,\n",
    "            temperature=0.6   if k_beams == 1 else None,\n",
    "            top_p=0.9         if k_beams == 1 else None,\n",
    "            top_k=50          if k_beams == 1 else None,\n",
    "            num_beams=k_beams,\n",
    "            use_cache=True, \n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.eos_token_id, # Ensures clean padding (right padding)\n",
    "            output_hidden_states=False,      # We rely on the hook to extract hidden states instead (more memory efficient)\n",
    "            output_attentions=False,         # We rely on the hook to extract attention map instead (more memory efficient)\n",
    "            output_logits=True,              # Logits not filtered/truncated by top-k/top-p sampling. Note: `output_scores=True` returns filtered logits. \n",
    "            return_dict_in_generate=True,    # Needed for access to beam_indices when num_beams > 1\n",
    "            early_stopping=False if k_beams == 1 else True, #Generation stops as soon as any sequence hits EOS, even if other candidates have not yet finished.\n",
    "            **generate_kwargs                # For future flexibility (e.g., output_attentions, output_scores)\n",
    "        )\n",
    "        return outputs \n",
    "\n",
    "\n",
    "def register_generation_activation_hook(\n",
    "    model: PreTrainedModel,\n",
    "    captured_hidden_list: List[torch.Tensor],\n",
    "    layer_idx: int = -1\n",
    ") -> Tuple[RemovableHandle, dict]:\n",
    "    \"\"\"\n",
    "    Attaches a forward hook to a specific transformer layer to capture hidden states\n",
    "    during autoregressive text generation i.e., at each decoding step.\n",
    "    (more memory-efficient than using output_hidden_states=True).\n",
    "    Transformer layer = self-attention + FFN + normalization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : PreTrainedModel\n",
    "        The Hugging Face causal language model (e.g., GPT, LLaMA).\n",
    "    captured_hidden_list : List[torch.Tensor]\n",
    "        A list that will be filled with hidden states for each generation step. \n",
    "        Each tensor has shape (batch_size * num_beams, seq_len, hidden_size).\n",
    "    layer_idx : int\n",
    "        Index of the transformer block to hook. Defaults to -1 (the last layer).\n",
    "        Use a positive integer if you want to hook an intermediate layer instead.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    RemovableHandle : A handle object\n",
    "        Call `handle.remove()` after generation to remove the hook.\n",
    "    call_counter : int \n",
    "        Stores the number of times the hook is activated.\n",
    "    \"\"\"\n",
    "    # Raise error if layer_idx not in correct range\n",
    "    num_layers = len(model.model.layers)\n",
    "    if not (layer_idx == -1 or 0 <= layer_idx < num_layers):\n",
    "        raise ValueError(\n",
    "            f\"`layer_idx` must be -1 or in [0, {num_layers - 1}], but got {layer_idx}.\"\n",
    "        )\n",
    "    \n",
    "    call_counter = {\"count\": 0} # count how many times the hook is triggered\n",
    "\n",
    "    def hook_fn(module, input, output):\n",
    "        \"\"\"Function called automatically by PyTorch just after\n",
    "            the layer has produced its output during the forward pass.\"\"\"\n",
    "        \n",
    "        call_counter[\"count\"] += 1 \n",
    "        # output is a tuple (hidden_states,) → keep [0]\n",
    "        if layer_idx == -1:\n",
    "            # Capture the final normalized output \n",
    "            captured_hidden_list.append(model.model.norm(output[0]).detach())  # post RMSNorm!\n",
    "        else:\n",
    "            # Capture raw hidden states before layer normalization\n",
    "            captured_hidden_list.append(output[0].detach()) #### TEST #### \n",
    "    \n",
    "    # Register hook on the transformer block\n",
    "    # When Pytorch pass through this layer during forward pass, it also execute hook_fn.\n",
    "    handle = model.model.layers[layer_idx].register_forward_hook(hook_fn)\n",
    "    \n",
    "    return handle, call_counter\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "def run_prompt_and_generation_activation_extraction(\n",
    "    model: PreTrainedModel,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    dataset: Dataset,\n",
    "    batch_size: int = 4,\n",
    "    idx_start_sample: int = 0,\n",
    "    max_samples: int = 1000,\n",
    "    save_to_pkl: bool = False,\n",
    "    output_path: str = \"outputs/all_batch_results.pkl\",\n",
    "    build_prompt_fn: Callable[[str, str], str] = None,\n",
    "    register_generation_activation_hook_fn: Callable = None,\n",
    "    layer_idx: int = -1,  \n",
    "    activation_source: Literal[\"prompt\", \"generation\", \"promptGeneration\"] = \"generation\",\n",
    "    k_beams : int = 1,\n",
    "    start_offset : int = 0,\n",
    "    end_offset : int = 0,\n",
    ") -> Union[Dict[str, List[np.ndarray]], None]:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    batch_activations = {}  # Chosen token activation vectors\n",
    "\n",
    "    # ==============================\n",
    "    # Patch selected layer with custom LlamaAttention Forward function to retrieve attention weights\n",
    "    # ==============================\n",
    "    for idx in [layer_idx]:  \n",
    "        model.model.layers[idx].self_attn.forward = patched_LlamaAttention_forward.__get__(\n",
    "            model.model.layers[idx].self_attn,\n",
    "            model.model.layers[idx].self_attn.__class__\n",
    "    )\n",
    "\n",
    "\n",
    "    for i in tqdm(range(idx_start_sample, idx_start_sample + max_samples, batch_size)):\n",
    "        \n",
    "        # ==============================\n",
    "        # Prepare input batch\n",
    "        # ==============================\n",
    "        batch = extract_batch(dataset, i, batch_size)\n",
    "        \n",
    "        prompts = [build_prompt_fn(s[\"context\"], s[\"question\"]) for s in batch]\n",
    "        inputs = tokenizer(prompts, padding=True, truncation=True, return_tensors=\"pt\").to(model.device)\n",
    "        prompt_ids = inputs[\"input_ids\"]\n",
    "        prompt_len = prompt_ids.shape[1] # Assumes prompts are padded to same length\n",
    "\n",
    "        print(\"Layer L = \", layer_idx)\n",
    "        print(\"prompt_len:\", prompt_len)\n",
    "\n",
    "        print(\"===================================\")\n",
    "        '''for i in range(inputs['input_ids'].shape[0]):\n",
    "            print(\"--- Example\", i)\n",
    "            print(\"prompt:\", prompts[i])\n",
    "            print(\"prompt_ids:\", prompt_ids[i])\n",
    "            print(\"len(prompt_ids): \", len(prompt_ids[i]))\n",
    "            print(\"attention_mask:\", inputs[\"attention_mask\"][i])'''\n",
    "        print(\"===================================\")\n",
    "\n",
    "        # ==============================\n",
    "        # Register forward hook to capture layer output\n",
    "        # ==============================\n",
    "        # This hook collects the hidden states at each decoding step\n",
    "        # activations = [prompt] + [gen_step_1, gen_step_2, ..., gen_step_49], len(activations)=50, if max_new_tokens=50.\n",
    "        activations = [] # activations[k] of Shape: (batch_size * k_beams, seq_len, hidden_size)\n",
    "        handle_act, call_counter_act = register_generation_activation_hook_fn(model, activations, layer_idx=layer_idx)\n",
    "\n",
    "        #  [attn_prompt, attn_gen1, attn_gen2, ...]\n",
    "        # attentions = [prompt] + [gen_step_1, gen_step_2, ..., gen_step_49], len(attentions)=50, if max_new_tokens=50.\n",
    "        attentions = [] # attentions[k] of Shape: (batch_size * k_beams, n_heads, tgt_seq_len, src_seq_len)\n",
    "        # tgt_seq_len: length of the sequence the model is currently producing (query)\n",
    "        # src_seq_len: length of the sequence the model is focusing on (key/value)\n",
    "        handle_attn, call_counter_attn = register_generation_attention_hook(model, attentions, layer_idx=layer_idx) # PUT function in params\n",
    "\n",
    "        # ==============================\n",
    "        # Run model forward pass (hook captures activations)\n",
    "        # ==============================\n",
    "        # Generate text from prompts using beam search or sampling. \n",
    "        outputs = generate(model, inputs, tokenizer, max_new_tokens=50, k_beams=k_beams)\n",
    "        gen_ids = outputs.sequences[:, prompt_len:]\n",
    "        gen_logits = torch.stack(outputs.logits, dim=1) \n",
    "\n",
    "        # Remove the hook to avoid memory leaks or duplicate logging\n",
    "        handle_act.remove() \n",
    "        handle_attn.remove()\n",
    "\n",
    "        # Forward pass to the model to retrieve prompt logits \n",
    "        with torch.no_grad():\n",
    "            prompt_logits = model(input_ids=inputs[\"input_ids\"]).logits\n",
    "\n",
    "\n",
    "         # Retrieve text of generated answers\n",
    "        gen_answers = tokenizer.batch_decode(\n",
    "            sequences=gen_ids, \n",
    "            skip_special_tokens=True\n",
    "        ) # Shape: [batch_size,]\n",
    "        \n",
    "        # Define prompt and generation hidden states \n",
    "        prompt_activations=activations[0]\n",
    "        generation_activations=activations[1:]\n",
    "\n",
    "        # Define prompt and generation attention maps\n",
    "        prompt_attentions=attentions[0]\n",
    "        generation_attentions=attentions[1:]\n",
    "\n",
    "        # ===============================\n",
    "        # Truncate activations to match real generation steps (cf. Understanding Note #1)\n",
    "        # ===============================\n",
    "        # During generation, the model may run extra forward passes (especially with beam search)\n",
    "        # beyond the number of tokens in the final output. This results in activations being longer\n",
    "        # than needed — we need to truncate them accordingly.\n",
    "        if k_beams > 1:\n",
    "            # In beam search, we use beam_indices.shape[1] to determine the actual number of generation steps\n",
    "            gen_len = outputs.beam_indices.shape[1]\n",
    "        else:\n",
    "            # In greedy/top-k sampling, gen_len is simply the number of new tokens beyond the prompt\n",
    "            gen_len = outputs.sequences.shape[1] - prompt_len\n",
    "\n",
    "        print(\"gen_len:\", gen_len)\n",
    "\n",
    "        # Sometimes, activations may include extra \"ghost\" steps (e.g., due to internal padding/sync in beam search)\n",
    "        bool_truncate_activations = (len(generation_activations) >= gen_len) \n",
    "        print(\"bool_truncate_activations:\", bool_truncate_activations)\n",
    " \n",
    "        if bool_truncate_activations:\n",
    "            # Truncate extra steps to ensure alignment with generated tokens\n",
    "            generation_activations = generation_activations[:gen_len]\n",
    "\n",
    "        if bool_truncate_activations:\n",
    "            expected_gen_len = gen_len  # All generated tokens have hidden states\n",
    "        else: \n",
    "            expected_gen_len  = gen_len - 1 # Drop final token to match activations[1:]\n",
    "\n",
    "        # Truncate generated sequences and beam paths accordingly\n",
    "        truncated_gen_sequences = outputs.sequences[:, prompt_len : prompt_len + expected_gen_len] # RENAME: truncated_gen_ids \n",
    "\n",
    "        print(\"gen_ids.shape\", gen_ids.shape)\n",
    "        print(\"gen_ids\", gen_ids)\n",
    "        print(\"truncated_gen_sequences.shape\", truncated_gen_sequences.shape)\n",
    "        print(\"truncated_gen_sequences\", truncated_gen_sequences)\n",
    "\n",
    "        \n",
    "        if k_beams > 1:\n",
    "            truncated_beam_indices = outputs.beam_indices[:, :expected_gen_len] \n",
    "\n",
    "        # ===============================\n",
    "        # Build generation and prompt attention mask\n",
    "        # ===============================\n",
    "        # This mask marks which generated tokens are valid (i.e., not padding).\n",
    "        # Positions are marked True up to and including the first eos_token_id\n",
    "        generation_attention_mask = build_generation_attention_mask(             # RENAME: TRUNCATED GENERATION ATTN MASK\n",
    "            gen_ids=truncated_gen_sequences, \n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        ) # Shape (batch_size, gen_len)\n",
    "\n",
    "        # For logits, we have 1 logit per generated ouput -> no need to truncate gen_sequences. \n",
    "        # Therefore, we extend generation_attention_mask (which corresponds to truncated_gen_sequences)\n",
    "        # to the right with 1 element by repeating the last mask value for each batch. \n",
    "        # We obtain the generation_attention_mask corresponding to non truncated gen_sequences. \n",
    "        generation_attention_mask_ext = torch.cat([generation_attention_mask, generation_attention_mask[:, -1:]], dim=1)\n",
    "\n",
    "        # Prompt attention mask\n",
    "        prompt_attention_mask = inputs[\"attention_mask\"] \n",
    "        # Shape (batch_size, prompt_len)\n",
    "\n",
    "        print(\"==================\")\n",
    "        perplexity = compute_perplexity(\n",
    "                prompt_logits=prompt_logits, \n",
    "                gen_logits=gen_logits,\n",
    "                prompt_ids=prompt_ids, \n",
    "                gen_ids=gen_ids,\n",
    "                prompt_attention_mask=prompt_attention_mask,\n",
    "                gen_attention_mask=generation_attention_mask_ext,\n",
    "                mode=activation_source,\n",
    "                min_k=None\n",
    "        )\n",
    "        print(\"perplexity:\", perplexity)\n",
    "\n",
    "\n",
    "        print(\"=======================================\")\n",
    "        print(\"outputs.sequences.shape: \", outputs.sequences.shape)\n",
    "        print(\"call_counter_act:\", call_counter_act)\n",
    "        print(\"call_counter_attn:\", call_counter_attn)\n",
    "\n",
    "        print(\"prompt_ids.shape\", prompt_ids.shape)\n",
    "        print(\"gen_ids.shape\", gen_ids.shape)\n",
    "        print(\"gen_logits.shape:\", gen_logits.shape)\n",
    "        print(\"prompt_logits.shape: \", prompt_logits.shape)\n",
    "        print(\"gen_ids: \", gen_ids)\n",
    "        print(\"generation_attention_mask.shape\", generation_attention_mask.shape)\n",
    "        print(\"generation_attention_mask\", generation_attention_mask)\n",
    "        print(\"generation_attention_mask_ext.shape\", generation_attention_mask_ext.shape)\n",
    "        print(\"generation_attention_mask_ext\", generation_attention_mask_ext)\n",
    "        print(\"prompt_attention_mask.shape\", prompt_attention_mask.shape)\n",
    "        print(\"gen_answers :\", gen_answers)\n",
    "        \n",
    "        print(\"=======================================\")\n",
    "        print(\"Length of activations:\", len(activations))\n",
    "        for i in range(len(activations)):\n",
    "            print(f\"Shape  of activations[{i}]: {activations[i].shape}\") \n",
    "        print(\"============\")\n",
    "        print(\"Length of attentions:\", len(attentions))\n",
    "        for i in range(len(attentions)):\n",
    "            print(f\"Shape  of attentions[{i}]: {attentions[i].shape}\") \n",
    "        print(\"=======================================\")\n",
    "\n",
    "        print(\"Compute attn_eig_prod:\")\n",
    "        attn_eig_prod = compute_attn_eig_prod(\n",
    "                prompt_attentions=prompt_attentions, \n",
    "                generation_attentions=generation_attentions,\n",
    "                prompt_attention_mask=prompt_attention_mask, \n",
    "                generation_attention_mask=generation_attention_mask,\n",
    "                mode=activation_source,\n",
    "        )\n",
    "        print(\"attn_eig_prod:\", attn_eig_prod)\n",
    "        print(\"==================\")\n",
    "        print(\"Compute logit_entropy:\")\n",
    "        print(\"prompt_logits(batch0, 5 first tokens)\", prompt_logits[0,:5,:])\n",
    "        logit_entropy = compute_logit_entropy(\n",
    "            prompt_logits=prompt_logits,\n",
    "            gen_logits=gen_logits,\n",
    "            prompt_attention_mask=prompt_attention_mask,\n",
    "            gen_attention_mask=generation_attention_mask_ext,\n",
    "            mode=activation_source,\n",
    "            top_k = 50,\n",
    "            window_size=None,\n",
    "            stride=None\n",
    "        )\n",
    "        print(\"logit_entropy:\", logit_entropy)\n",
    "        print(\"==================\")\n",
    "        print(\"Compute window_logit_entropy:\")\n",
    "        window_logit_entropy = compute_logit_entropy(\n",
    "            prompt_logits=prompt_logits,\n",
    "            gen_logits=gen_logits,\n",
    "            prompt_attention_mask=prompt_attention_mask,\n",
    "            gen_attention_mask=generation_attention_mask_ext,\n",
    "            mode=activation_source,\n",
    "            top_k = 50,\n",
    "            window_size=1,\n",
    "            stride=1\n",
    "        )\n",
    "        print(\"window_logit_entropy:\", window_logit_entropy)\n",
    "        \n",
    "    if not save_to_pkl:\n",
    "        return batch_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer L =  -1\n",
      "prompt_len: 281\n",
      "===================================\n",
      "===================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen_len: 10\n",
      "bool_truncate_activations: False\n",
      "gen_ids.shape torch.Size([2, 10])\n",
      "gen_ids tensor([[29871,   443, 12011,   519,     2,     2,     2,     2,     2,     2],\n",
      "        [29871,  6106,   292,   322,  6025,  3277,  5100,  2187, 29889,     2]],\n",
      "       device='cuda:0')\n",
      "truncated_gen_sequences.shape torch.Size([2, 9])\n",
      "truncated_gen_sequences tensor([[29871,   443, 12011,   519,     2,     2,     2,     2,     2],\n",
      "        [29871,  6106,   292,   322,  6025,  3277,  5100,  2187, 29889]],\n",
      "       device='cuda:0')\n",
      "==================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "mode must be in {'prompt','generation','promptGeneration'}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[60]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m start_offset = \u001b[32m0\u001b[39m\n\u001b[32m      7\u001b[39m end_offset = \u001b[32m0\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m result = \u001b[43mrun_prompt_and_generation_activation_extraction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mid_fit_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43midx_start_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_to_pkl\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moutputs/all_batch_results.pkl\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbuild_prompt_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbuild_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mregister_generation_activation_hook_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mregister_generation_activation_hook\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mactivation_source\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlol\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mk_beams\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstart_offset\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mend_offset\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_offset\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[59]\u001b[39m\u001b[32m, line 299\u001b[39m, in \u001b[36mrun_prompt_and_generation_activation_extraction\u001b[39m\u001b[34m(model, tokenizer, dataset, batch_size, idx_start_sample, max_samples, save_to_pkl, output_path, build_prompt_fn, register_generation_activation_hook_fn, layer_idx, activation_source, k_beams, start_offset, end_offset)\u001b[39m\n\u001b[32m    296\u001b[39m \u001b[38;5;66;03m# Shape (batch_size, prompt_len)\u001b[39;00m\n\u001b[32m    298\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m==================\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m299\u001b[39m perplexity = \u001b[43mcompute_perplexity\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt_logits\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt_logits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    301\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgen_logits\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgen_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    303\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgen_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgen_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    304\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    305\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgen_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_attention_mask_ext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    306\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mactivation_source\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    307\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmin_k\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m    308\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    309\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mperplexity:\u001b[39m\u001b[33m\"\u001b[39m, perplexity)\n\u001b[32m    312\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=======================================\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 102\u001b[39m, in \u001b[36mcompute_perplexity\u001b[39m\u001b[34m(prompt_logits, gen_logits, prompt_ids, gen_ids, prompt_attention_mask, gen_attention_mask, mode, min_k)\u001b[39m\n\u001b[32m     99\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m min_k < \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m min_k > \u001b[32m1\u001b[39m: \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mmin_k must be between 0 and 1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m'\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mgeneration\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mpromptGeneration\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mmode must be in \u001b[39m\u001b[33m{\u001b[39m\u001b[33m'\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m'\u001b[39m\u001b[33m,\u001b[39m\u001b[33m'\u001b[39m\u001b[33mgeneration\u001b[39m\u001b[33m'\u001b[39m\u001b[33m,\u001b[39m\u001b[33m'\u001b[39m\u001b[33mpromptGeneration\u001b[39m\u001b[33m'\u001b[39m\u001b[33m}\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    104\u001b[39m \u001b[38;5;66;03m# ==============================\u001b[39;00m\n\u001b[32m    105\u001b[39m \u001b[38;5;66;03m# Apply softmax over vocabulary dimension and take log to get log-probabilities\u001b[39;00m\n\u001b[32m    106\u001b[39m \u001b[38;5;66;03m# ==============================\u001b[39;00m\n\u001b[32m    107\u001b[39m prompt_log_probs = torch.log_softmax(prompt_logits, dim=-\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# (batch_size, prompt_len, vocab_size)\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: mode must be in {'prompt','generation','promptGeneration'}"
     ]
    }
   ],
   "source": [
    "# Clear memory to avoid \"CUDA out of memory\"\n",
    "# -----------------------------------\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "\n",
    "start_offset = 0\n",
    "end_offset = 0\n",
    "result = run_prompt_and_generation_activation_extraction(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=id_fit_dataset,\n",
    "    batch_size=2,\n",
    "    idx_start_sample= 0,\n",
    "    max_samples=2,\n",
    "    save_to_pkl = False,\n",
    "    output_path = \"outputs/all_batch_results.pkl\",\n",
    "    build_prompt_fn=build_prompt,\n",
    "    register_generation_activation_hook_fn=register_generation_activation_hook,\n",
    "    layer_idx = -1,  \n",
    "    activation_source = \"lol\",\n",
    "    k_beams=1,\n",
    "    start_offset = start_offset,\n",
    "    end_offset = end_offset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of activations: 10\n",
      "Shape  of activations[0]: torch.Size([2, 281, 4096])\n",
      "Shape  of activations[1]: torch.Size([2, 1, 4096])\n",
      "Shape  of activations[2]: torch.Size([2, 1, 4096])\n",
      "Shape  of activations[3]: torch.Size([2, 1, 4096])\n",
      "Shape  of activations[4]: torch.Size([2, 1, 4096])\n",
      "Shape  of activations[5]: torch.Size([2, 1, 4096])\n",
      "Shape  of activations[6]: torch.Size([2, 1, 4096])\n",
      "Shape  of activations[7]: torch.Size([2, 1, 4096])\n",
      "Shape  of activations[8]: torch.Size([2, 1, 4096])\n",
      "Shape  of activations[9]: torch.Size([2, 1, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from src.inference.inference_utils import (\n",
    "    run_prompt_and_generation_activation_extraction, \n",
    "    build_prompt\n",
    ")\n",
    "from src.inference.activation_utils import (\n",
    "   register_generation_activation_hook ,\n",
    "   extract_token_activations\n",
    ")\n",
    "\n",
    "\n",
    "result = run_prompt_and_generation_activation_extraction(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=id_fit_dataset,\n",
    "    batch_size=2,\n",
    "    idx_start_sample= 0,\n",
    "    max_samples= 2,\n",
    "    save_to_pkl = False,\n",
    "    output_path = \"outputs/all_batch_results.pkl\",\n",
    "    build_prompt_fn=build_prompt,\n",
    "    register_generation_activation_hook_fn=register_generation_activation_hook,\n",
    "    layer_idx = -1,  \n",
    "    extract_token_activations_fn = extract_token_activations,\n",
    "    activation_source = \"promptGeneration\",\n",
    "    k_beams=1,\n",
    "    start_offset = 0,\n",
    "    end_offset = 0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonctions du papier LLM-check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hidden_acts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mLength of outputs.hidden_states:  7 \u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m( = number of generated tokens - 1 element to exclude last generated token + 1 element for prompt)\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m \u001b[33;03m**** Un score par couche, par sample (et pas par head, car les hidden states ne sont pas splittés par head). ****\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# dans compute_scores()\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer_num \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[43mhidden_acts\u001b[49m[\u001b[32m0\u001b[39m])):\n\u001b[32m     21\u001b[39m     mt_score.append(get_svd_eval(hidden_acts, layer_num, tok_lens, use_toklens)[\u001b[32m0\u001b[39m])\n\u001b[32m     22\u001b[39m     indiv_scores[mt][\u001b[33m\"\u001b[39m\u001b[33mHly\u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(layer_num)].append(mt_score[-\u001b[32m1\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'hidden_acts' is not defined"
     ]
    }
   ],
   "source": [
    "# Fonctions exactement comme dans le papier LLM check \n",
    "\n",
    "\"\"\"\n",
    "Length of outputs.hidden_states:  7 \n",
    "( = number of generated tokens - 1 element to exclude last generated token + 1 element for prompt)\n",
    "L : layer, batch_size = 2, hidden_size = 4096, prompt_len = 329\n",
    "Shape of outputs.hidden_states[0][L]: torch.Size([2, 329, 4096])\n",
    "Shape of activations[1][L]: torch.Size([2, 1, 4096])\n",
    "Shape of activations[-1][L]: torch.Size([2, 1, 4096])\n",
    "\n",
    "hidden_act = [x[0].to(torch.float32).detach().cpu() for x in outputs.hidden_states]\n",
    "\n",
    "Pour chaque couche (layer_num), tu calcules un score SVD sur les activations \n",
    "cachées de cette couche (pour chaque sample).\n",
    "get_svd_eval retourne un score par sample pour cette couche.\n",
    "Ces scores sont stockés dans indiv_scores[mt][\"HlyX\"] (X = numéro de la couche).\n",
    "Donc :\n",
    "\n",
    "**** Un score par couche, par sample (et pas par head, car les hidden states ne sont pas splittés par head). ****\n",
    "\"\"\"\n",
    "# dans compute_scores()\n",
    "for layer_num in range(1, len(hidden_acts[0])):\n",
    "    mt_score.append(get_svd_eval(hidden_acts, layer_num, tok_lens, use_toklens)[0])\n",
    "    indiv_scores[mt][\"Hly\" + str(layer_num)].append(mt_score[-1])\n",
    "\n",
    "\n",
    "def get_svd_eval(hidden_acts, layer_num=15, tok_lens=[], use_toklens=True):\n",
    "    \"\"\"Evaluate hidden states at a given layer using SVD-based scoring.\n",
    "\n",
    "    For each sample, this function extracts the hidden states at a specified layer,\n",
    "    optionally slices them according to `tok_lens`, and computes the SVD-based score.\n",
    "\n",
    "    Args:\n",
    "        hidden_acts (list): A list of tuples, each containing hidden states for all layers\n",
    "            for a single sample.\n",
    "        layer_num (int, optional): The layer index to evaluate. Defaults to 15.\n",
    "        tok_lens (list, optional): A list of (start, end) indices for each sample to slice\n",
    "            the hidden states. Defaults to [].\n",
    "        use_toklens (bool, optional): Whether to slice the hidden states using `tok_lens`.\n",
    "            Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        np.array: An array of SVD-based scores for each sample.\n",
    "    \"\"\"\n",
    "    svd_scores = []\n",
    "    for i in range(len(hidden_acts)): # loop sur les samples \n",
    "        Z = hidden_acts[i][layer_num] # activations pour le sample i à la couche layer_num, shape (seq_len, hidden_size)\n",
    "\n",
    "        if use_toklens and tok_lens[i]:\n",
    "            i1, i2 = tok_lens[i][0], tok_lens[i][1]\n",
    "            Z = Z[i1:i2, :]\n",
    "\n",
    "        Z = torch.transpose(Z, 0, 1)\n",
    "        svd_scores.append(centered_svd_val(Z).item())\n",
    "    # print(\"Sigma matrix shape:\",Z.shape[1])\n",
    "    return np.stack(svd_scores)\n",
    "\n",
    "\n",
    "def centered_svd_val(Z, alpha=0.001):\n",
    "    \"\"\"Compute the mean log singular value of a centered covariance matrix.\n",
    "\n",
    "    This function centers the data and computes the singular value decomposition\n",
    "    (SVD) of the resulting covariance matrix. It then returns the mean of the\n",
    "    log singular values, regularized by `alpha`.\n",
    "\n",
    "    Args:\n",
    "        Z (torch.Tensor): A 2D tensor representing features hidden acts.\n",
    "        alpha (float, optional): Regularization parameter added to the covariance matrix.\n",
    "            Defaults to 0.001.\n",
    "\n",
    "    Returns:\n",
    "        float: The mean of the log singular values of the centered covariance matrix.\n",
    "    \"\"\"\n",
    "    # assumes Z is in full precision\n",
    "    # Center the lines of  Z (i.e. subtract the average of each line).\n",
    "    # Allows to study variance without bias due to a non-zero mean.\n",
    "    J = torch.eye(Z.shape[0]) - (1 / Z.shape[0]) * torch.ones(Z.shape[0], Z.shape[0])\n",
    "    # Compute column-centered covariance matrix of Z\n",
    "    Sigma = torch.matmul(torch.matmul(Z.t(), J), Z)\n",
    "    # Regularization for stabilization\n",
    "    Sigma = Sigma + alpha * torch.eye(Sigma.shape[0])\n",
    "    # Singular Value Decomposition\n",
    "    svdvals = torch.linalg.svdvals(Sigma)\n",
    "    # Final Score\n",
    "    eigscore = torch.log(svdvals).mean() # multiplication by 2 missing from the paper ? \n",
    "    return eigscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tok_in:  tensor([[    1,  2581, 26099,  1597,  9577,   432,   324,   347,  9115,   332]],\n",
      "       device='cuda:0')\n",
      "tok_in.shape:  torch.Size([1, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(logit):  1\n",
      "logit.shape : torch.Size([1, 10, 32000])\n",
      "=============len(attn): 32\n",
      "logits[i]:  tensor([[ 1.0217e-01, -2.1973e-01,  3.1348e-01,  ...,  1.3281e+00,\n",
      "          1.8799e+00,  6.4502e-01],\n",
      "        [-6.7344e+00, -6.9375e+00, -1.1934e+00,  ..., -2.8359e+00,\n",
      "         -7.2969e+00, -3.1680e+00],\n",
      "        [-3.2051e+00, -3.1211e+00, -3.9014e-01,  ..., -2.3281e+00,\n",
      "         -7.9648e+00, -3.3086e+00],\n",
      "        ...,\n",
      "        [-3.1445e+00, -4.4258e+00,  2.3438e+00,  ..., -4.1289e+00,\n",
      "         -7.4336e+00, -1.7598e+00],\n",
      "        [-3.5176e+00, -2.7402e+00,  4.0391e+00,  ..., -1.5322e+00,\n",
      "         -2.4648e+00, -5.1270e-03],\n",
      "        [-4.9492e+00, -4.7539e+00,  3.8203e+00,  ..., -4.7812e+00,\n",
      "         -6.2500e+00, -4.0703e+00]], dtype=torch.float16)\n",
      "logits[i].shape:  torch.Size([10, 32000])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 256\u001b[39m\n\u001b[32m    253\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=============len(attn):\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(attn))\n\u001b[32m    254\u001b[39m tok_in = tok_in.cpu()\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m \u001b[43mcompute_scores\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mlogit\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_acts\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mhidden_act\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# hidden_acts est la liste des activations de chaque couche pour ce sample\u001b[39;49;00m\n\u001b[32m    259\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattns\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmt_list\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlogit\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# ['hidden', 'attns', 'logit'],\u001b[39;49;00m\n\u001b[32m    261\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtok_ins\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtok_in\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 62\u001b[39m, in \u001b[36mcompute_scores\u001b[39m\u001b[34m(logits, hidden_acts, attns, tok_ins, mt_list, indiv_scores, tok_lens, use_toklens)\u001b[39m\n\u001b[32m     60\u001b[39m mt_score = []\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mt == \u001b[33m\"\u001b[39m\u001b[33mlogit\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m     mt_score.append(\u001b[43mperplexity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtok_ins\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtok_lens\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m])\n\u001b[32m     63\u001b[39m     \u001b[38;5;66;03m#indiv_scores[mt][\"perplexity\"].append(mt_score[-1])\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m     \u001b[38;5;66;03m#mt_score.append(window_logit_entropy(logits, tok_lens, w=1)[0])\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     68\u001b[39m     \u001b[38;5;66;03m#mt_score.append(logit_entropy(logits, tok_lens, top_k=50)[0])\u001b[39;00m\n\u001b[32m     69\u001b[39m     \u001b[38;5;66;03m#indiv_scores[mt][\"logit_entropy\"].append(mt_score[-1])\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m mt == \u001b[33m\"\u001b[39m\u001b[33mhidden\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 181\u001b[39m, in \u001b[36mperplexity\u001b[39m\u001b[34m(logits, tok_ins, tok_lens, min_k)\u001b[39m\n\u001b[32m    178\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mlogits[i]: \u001b[39m\u001b[33m\"\u001b[39m, logits[i])\n\u001b[32m    179\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mlogits[i].shape: \u001b[39m\u001b[33m\"\u001b[39m, logits[i].shape)\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m i1, i2 = \u001b[43mtok_lens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m[\u001b[32m0\u001b[39m], tok_lens[i][\u001b[32m1\u001b[39m]\n\u001b[32m    183\u001b[39m pr = torch.log(softmax(logits[i]))[torch.arange(i1, i2) - \u001b[32m1\u001b[39m, tok_ins[i][\u001b[32m0\u001b[39m, i1:i2]]\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m min_k \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Faire tourner les fonctions du papier LLM check sur ma machine \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def get_model_vals(model, tok_in):\n",
    "    \"\"\"Run the model forward pass to obtain logits, hidden states, and attention scores.\n",
    "\n",
    "    Args:\n",
    "        model: A pretrained model compatible with the transformers API.\n",
    "        tok_in (torch.Tensor): A tensor of tokenized input IDs.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple `(logits, hidden_states, attentions)` where:\n",
    "        logits (torch.Tensor): Output logits from the model.\n",
    "        hidden_states (tuple of torch.Tensor): Hidden states from each model layer.\n",
    "        attentions (tuple of torch.Tensor): Attention weights from each model layer.\n",
    "    \"\"\"\n",
    "    kwargs = {\n",
    "        \"input_ids\": tok_in,\n",
    "        \"use_cache\": False,\n",
    "        \"past_key_values\": None,\n",
    "        \"output_attentions\": True,\n",
    "        \"output_hidden_states\": True,\n",
    "        \"return_dict\": True,\n",
    "    }\n",
    "    with torch.no_grad():\n",
    "        output = model(**kwargs)\n",
    "    return output.logits, output.hidden_states, output.attentions\n",
    "\n",
    "\n",
    "\n",
    "#def compute_scores(logits, hidden_acts, attns, scores,  mt_list, tok_ins, indiv_scores=None, tok_lens=[], use_toklens=False):\n",
    "def compute_scores(logits, hidden_acts, attns, tok_ins, mt_list = ['hidden'], indiv_scores=None, tok_lens=[], use_toklens=False):\n",
    "    \"\"\"Compute various evaluation scores (e.g., perplexity, entropy, SVD scores) from model outputs.\n",
    "\n",
    "    This function takes model outputs (logits, hidden states, attentions) and computes\n",
    "    a list of metric scores defined by `mt_list`. The computed scores are appended\n",
    "    to `scores` and `indiv_scores` dictionaries for tracking.\n",
    "\n",
    "    NOTE: The indiv_scores score dictionary will be saved to disk and then used for final metric computation in\n",
    "    check scores ipynb\n",
    "\n",
    "    Args:\n",
    "        logits: Model logits.\n",
    "        hidden_acts: Hidden activations.\n",
    "        attns: Attention matrices.\n",
    "        scores (list): A list to store aggregated scores across samples.\n",
    "        indiv_scores (dict): A dictionary to store metric-specific scores for each sample\n",
    "        mt_list (list): A list of metric types to compute.\n",
    "        tok_ins: A list of tokenized inputs for each sample.\n",
    "        tok_lens: A list of tuples indicating the start and end token indices for each sample.\n",
    "        use_toklens (bool, optional): Whether to use `tok_lens` to slice sequences. Defaults to True.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If an invalid metric type is encountered in `mt_list`.\n",
    "    \"\"\"\n",
    "    j=0\n",
    "    sample_scores = []\n",
    "    for mt in mt_list:\n",
    "        mt_score = []\n",
    "        if mt == \"logit\":\n",
    "            mt_score.append(perplexity(logits, tok_ins, tok_lens)[0])\n",
    "            #indiv_scores[mt][\"perplexity\"].append(mt_score[-1])\n",
    "\n",
    "            #mt_score.append(window_logit_entropy(logits, tok_lens, w=1)[0])\n",
    "            #indiv_scores[mt][\"window_entropy\"].append(mt_score[-1])\n",
    "\n",
    "            #mt_score.append(logit_entropy(logits, tok_lens, top_k=50)[0])\n",
    "            #indiv_scores[mt][\"logit_entropy\"].append(mt_score[-1])\n",
    "\n",
    "        elif mt == \"hidden\":\n",
    "            print(\"=============== j ===============\", j)\n",
    "            j+=1\n",
    "            for layer_num in range(1, len(hidden_acts[0])):\n",
    "                print(\"****** layer_num: *******\", layer_num)\n",
    "                mt_score.append(get_svd_eval(hidden_acts, layer_num, tok_lens, use_toklens)[0])\n",
    "                #indiv_scores[mt][\"Hly\" + str(layer_num)].append(mt_score[-1])\n",
    "\n",
    "        elif mt == \"attns\":\n",
    "            for layer_num in range(1, len(attns[0])):\n",
    "                mt_score.append(get_attn_eig_prod(attns, layer_num, tok_lens, use_toklens)[0])\n",
    "                #indiv_scores[mt][\"Attn\" + str(layer_num)].append(mt_score[-1])\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Invalid method type\")\n",
    "\n",
    "        sample_scores.extend(mt_score)\n",
    "    #scores.append(sample_scores)\n",
    "\n",
    "def centered_svd_val(Z, alpha=0.001):\n",
    "    \"\"\"Compute the mean log singular value of a centered covariance matrix.\n",
    "\n",
    "    This function centers the data and computes the singular value decomposition\n",
    "    (SVD) of the resulting covariance matrix. It then returns the mean of the\n",
    "    log singular values, regularized by `alpha`.\n",
    "\n",
    "    Args:\n",
    "        Z (torch.Tensor): A 2D tensor representing features hidden acts.\n",
    "        alpha (float, optional): Regularization parameter added to the covariance matrix.\n",
    "            Defaults to 0.001.\n",
    "\n",
    "    Returns:\n",
    "        float: The mean of the log singular values of the centered covariance matrix.\n",
    "    \"\"\"\n",
    "    # assumes Z is in full precision\n",
    "    print(\"Z.shape[0]: \", Z.shape[0])\n",
    "    print(\"--Z.shape: \", Z.shape)\n",
    "    J = torch.eye(Z.shape[0]) - (1 / Z.shape[0]) * torch.ones(Z.shape[0], Z.shape[0])\n",
    "    print(\"J.shape: \", J.shape)\n",
    "    Sigma = torch.matmul(torch.matmul(Z.t(), J), Z)\n",
    "    Sigma = Sigma + alpha * torch.eye(Sigma.shape[0])\n",
    "    print(\"Sigma.shape: \", Sigma.shape)\n",
    "    svdvals = torch.linalg.svdvals(Sigma)\n",
    "    eigscore = torch.log(svdvals).mean()\n",
    "    return eigscore\n",
    "\n",
    "def get_svd_eval(hidden_acts, layer_num=15, tok_lens=[], use_toklens=False):\n",
    "    \"\"\"Evaluate hidden states at a given layer using SVD-based scoring.\n",
    "\n",
    "    For each sample, this function extracts the hidden states at a specified layer,\n",
    "    optionally slices them according to `tok_lens`, and computes the SVD-based score.\n",
    "\n",
    "    Args:\n",
    "        hidden_acts (list): A list of tuples, each containing hidden states for all layers\n",
    "            for a single sample.\n",
    "        layer_num (int, optional): The layer index to evaluate. Defaults to 15.\n",
    "        tok_lens (list, optional): A list of (start, end) indices for each sample to slice\n",
    "            the hidden states. Defaults to [].\n",
    "        use_toklens (bool, optional): Whether to slice the hidden states using `tok_lens`.\n",
    "            Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        np.array: An array of SVD-based scores for each sample.\n",
    "    \"\"\"\n",
    "    svd_scores = []\n",
    "    print(\"len(hidden_acts): \", len(hidden_acts))\n",
    "    for i in range(len(hidden_acts)):\n",
    "        print(\"i: \", i)\n",
    "        Z = hidden_acts[i][layer_num]\n",
    "        print(\"Z.shape: \", Z.shape) # (seq_len, hidden_size)\n",
    "\n",
    "        if use_toklens and tok_lens[i]:\n",
    "            i1, i2 = tok_lens[i][0], tok_lens[i][1]\n",
    "            Z = Z[i1:i2, :]\n",
    "\n",
    "        Z = torch.transpose(Z, 0, 1) # (hidden_size, seq_len)\n",
    "        print(\"Z.T.shape: \", Z.shape)\n",
    "        svd_scores.append(centered_svd_val(Z).item())\n",
    "        print(\"len(svd_scores)\", len(svd_scores))\n",
    "        print(\"svd_scores[0]: \", svd_scores)\n",
    "    # print(\"Sigma matrix shape:\",Z.shape[1])\n",
    "    return np.stack(svd_scores)\n",
    "\n",
    "\n",
    "def perplexity(logits, tok_ins, tok_lens, min_k=None):\n",
    "    \"\"\"Compute the perplexity of model predictions for given tokenized inputs.\n",
    "\n",
    "    This function computes the perplexity by taking the negative log probability\n",
    "    of the correct tokens and exponentiating the mean. If `min_k` is provided,\n",
    "    it filters the lowest probabilities to compute a restricted perplexity.\n",
    "\n",
    "    Args:\n",
    "        logits: A list or array of model logits (samples x seq_len x vocab_size).\n",
    "        tok_ins: A list of tokenized input IDs for each sample.\n",
    "        tok_lens (list): A list of (start, end) indices specifying the portion of the\n",
    "            sequence to evaluate.\n",
    "        min_k (float, optional): A fraction of tokens to consider from the lowest\n",
    "            probabilities. If not None, only these tokens are considered.\n",
    "\n",
    "    Returns:\n",
    "        np.array: An array of perplexity values for each sample.\n",
    "    \"\"\"\n",
    "    softmax = torch.nn.Softmax(dim=-1)\n",
    "    ppls = []\n",
    "\n",
    "    \n",
    "    for i in range(len(logits)):\n",
    "        print(\"logits[i]: \", logits[i])\n",
    "        print(\"logits[i].shape: \", logits[i].shape)\n",
    "        \n",
    "        i1, i2 = tok_lens[i][0], tok_lens[i][1]\n",
    "        \n",
    "        pr = torch.log(softmax(logits[i]))[torch.arange(i1, i2) - 1, tok_ins[i][0, i1:i2]]\n",
    "        if min_k is not None:\n",
    "            pr = torch.topk(pr, k=int(min_k * len(pr)), largest=False).values\n",
    "        ppls.append(torch.exp(-pr.mean()).item())\n",
    "\n",
    "    return np.stack(ppls)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_attn_eig_prod(attns, layer_num=15, tok_lens=[], use_toklens=True):\n",
    "    \"\"\"Compute an eigenvalue-based attention score by analyzing attention matrices.\n",
    "\n",
    "    This function takes the attention matrices of a given layer and for each sample,\n",
    "    computes the mean log of the diagonal elements (assumed to be eigenvalues) across\n",
    "    all attention heads. Slices are applied if `tok_lens` is used.\n",
    "\n",
    "    Args:\n",
    "        attns (list): A list of tuples, each containing attention matrices for all layers\n",
    "            and heads for a single sample.\n",
    "        layer_num (int, optional): The layer index to evaluate. Defaults to 15.\n",
    "        tok_lens (list, optional): A list of (start, end) indices for each sample to slice\n",
    "            the attention matrices. Defaults to [].\n",
    "        use_toklens (bool, optional): Whether to slice the attention matrices using `tok_lens`.\n",
    "            Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        np.array: An array of computed attention-based eigenvalue scores for each sample.\n",
    "    \"\"\"\n",
    "    attn_scores = []\n",
    "\n",
    "    for i in range(len(attns)):  # iterating over number of samples\n",
    "        eigscore = 0.0\n",
    "        counter = 0\n",
    "        for attn_head_num in range(len(attns[i][layer_num])):  # iterating over number of attn heads\n",
    "            counter += 1\n",
    "            # attns[i][layer_num][j] is of size seq_len x seq_len = [10,10] if 10 tokens in the sentence\n",
    "            Sigma = attns[i][layer_num][attn_head_num]\n",
    "            #print(\"Attention, Sigma.shape: \", Sigma.shape)\n",
    "\n",
    "            if use_toklens and tok_lens[i]:\n",
    "                i1, i2 = tok_lens[i][0], tok_lens[i][1]\n",
    "                Sigma = Sigma[i1:i2, i1:i2]\n",
    "\n",
    "            eigscore += torch.log(torch.diagonal(Sigma, 0)).mean()\n",
    "            #print(\"eigscore: \", eigscore)\n",
    "             \n",
    "        attn_scores.append(eigscore.item())\n",
    "        #print(\"len(attn_scores): \", len(attn_scores))\n",
    "        res = np.stack(attn_scores)\n",
    "        #print(\"res.shape: \", res.shape)\n",
    "\n",
    "    #print(\"Counter: \", counter)\n",
    "    return res\n",
    "\n",
    "\n",
    "prompts = [\"Je suis une très jolie fleur\"]\n",
    "inputs = tokenizer(prompts, padding=True, truncation=True, return_tensors=\"pt\").to(model.device)\n",
    "tok_in  = inputs['input_ids']\n",
    "print(\"tok_in: \", tok_in)\n",
    "print(\"tok_in.shape: \", tok_in.shape)\n",
    "\n",
    "logit, hidden_act, attn = get_model_vals(model, tok_in.to(0))\n",
    "print(\"len(logit): \", len(logit))\n",
    "print(\"logit.shape :\", logit.shape)\n",
    "\n",
    "# Unpacking the values into lists on CPU\n",
    "logit = logit[0].cpu()\n",
    "hidden_act = [x[0].to(torch.float32).detach().cpu() for x in hidden_act]\n",
    "attn = [x[0].to(torch.float32).detach().cpu() for x in attn]\n",
    "print(\"=============len(attn):\", len(attn))\n",
    "tok_in = tok_in.cpu()\n",
    "\n",
    "compute_scores(\n",
    "    [logit],\n",
    "    hidden_acts=[hidden_act], # hidden_acts est la liste des activations de chaque couche pour ce sample\n",
    "    attns= [attn],\n",
    "    mt_list= ['logit'], # ['hidden', 'attns', 'logit'],\n",
    "    tok_ins=[tok_in]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brouillon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (694832033.py, line 2)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31melif mode == \"cov_svd\":\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "    # ================\n",
    "    elif mode == \"cov_svd\":\n",
    "        # Compute the mean of the log of singular values of the centered covariance \n",
    "        # for each sample in the batch, taking into account only valid tokens.\n",
    "        svd_scores = []\n",
    "        batch_size = selected_layer.shape[0]\n",
    "        for i in range(batch_size):\n",
    "            # Select valid tokens \n",
    "            mask = attention_mask[i].bool()\n",
    "            Z = selected_layer[i][mask]  # (num_valid_tokens, hidden_size)\n",
    "            if Z.shape[0] == 0:\n",
    "                svd_scores.append(float('nan'))\n",
    "                continue\n",
    "            # Transpose to have (hidden_size, num_valid_tokens)\n",
    "            Z = Z.transpose(0, 1)\n",
    "            # Assumes Z is in full precision\n",
    "            # Center the features of Z (subtract the average of each line) and compute covariance on tokens.\n",
    "            # Allows to study variance without bias due to a non-zero mean.\n",
    "            d = Z.shape[0] # hidden_size\n",
    "            J = torch.eye(d, device=Z.device) - (1 / d) * torch.ones(d, d, device=Z.device)\n",
    "            # Compute centered covariance matrix of Z\n",
    "            Sigma = torch.matmul(torch.matmul(Z, J), Z.t())\n",
    "            # Regularization for stabilization\n",
    "            Sigma = Sigma + alpha * torch.eye(Sigma.shape[0], device=Z.device)\n",
    "            # Singular Value Decomposition\n",
    "            svdvals = torch.linalg.svdvals(Sigma)\n",
    "            # Final Score\n",
    "            eigscore = torch.log(svdvals).mean() # mult by 2 missing from the paper? \n",
    "            svd_scores.append(eigscore.item())\n",
    "        aggregated_tokens = np.array(svd_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizer, PreTrainedModel\n",
    "import torch\n",
    "from torch.utils.hooks import RemovableHandle\n",
    "from typing import Tuple, Literal, List, Optional, Dict\n",
    "\n",
    "def extract_token_activations(\n",
    "    selected_layer: torch.Tensor,\n",
    "    attention_mask: torch.Tensor,\n",
    "    device: torch.device,\n",
    "    modes: List[Literal[\n",
    "        \"average\", \"last\", \"max\", \"first_generated\", \n",
    "        \"token_cov_svd\", \"feat_cov_svd\", \n",
    "        \"token_cov_stats\", \"feat_cov_stats\", \"feat_cov_var\"\n",
    "    ]] = [\"average\"],\n",
    "    skip_length: Optional[int] = None,\n",
    "    alpha: int = 0.001,\n",
    ") -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"   \n",
    "    Aggregate token-level activations over a specified span for each sequence in a batch,\n",
    "    using various aggregation modes and attention mask.\n",
    "\n",
    "    This function takes as input:\n",
    "      - The layer activations (selected_layer) for each token in a batch of sequences,\n",
    "      - An attention mask (attention_mask) of the same shape, where 1 indicates tokens to include\n",
    "        in the aggregation and 0 marks tokens to ignore.\n",
    "\n",
    "    The attention mask may be the original model mask, or a custom mask generated using\n",
    "    `compute_offset_attention_mask` to dynamically select a sub-span of tokens.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    selected_layer : torch.Tensor\n",
    "        Tensor of shape (batch_size, seq_len, hidden_size) containing model activations for each token.\n",
    "    attention_mask : torch.Tensor\n",
    "        Attention mask of shape (batch_size, seq_len),  1 for real tokens, 0 for padding.\n",
    "    device : torch.device\n",
    "        Device for computation.\n",
    "    modes : List[str]\n",
    "        List of aggregation modes to compute. Computed using only valid tokens where attention_mask == 1.\n",
    "        Supported:\n",
    "        - \"average\": Mean activation vector across valid tokens. Shape: (batch_size, hidden_size)\n",
    "        - \"max\": Element-wise max activation across valid tokens. Shape: (batch_size, hidden_size)\n",
    "        - \"last\": Activation vector of last valid token in each sequence. Shape: (batch_size, hidden_size)\n",
    "        - \"first_generated\": Activation of the first generated valid token in each sequence. Shape: (batch_size, hidden_size)\n",
    "             If skip_length is provided, selects the token starting from that offset. \n",
    "        - \"token_cov_svd\": Mean log singular value of the centered token covariance matrix. Shape: (batch_size,)\n",
    "        - \"feat_cov_svd\": Mean log singular value of the centered feature covariance matrix. Shape: (batch_size,)\n",
    "        - \"token_cov_stats\": Statistics (mean, std, min, max) of the centered token covariance matrix. Shape: (batch_size, 4)\n",
    "        - \"feat_cov_stats\": Statistics (mean, std, min, max) of the centered feature covariance matrix. Shape: (batch_size, 4)\n",
    "        - \"feat_cov_var\": Diagonal of the centered feature covariance matrix (variances). Shape: (batch_size, hidden_size)\n",
    "\n",
    "    skip_length : Optional[int]\n",
    "        If provided, used to explicitly select the first generated token (useful for \"first_generated\" mode).\n",
    "    alpha : float\n",
    "        Regularization parameter added to the covariance matrix.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, torch.Tensor or np.ndarray]\n",
    "        Dictionary mapping each mode to its result:\n",
    "            - (batch_size, hidden_size) for \"average\", \"max\", \"last\", \"first_generated\", \"feat_cov_var\"\n",
    "            - (batch_size,) for \"token_cov_svd\", \"feat_cov_svd\"\n",
    "            -  (batch_size, 4) for \"token_cov_stats\", \"feat_cov_stats\"\n",
    "        \n",
    "    NOTE: computation `token_cov_svd` score from: \n",
    "    \"LLM-Check: Investigating Detection of Hallucinations in Large Language Models\"\n",
    "    (Sriramanan et al. 2024)\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size, seq_len, hidden_size = selected_layer.shape\n",
    "    print(f\"batch_size: {batch_size}, hidden_size: {hidden_size}\")\n",
    "    aggregated_tokens = {}\n",
    "    \n",
    "    # Move to device \n",
    "    attention_mask = attention_mask.to(selected_layer.device)\n",
    "    print(\"selected_layer.device\", selected_layer.device)\n",
    "    print(\"attention_mask.devce\", attention_mask.device)\n",
    "\n",
    "    # =======================================\n",
    "    # Select the first token with optional offset `skip_length`\n",
    "    # =======================================\n",
    "    if \"first_generated\" in modes:\n",
    "        batch_indices = torch.arange(batch_size, device=device)\n",
    "        if skip_length is not None:\n",
    "            first_indices = torch.full((batch_size,), skip_length, device=device, dtype=torch.long)\n",
    "        else:\n",
    "            first_indices = (attention_mask == 1).float().argmax(dim=1)\n",
    "        first = selected_layer[batch_indices, first_indices] # Shape: (batch_size, hidden_size)\n",
    "        aggregated_tokens[\"first_generated\"] = first\n",
    "\n",
    "    # =======================================\n",
    "    # Select the last token \n",
    "    # =======================================\n",
    "    if \"last\" in modes:\n",
    "        last_indices = attention_mask.shape[1] - 1 - attention_mask.flip(dims=[1]).float().argmax(dim=1)\n",
    "        batch_indices = torch.arange(batch_size, device=device)\n",
    "        last = selected_layer[batch_indices, last_indices]  # Shape: (batch_size, hidden_size)\n",
    "        aggregated_tokens[\"last\"] = last\n",
    "\n",
    "    # =======================================\n",
    "    # Apply mask and compute aggregation \n",
    "    # =======================================\n",
    "    if \"average\" in modes or \"max\" in modes:\n",
    "        # Add one dimension for the broadcast on hidden_size\n",
    "        mask_float = attention_mask.float().unsqueeze(-1)  # (batch_size, num_valid_tokens, 1)\n",
    "        # Apply the mask to the activations: zero out tokens outside the target interval\n",
    "        masked = selected_layer * mask_float\n",
    "        #  Count the number of selected tokens for each sequence (avoid division by zero with clamp)\n",
    "        counts = mask_float.sum(dim=1).clamp(min=1e-6)\n",
    "        if \"average\" in modes:\n",
    "            # Compute the mean activation vector for each sequence over the selected interval\n",
    "            avg = masked.sum(dim=1) / counts # Shape: (batch_size, hidden_size)\n",
    "            aggregated_tokens[\"average\"] = avg\n",
    "        if \"max\" in modes:\n",
    "            # Replace padding with -inf to exclude from max calculation\n",
    "            masked_max = masked.masked_fill(mask_float.logical_not(), float('-inf'))\n",
    "            # Extract maximum values across sequence dimension\n",
    "            max_vals, _ = masked_max.max(dim=1) # Shape: (batch_size, hidden_size)\n",
    "            aggregated_tokens[\"max\"] = max_vals\n",
    "\n",
    "    # =======================================\n",
    "    # Covariance-based metrics\n",
    "    # =======================================\n",
    "    if any(m in modes for m in [\"token_cov_svd\", \"feat_cov_svd\", \"token_cov_stats\", \"feat_cov_stats\", \"feat_cov_var\"]):\n",
    "        token_cov_svd = [] \n",
    "        feat_cov_svd = [] \n",
    "        token_cov_stats = []\n",
    "        feat_cov_stats = []\n",
    "        feat_cov_var = []\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            # Select valid tokens \n",
    "            mask = attention_mask[i].bool()\n",
    "            Z = selected_layer[i][mask]  # (num_valid_tokens, hidden_size)\n",
    "            if Z.shape[0] == 0:\n",
    "                feat_cov_var.append(torch.full((hidden_size,), float('nan')))\n",
    "                token_cov_svd.append(float('nan'))\n",
    "                feat_cov_svd.append(float('nan'))\n",
    "                token_cov_stats.append(dict())\n",
    "                feat_cov_stats.append(dict())\n",
    "                continue\n",
    "            \n",
    "            if Z.dtype != torch.float32:\n",
    "                Z = Z.to(torch.float32)\n",
    "            num_valid_tokens = Z.shape[0]\n",
    "\n",
    "            # Compute covariance matrix on tokens : Sigma_token \n",
    "            # ---------------------------------------\n",
    "            # Assumes Z is in full precision\n",
    "            # Center the features of Z (subtract the average of each line) and compute covariance on tokens.\n",
    "            # Allows to study variance without bias due to a non-zero mean.\n",
    "            J = torch.eye(hidden_size, device=Z.device, dtype=Z.dtype) - (1 / hidden_size) * torch.ones(hidden_size, hidden_size, device=Z.device, dtype=Z.dtype)\n",
    "            print(\"J.shape:\", J.shape)\n",
    "            # Compute centered covariance matrix of Z\n",
    "            Sigma_token = torch.matmul(torch.matmul(Z, J), Z.t()) # (num_valid_tokens, num_valid_tokens)\n",
    "            # Regularization for stabilization\n",
    "            Sigma_token = Sigma_token + alpha * torch.eye(Sigma_token.shape[0], device=Z.device, dtype=Z.dtype)\n",
    "            print(\"1) Sigma_token stats:\", Sigma_token.mean(), Sigma_token.min(), Sigma_token.max())\n",
    "\n",
    "            # 2. Token covariance\n",
    "            Z_token_centered = Z - Z.mean(dim=1, keepdim=True)\n",
    "            Sigma_token = (Z_token_centered @ Z_token_centered.t()) / max(1, Z.shape[1] - 1)\n",
    "            Sigma_token += alpha * torch.eye(Z.shape[0], device=Z.device, dtype=Z.dtype)\n",
    "            print(\"2) Sigma_token stats:\", Sigma_token.mean(), Sigma_token.min(), Sigma_token.max())\n",
    "\n",
    "\n",
    "    \n",
    "            # Compute covariance matrix on features : Sigma_feat\n",
    "            # ---------------------------------------\n",
    "            # Center the features of Z (subtract the average of each column) and compute covariance on features.\n",
    "            J = torch.eye(num_valid_tokens, device=Z.device, dtype=Z.dtype) - (1 / num_valid_tokens) * torch.ones(num_valid_tokens, num_valid_tokens, device=Z.device, dtype=Z.dtype)\n",
    "            # Compute centered covariance matrix of Z\n",
    "            Sigma_feat = torch.matmul(torch.matmul(Z.t(), J), Z) # (hidden_size, hidden_size)\n",
    "            # Regularization for stabilization\n",
    "            Sigma_feat = Sigma_feat + alpha * torch.eye(Sigma_feat.shape[0], device=Z.device, dtype=Z.dtype)\n",
    "            \n",
    "            # Statistics of the token covariance matrix\n",
    "            # ---------------------------------------\n",
    "            if Sigma_token.dtype != torch.float32:\n",
    "                Sigma_token = Sigma_token.to(torch.float32)\n",
    "            \n",
    "            Sigma_token_diag = Sigma_token.diag()\n",
    "            token_stats = [\n",
    "                Sigma_token_diag.mean().item(),\n",
    "                Sigma_token_diag.std().item(),\n",
    "                Sigma_token_diag.min().item(),\n",
    "                Sigma_token_diag.max().item(),\n",
    "            ]\n",
    "            token_cov_stats.append(token_stats)\n",
    "\n",
    "            # Singular value decomposition (SVD) of the token covariance matrix\n",
    "            # ---------------------------------------\n",
    "            token_svdvals = torch.linalg.svdvals(Sigma_token) # Singular Value Decomposition\n",
    "            token_eigscore = torch.log(token_svdvals).mean()  # mult by 2 missing from the paper? \n",
    "            token_cov_svd.append(token_eigscore)\n",
    "\n",
    "            # Statistics of the feature covariance matrix\n",
    "            # ---------------------------------------\n",
    "            Sigma_feat_diag = Sigma_feat.diag()\n",
    "            \n",
    "            if Sigma_feat_diag.dtype != torch.float32:\n",
    "                Sigma_feat_diag = Sigma_feat_diag.to(torch.float32)\n",
    "\n",
    "            feat_stats = [\n",
    "                Sigma_feat_diag.mean().item(),\n",
    "                Sigma_feat_diag.std().item(),\n",
    "                Sigma_feat_diag.min().item(),\n",
    "                Sigma_feat_diag.max().item()\n",
    "            ]\n",
    "            feat_cov_var.append(Sigma_feat_diag)\n",
    "            feat_cov_stats.append(feat_stats)\n",
    "            \n",
    "            # Singular value decomposition (SVD) of the feature covariance matrix\n",
    "            # ---------------------------------------\n",
    "            feat_svdvals = torch.linalg.svdvals(Sigma_feat) # Singular Value Decomposition\n",
    "            feat_eigscore = torch.log(feat_svdvals).mean() \n",
    "            feat_cov_svd.append(feat_eigscore)\n",
    "\n",
    "        # Return scores\n",
    "        # ---------------------------------------\n",
    "        if \"token_cov_svd\" in modes:\n",
    "            aggregated_tokens[\"token_cov_svd\"] = torch.stack(token_cov_svd) # (batch_size,) \n",
    "        if \"feat_cov_svd\" in modes:\n",
    "            aggregated_tokens[\"feat_cov_svd\"] = torch.stack(feat_cov_svd) # (batch_size,) \n",
    "        if \"token_cov_stats\" in modes:\n",
    "            aggregated_tokens[\"token_cov_stats\"] = torch.tensor(token_cov_stats) # (batch_size, 4) \n",
    "        if \"feat_cov_stats\" in modes:\n",
    "            aggregated_tokens[\"feat_cov_stats\"] = torch.tensor(feat_cov_stats) # (batch_size, 4) \n",
    "        if \"feat_cov_var\" in modes:\n",
    "            aggregated_tokens[\"feat_cov_var\"] = torch.stack(feat_cov_var, dim=0) # (batch_size, hidden_size) \n",
    "\n",
    "    print(\"====================\")\n",
    "    print(aggregated_tokens)\n",
    "    return aggregated_tokens\n",
    "\n",
    "# Mettre tous les aggregated tokens sur le CPU ? \n",
    "# non mais nettoyer la mémoire à chaque fois: torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizer, PreTrainedModel, BatchEncoding\n",
    "import torch\n",
    "from datasets import  Dataset\n",
    "from tqdm import tqdm\n",
    "from typing import List, Callable, Union, Literal, Dict\n",
    "\n",
    "from src.inference.activation_utils import (\n",
    "    compute_offset_attention_mask,\n",
    ")\n",
    "from src.inference.inference_utils import (\n",
    "    build_prompt,\n",
    "    extract_batch, \n",
    "    align_generation_hidden_states,\n",
    "    align_prompt_hidden_states,\n",
    "    build_generation_attention_mask)\n",
    "\n",
    "def generate(\n",
    "    model: PreTrainedModel,\n",
    "    inputs: BatchEncoding,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    max_new_tokens: int = 50,\n",
    "    k_beams: int = 1,\n",
    "    **generate_kwargs\n",
    ") -> Union[torch.Tensor, Dict[str, torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Generate sequences from the model with optional beam search.\n",
    "    Supports advanced options via **generate_kwargs (e.g., output_attentions).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : PreTrainedModel\n",
    "        The language model to use for generation.\n",
    "    inputs : BatchEncoding\n",
    "        Tokenized input prompts.\n",
    "    tokenizer : PreTrainedTokenizer\n",
    "        Tokenizer providing eos and pad token IDs.\n",
    "    max_new_tokens : int, optional\n",
    "        Maximum number of new tokens to generate.\n",
    "    k_beams : int, optional\n",
    "        Number of beams to use. If 1, uses sampling. If >1, beam search is enabled.\n",
    "    **generate_kwargs : dict\n",
    "        Additional keyword arguments passed to `model.generate()`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Union[torch.Tensor, Dict[str, torch.Tensor]]\n",
    "        - If k_beams == 1:\n",
    "            Returns a tensor of generated token IDs: shape (batch_size, prompt_len + gen_len)\n",
    "        - If k_beams > 1:\n",
    "            Returns a dictionary with keys:\n",
    "                - \"sequences\": the generated token IDs\n",
    "                - \"beam_indices\": the beam path for each token\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=1, #max_new_tokens,\n",
    "            do_sample=True    if k_beams == 1 else False,\n",
    "            temperature=0.6   if k_beams == 1 else None,\n",
    "            top_p=0.9         if k_beams == 1 else None,\n",
    "            top_k=50          if k_beams == 1 else None,\n",
    "            num_beams=k_beams,\n",
    "            use_cache=True, \n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id, # Ensures clean padding (right padding)\n",
    "            output_hidden_states=False,      # We rely on the hook to extract hidden states instead (more memory efficient)\n",
    "            output_attentions=False,         # We rely on the hook to extract attention map instead (more memory efficient)\n",
    "            return_dict_in_generate=True,    # Needed for access to beam_indices when num_beams > 1\n",
    "            early_stopping=False if k_beams == 1 else True, #Generation stops as soon as any sequence hits EOS, even if other candidates have not yet finished.\n",
    "            **generate_kwargs                # For future flexibility (e.g., output_attentions, output_scores)\n",
    "        )\n",
    "        return outputs \n",
    "\n",
    "\n",
    "def register_generation_activation_hook(\n",
    "    model: PreTrainedModel,\n",
    "    captured_hidden_list: List[torch.Tensor],\n",
    "    layer_idx: int = -1\n",
    ") -> Tuple[RemovableHandle, dict]:\n",
    "    \"\"\"\n",
    "    Attaches a forward hook to a specific transformer layer to capture hidden states\n",
    "    during autoregressive text generation i.e., at each decoding step.\n",
    "    (more memory-efficient than using output_hidden_states=True).\n",
    "    Transformer layer = self-attention + FFN + normalization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : PreTrainedModel\n",
    "        The Hugging Face causal language model (e.g., GPT, LLaMA).\n",
    "    captured_hidden_list : List[torch.Tensor]\n",
    "        A list that will be filled with hidden states for each generation step. \n",
    "        Each tensor has shape (batch_size * num_beams, seq_len, hidden_size).\n",
    "    layer_idx : int\n",
    "        Index of the transformer block to hook. Defaults to -1 (the last layer).\n",
    "        Use a positive integer if you want to hook an intermediate layer instead.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    RemovableHandle : A handle object\n",
    "        Call `handle.remove()` after generation to remove the hook.\n",
    "    call_counter : int \n",
    "        Stores the number of times the hook is activated.\n",
    "    \"\"\"\n",
    "    # Raise error if layer_idx not in correct range\n",
    "    num_layers = len(model.model.layers)\n",
    "    if not (layer_idx == -1 or 0 <= layer_idx < num_layers):\n",
    "        raise ValueError(\n",
    "            f\"`layer_idx` must be -1 or in [0, {num_layers - 1}], but got {layer_idx}.\"\n",
    "        )\n",
    "    \n",
    "    call_counter = {\"count\": 0} # count how many times the hook is triggered\n",
    "\n",
    "    def hook_fn(module, input, output):\n",
    "        \"\"\"Function called automatically by PyTorch just after\n",
    "            the layer has produced its output during the forward pass.\"\"\"\n",
    "        \n",
    "        call_counter[\"count\"] += 1 \n",
    "\n",
    "        # output is a tuple (hidden_states,) → keep [0]\n",
    "        if layer_idx == -1:\n",
    "            # Capture the final normalized output \n",
    "            captured_hidden_list.append(model.model.norm(output[0]).detach())  # post RMSNorm!\n",
    "        else:\n",
    "            # Capture raw hidden states before layer normalization\n",
    "            captured_hidden_list.append(output[0].detach()) #### TEST #### \n",
    "    \n",
    "    # Register hook on the transformer block\n",
    "    # When Pytorch pass through this layer during forward pass, it also execute hook_fn.\n",
    "    handle = model.model.layers[layer_idx].register_forward_hook(hook_fn)\n",
    "    \n",
    "    return handle, call_counter\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "def run_prompt_and_generation_activation_extraction(\n",
    "    model: PreTrainedModel,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    dataset: Dataset,\n",
    "    batch_size: int = 4,\n",
    "    idx_start_sample: int = 0,\n",
    "    max_samples: int = 1000,\n",
    "    save_to_pkl: bool = False,\n",
    "    output_path: str = \"outputs/all_batch_results.pkl\",\n",
    "    build_prompt_fn: Callable[[str, str], str] = None,\n",
    "    register_generation_activation_hook_fn: Callable = None,\n",
    "    layer_idx: int = -1,  \n",
    "    extract_token_activations_fn: Callable = None,\n",
    "    activation_source: Literal[\"prompt\", \"generation\", \"promptGeneration\"] = \"generation\",\n",
    "    k_beams : int = 1,\n",
    "    start_offset : int = 0,\n",
    "    end_offset : int = 0,\n",
    ") -> Union[Dict[str, List[np.ndarray]], None]:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    batch_activations = {}  # Chosen token activation vectors\n",
    "\n",
    "    for i in tqdm(range(idx_start_sample, idx_start_sample + max_samples, batch_size)):\n",
    "        \n",
    "        # ==============================\n",
    "        # Prepare input batch\n",
    "        # ==============================\n",
    "        batch = extract_batch(dataset, i, batch_size)\n",
    "        print(\"batch[0]: \", batch[0])\n",
    "        \n",
    "        prompts = [build_prompt_fn(s[\"context\"], s[\"question\"]) for s in batch]\n",
    "        #prompts = [build_prompt_fn(s[\"context\"], s[\"question\"]) for s in [batch[1], batch[1]]]\n",
    "        #prompts = [build_prompt_fn(batch[0][\"context\"][:10], batch[0][\"question\"][:5]), \\\n",
    "        #           build_prompt_fn(batch[1][\"context\"][:15], batch[1][\"question\"][:10]) ]\n",
    "        #prompts = [s[\"context\"] + s[\"question\"] for s in batch]\n",
    "        #prompts = [batch[0][\"context\"][:5], batch[0][\"question\"][:5], \\\n",
    "        #           batch[1][\"context\"][:20], batch[1][\"question\"][:20]]\n",
    "        #prompts = [\"ceci est un test\", \"Je ne sais pas pourquoi il y a un probleme\"]\n",
    "\n",
    "        inputs = tokenizer(prompts, padding=True, truncation=True, return_tensors=\"pt\").to(model.device)\n",
    "        prompt_len = inputs[\"input_ids\"].shape[1] # Assumes prompts are padded to same length\n",
    "\n",
    "        for i in range(inputs['input_ids'].shape[0]):\n",
    "            print(\"--- Example\", i)\n",
    "            print(\"prompt:\", prompts[i])\n",
    "            print(\"input_ids:\", inputs[\"input_ids\"][i])\n",
    "            print(\"len(input_ids): \", len(inputs[\"input_ids\"][i]))\n",
    "            print(\"attention_mask:\", inputs[\"attention_mask\"][i])\n",
    "            print(\"pad positions:\", (inputs['input_ids'][i] == tokenizer.pad_token_id).nonzero())\n",
    "            print(\"EOS positions:\", (inputs['input_ids'][i] == tokenizer.eos_token_id).nonzero())\n",
    "\n",
    "\n",
    "        max_id = inputs['input_ids'].max().item()\n",
    "        print(\"max input id:\", max_id)\n",
    "        print(\"embedding matrix size:\", model.get_input_embeddings().weight.size(0))\n",
    "        assert max_id < model.get_input_embeddings().weight.size(0)\n",
    "\n",
    "        # ==============================\n",
    "        # Register forward hook to capture layer output\n",
    "        # ==============================\n",
    "        # This hook collects the hidden states at each decoding step\n",
    "        # activations = [prompt] + [gen_step_1, gen_step_2, ..., gen_step_49], len(activations)=50, if max_new_tokens=50.\n",
    "        activations = [] # activations[k] of Shape: (batch_size * k_beams, seq_len, hidden_size)\n",
    "        #handle, call_counter_act = register_generation_activation_hook_fn(model, activations, layer_idx=layer_idx)\n",
    "        \n",
    "        attentions = [] # attentions[k] of Shape: (batch  *k_beams, n_heads, tgt_seq_len, src_seq_len)\n",
    "        #handle_attn, call_counter_attn = register_generation_attention_hook(model, attentions, layer_idx=layer_idx) # PUT function in params\n",
    "\n",
    "        # ==============================\n",
    "        # Run model forward pass (hook captures activations)\n",
    "        # ==============================\n",
    "        # Generate text from prompts using beam search or sampling. \n",
    "        outputs = generate(model, inputs, tokenizer, max_new_tokens=50, k_beams=k_beams)\n",
    "\n",
    "        print(\"attentions: \", attentions)\n",
    "        \n",
    "    \n",
    "        print(\"len(outputs.attentions): \", len(outputs.attentions))\n",
    "        print(\"outputs.attentions[0].shape: \", outputs.attentions[0].shape)\n",
    "        \n",
    "    \n",
    "        print(\"outputs.sequences.device: \", outputs.sequences.device)\n",
    "    \n",
    "        # Retrieve text of generated answers\n",
    "        gen_answers = tokenizer.batch_decode(\n",
    "            outputs.sequences[:, prompt_len:], \n",
    "            skip_special_tokens=True\n",
    "        ) # Shape: [batch_size,]\n",
    "        \n",
    "        # Define prompt and generation hidden states \n",
    "        #   outputs.hidden_states: list of hidden states at each generation step.\n",
    "        #   - outputs.hidden_states[0][layer_idx]: hidden states for the entire prompt\n",
    "        #   Shape: (batch_size * k_beams, prompt_len, hidden_size)\n",
    "        #   - outputs.hidden_states[1 + i][layer_idx]: hidden states after generating the i-th new token\n",
    "        #   Shape: (batch_size * k_beams, 1, hidden_size), for i = 0, ..., max_new_tokens-1\n",
    "        #   So, for max_new_tokens=50 in generate(): len(outputs.hidden_states)=50\n",
    "        #   and outputs.hidden_states: [prompt] + [gen_step_1] + [gen_step_2] + ... + [gen_step_49]\n",
    "        prompt_activations=outputs.hidden_states[0][layer_idx] \n",
    "        generation_activations=[outputs.hidden_states[k][layer_idx] for k in range(1, len(outputs.hidden_states))] \n",
    "        \n",
    "        \n",
    "        print(\"len(prompt_activations): \", len(prompt_activations))\n",
    "        print(\"len(generation_activations): \", len(generation_activations))\n",
    "\n",
    "        print(\"generation_activations[0].device: \", generation_activations[0].device)\n",
    "        print(\"prompt_activations.device:\", prompt_activations.device)\n",
    "\n",
    "        print(\"Layer L = \", layer_idx)\n",
    "        print(\"=======================================\")\n",
    "        print(\"outputs.sequences.shape[1]: \", outputs.sequences.shape[1])\n",
    "        if k_beams > 1:\n",
    "            print(\"outputs.beam_indices:\", outputs.beam_indices)\n",
    "            print(\"outputs.beam_indices.shape[1] :\", outputs.beam_indices.shape[1]) \n",
    "        print(\"outputs.sequences[:,prompt_len:] :\", outputs.sequences[: , prompt_len:])\n",
    "        print(\"outputs.sequences[: , prompt_len:].shape[1]:\", outputs.sequences[: , prompt_len:].shape[1])\n",
    "        print(\"gen_answers :\", gen_answers)\n",
    "        print(\"=======================================\")\n",
    "        print(\"Length of activations:\", len(activations))\n",
    "        print(\"Shape of activations[0]:\", activations[0].shape)\n",
    "        print(\"Shape of activations[1]:\", activations[1].shape)\n",
    "        print(\"Shape of activations[-1]:\", activations[-1].shape)\n",
    "        print(\"============\")\n",
    "        print(\"Length of outputs.hidden_states: \", len(outputs.hidden_states))\n",
    "        print(\"Shape of outputs.hidden_states[0][L]:\", outputs.hidden_states[0][layer_idx].shape)\n",
    "        print(\"Shape of outputs.hidden_states[1][L]:\", outputs.hidden_states[1][layer_idx].shape)\n",
    "        print(\"Shape of outputs.hidden_states[-1][L]:\", outputs.hidden_states[-1][layer_idx].shape)\n",
    "        print(\"=======================================\")\n",
    "\n",
    "        print(\"activations[0] stats\")\n",
    "        print(\"mean\", activations[0].mean().item(),\n",
    "              \"min\", activations[0].min().item(), \n",
    "              \"max\", activations[0].max().item(), \n",
    "              \"has inf?\", torch.isinf(activations[0]).any().item(),\n",
    "              \"has nan?\", torch.isnan(activations[0]).any().item())\n",
    "        print(\"outputs.hidden_states[0][layer_idx] stats\")\n",
    "        print(\"mean\", outputs.hidden_states[0][layer_idx].mean().item(),\n",
    "              \"min\", outputs.hidden_states[0][layer_idx].min().item(), \n",
    "              \"max\", outputs.hidden_states[0][layer_idx].max().item(), \n",
    "              \"has inf?\", torch.isinf(outputs.hidden_states[0][layer_idx]).any().item(),\n",
    "              \"has nan?\", torch.isnan(outputs.hidden_states[0][layer_idx]).any().item())\n",
    "        \n",
    "        print(\"activations[1:][-1] stats\")\n",
    "        print(\"mean\", activations[1:][-1].mean().item(),\n",
    "              \"min\", activations[1:][-1].min().item(), \n",
    "              \"max\", activations[1:][-1].max().item(), \n",
    "              \"has inf?\", torch.isinf(activations[1:][-1]).any().item(),\n",
    "            \"has nan?\", torch.isnan(activations[1:][-1]).any().item())\n",
    "        print(\"outputs.hidden_states[-1][layer_idx] stats\")\n",
    "        print(\"mean\", outputs.hidden_states[-1][layer_idx].mean().item(),\n",
    "              \"min\", outputs.hidden_states[-1][layer_idx].min().item(), \n",
    "              \"max\", outputs.hidden_states[-1][layer_idx].max().item(), \n",
    "              \"has inf?\", torch.isinf(outputs.hidden_states[-1][layer_idx]).any().item(),\n",
    "            \"has nan?\", torch.isnan(outputs.hidden_states[-1][layer_idx]).any().item())\n",
    "        print(\"=======================================\")\n",
    "\n",
    "        # ===============================\n",
    "        # Truncate activations to match real generation steps (cf. Understanding Note #1)\n",
    "        # ===============================\n",
    "        # During generation, the model may run extra forward passes (especially with beam search)\n",
    "        # beyond the number of tokens in the final output. This results in activations being longer\n",
    "        # than needed — we need to truncate them accordingly.\n",
    "        # (see Understanding Note #1).\n",
    "        if k_beams > 1:\n",
    "            # In beam search, we use beam_indices.shape[1] to determine the actual number of generation steps\n",
    "            gen_len = outputs.beam_indices.shape[1]\n",
    "        else:\n",
    "            # In greedy/top-k sampling, gen_len is simply the number of new tokens beyond the prompt\n",
    "            gen_len = outputs.sequences.shape[1] - prompt_len\n",
    "\n",
    "        # Sometimes, activations may include extra \"ghost\" steps (e.g., due to internal padding/sync in beam search)\n",
    "        bool_truncate_activations = (len(generation_activations) >= gen_len) \n",
    " \n",
    "        if bool_truncate_activations:\n",
    "            # Truncate extra steps to ensure alignment with generated tokens\n",
    "            generation_activations = generation_activations[:gen_len]\n",
    "\n",
    "        \"\"\"\n",
    "        ==================================\n",
    "        Understanding Note #1:\n",
    "        ==================================\n",
    "        When using beam search in Hugging Face Transformers, the number of decoder hidden states\n",
    "        (len(outputs.hidden_states)) can be greater than the number of tokens in the final generated \n",
    "        sequence (outputs.sequences[:,prompt_len:].shape[1] = outputs.beam_indices.shape[1]). \n",
    "        This happens because, during beam search, the model explores multiple candidate sequences \n",
    "        (beams) at each generation step and continues generating until a stopping condition is met \n",
    "        (such as all beams reaching EOS or the maximum number of tokens). But because beams can \n",
    "        finish at different steps (some hitting EOS early, others continuing), the model must keep\n",
    "        generating for the remaining active beams. \n",
    "        *Note* that in our code, outputs.hidden_states and activations are the same. \n",
    "      \n",
    "        Explanation from Hugging Face, January 2023: \n",
    "        (https://github.com/huggingface/transformers/issues/21374)\n",
    "        \"Beam Search: Here it's trickier. In essence, beam search looks for candidate outputs until it hits \n",
    "        a stopping condition. The candidate outputs can have fewer tokens than the total number of generation \n",
    "        steps -- for instance, in an encoder-decoder text model, if your input is How much is 2 + 2? and the \n",
    "        model generates as candidates <BOS>4<EOS> (3 tokens) and <BOS>The answer is potato<EOS> \n",
    "        (for argument's sake, 6 tokens) before deciding to stop, you should see sequences with shape [1, 3] \n",
    "        and decoder_hidden_states with length 5, because 5 tokens were generated internally before settling \n",
    "        on the 1st candidate.\"    \n",
    "        \"\"\"\n",
    "\n",
    "        # ===============================\n",
    "        # Truncate generated token IDs to match activations (cf. Understanding Note #2) \n",
    "        # ===============================\n",
    "        # - When N tokens are generated, only the first N-1 tokens have corresponding hidden states.\n",
    "        #   So activations[1:] covers only the first N-1 steps (cf. Understanding Note #2).\n",
    "        #   Therefore, we exclude the last generated token from outputs.sequences and beam_indices\n",
    "        #   to match activations[1:]\n",
    "        # - Exception: if activations were truncated earlier (bool_truncate_activations = True),\n",
    "        #   then we already lost activations of the final decoding step(s), and our activations[1:]\n",
    "        #   only cover the available tokens. In that case, we keep the full `gen_len` to match.\n",
    "        # (see Understanding Note #2)\n",
    "        if bool_truncate_activations:\n",
    "            expected_gen_len = gen_len  # All generated tokens have hidden states\n",
    "        else: \n",
    "            expected_gen_len  = gen_len - 1 # Drop final token to match activations[1:]\n",
    "\n",
    "        # Truncate generated sequences and beam paths accordingly\n",
    "        truncated_gen_sequences = outputs.sequences[:, prompt_len : prompt_len + expected_gen_len]\n",
    "        if k_beams > 1:\n",
    "            truncated_beam_indices = outputs.beam_indices[:, :expected_gen_len] \n",
    "\n",
    "        \"\"\"\n",
    "        ==================================\n",
    "        Understanding Note #2:\n",
    "        ==================================\n",
    "        When using model.generate() with output_hidden_states=True (what we are replicating here with the hook),\n",
    "        use_cache=True and max_new_tokens=30, there is always an offset between the length of the \n",
    "        generated sequence (outputs.sequences.shape[1][prompt_len:]) and the length of len(outputs.hidden_states) : \n",
    "        * outputs.sequences.shape[1] = prompt_len (17) + max_new_tokens (30) = 47\n",
    "        * len(outputs.hidden_states) = max_new_tokens (30)\n",
    "            With : \n",
    "            * outputs.hidden_states[0][layer_idx].shape = (batch_size, prompt_len, hidden_size)           --> includes the prompt ! \n",
    "            * outputs.hidden_states[i][layer_idx].shape = (batch_size, 1, hidden_size) with 1 <= i <= 29  --> stops at 29 ! \n",
    "        *Note* that in our code, outputs.hidden_states and activations are the same. \n",
    "            \n",
    "        Explanation from Hugging Face, April 2024 \n",
    "        (https://github.com/huggingface/transformers/issues/30036):\n",
    "        \"If you have 30 tokens at the end of generation, you'll always have 29 hidden states.\n",
    "        The token with index N is used to produce hidden states with index N, which is then used \n",
    "        to get the token with index N+1. The generation ends as soon as the target number of \n",
    "        tokens is obtained so, when we obtain the 30th token, we don't spend compute to get the 30th \n",
    "        set of hidden states. You can, however, manually run an additional forward pass to obtain the \n",
    "        30th set of hidden states, corresponding to the 30th token and used to obtain the 31st token.\n",
    "        \"\"\"\n",
    "        # ===============================\n",
    "        # Align generated and prompt hidden states\n",
    "        # ===============================\n",
    "        # Extract the hidden states that correspond to the generated sequence\n",
    "        # selected by the beam search (or top-k sampling if k_beams = 1)\n",
    "        aligned_generation_hidden_states = align_generation_hidden_states(\n",
    "            generation_activations=generation_activations, \n",
    "            beam_indices=truncated_beam_indices if k_beams > 1 else None,\n",
    "            k_beams=k_beams\n",
    "        ) # Shape: (batch_size, gen_len, hidden_size)\n",
    "\n",
    "        # Extract the hidden states that correspond to the prompt\n",
    "        aligned_prompt_hidden_states = align_prompt_hidden_states(\n",
    "            prompt_activations=prompt_activations, \n",
    "            k_beams=k_beams\n",
    "        ) # Shape: (batch_size, prompt_len, hidden_size)\n",
    "\n",
    "        # Concatenate the prompt and generation aligned hidden states  \n",
    "        aligned_prompt_and_gen_hidden_states = torch.cat(\n",
    "            [aligned_prompt_hidden_states, \n",
    "             aligned_generation_hidden_states], \n",
    "             dim=1\n",
    "        ) # Shape: (batch_size, prompt_len + gen_len, hidden_size)\n",
    "\n",
    "        \n",
    "        print(\"=======================================\")\n",
    "        print(\"aligned_prompt_hidden_states stats\")\n",
    "        print(\" min\", aligned_prompt_hidden_states.min().item(), \n",
    "              \"max\", aligned_prompt_hidden_states.max().item(), \n",
    "              \"has inf?\", torch.isinf(aligned_prompt_hidden_states).any().item(),\n",
    "            \"has nan?\", torch.isnan(aligned_prompt_hidden_states).any().item())\n",
    "\n",
    "        print(\"aligned_generation_hidden_states stats\")\n",
    "        print(\" min\", aligned_generation_hidden_states.min().item(), \n",
    "              \"max\", aligned_generation_hidden_states.max().item(), \n",
    "              \"has inf?\", torch.isinf(aligned_generation_hidden_states).any().item(),\n",
    "            \"has nan?\", torch.isnan(aligned_generation_hidden_states).any().item())\n",
    "        print(\"=======================================\")\n",
    "\n",
    "\n",
    "        # ===============================\n",
    "        # Build generation and prompt attention mask\n",
    "        # ===============================\n",
    "        # This mask marks which generated tokens are valid (i.e., not padding).\n",
    "        # Positions are marked True up to and including the first eos_token_id\n",
    "        generation_attention_mask = build_generation_attention_mask(\n",
    "            gen_ids=truncated_gen_sequences, \n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        ) # Shape (batch_size, gen_len)\n",
    "\n",
    "        # Prompt attention mask\n",
    "        prompt_attention_mask = inputs[\"attention_mask\"] \n",
    "        # Shape (batch_size, prompt_len)\n",
    "        \n",
    "        # ===============================\n",
    "        # Modify prompt attention mask with offsets\n",
    "        # ===============================\n",
    "        if start_offset !=0 or end_offset !=0:\n",
    "            prompt_attention_mask, start_indices, end_indices = compute_offset_attention_mask(\n",
    "                attention_mask=prompt_attention_mask, \n",
    "                start_offset=start_offset, \n",
    "                end_offset=end_offset\n",
    "            ) # Shape (batch_size, prompt_len), (batch_size,), (batch_size,)\n",
    "\n",
    "        # Concatenate the prompt and generation attention mask\n",
    "        prompt_and_gen_attention_mask = torch.cat(\n",
    "            [prompt_attention_mask,\n",
    "            generation_attention_mask],\n",
    "            dim=1\n",
    "        ) # Shape (batch_size, prompt_len + gen_len)\n",
    "\n",
    "        # ==============================\n",
    "        # Extract token activations from captured layer, based on source\n",
    "        # ==============================\n",
    "        if activation_source == \"generation\":\n",
    "            # Return only the token activations from the generated answer \n",
    "            selected_token_vecs = extract_token_activations_fn(\n",
    "                    selected_layer=aligned_generation_hidden_states, \n",
    "                    attention_mask=generation_attention_mask, \n",
    "                    device=aligned_generation_hidden_states.device,\n",
    "                ) # Shape (batch_size, hidden_size)\n",
    "            \n",
    "        elif activation_source == \"prompt\":    \n",
    "            # Return only the token activations from the prompt\n",
    "            selected_token_vecs = extract_token_activations_fn(\n",
    "                    selected_layer=aligned_prompt_hidden_states, \n",
    "                    attention_mask=prompt_attention_mask, \n",
    "                    device=aligned_prompt_hidden_states.device,\n",
    "                ) # Shape (batch_size, hidden_size)\n",
    "            \n",
    "        elif activation_source == \"promptGeneration\":\n",
    "            # Return token activations from the concatenated prompt + generated answer \n",
    "            selected_token_vecs = extract_token_activations_fn(\n",
    "                    selected_layer=aligned_prompt_and_gen_hidden_states, \n",
    "                    attention_mask=prompt_and_gen_attention_mask, \n",
    "                    device=aligned_prompt_and_gen_hidden_states.device,\n",
    "                    skip_length=prompt_len \n",
    "                    # skip_length: exclude prompt from computation if \n",
    "                    # mode=='first_generated' in `extract_token_activations_fn`\n",
    "                ) # Shape (batch_size, hidden_size)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Invalid value for `activation_source`: '{activation_source}'. \"\n",
    "                f\"Expected one of: ['prompt', 'generation', 'promptGeneration'].\"\n",
    "            )    \n",
    "        \n",
    "        print(\"selected_token_vecs:\", selected_token_vecs)\n",
    "        # ==============================\n",
    "        # Store results (to file or memory)\n",
    "        # ==============================\n",
    "        activations = {}\n",
    "        for mode, tensor in selected_token_vecs.items():\n",
    "            activations[mode] = [tensor[j].unsqueeze(0).cpu().numpy() for j in range(tensor.size(0))]\n",
    "\n",
    "        batch_dataset_ids = []; batch_dataset_original_idx = []; batch_context = []\n",
    "        batch_question = []; batch_gt_answers = []; batch_title = []\n",
    "        for s in batch:\n",
    "            batch_dataset_ids.append(s['id'])\n",
    "            batch_dataset_original_idx.append(s['original_index'])\n",
    "            batch_context.append(s['context'])\n",
    "            batch_question.append(s['question'])\n",
    "            batch_gt_answers.append(s['answers'])\n",
    "            batch_title.append(s['title'])\n",
    "        \n",
    "        batch_results = {\n",
    "            \"id\": batch_dataset_ids,\n",
    "            \"original_indices\": batch_dataset_original_idx,\n",
    "            \"activations\": activations, # Dict\n",
    "            \"gen_answers\": gen_answers,\n",
    "            \"gt_answers\": batch_gt_answers,\n",
    "            \"context\": batch_context,\n",
    "            \"question\": batch_question,\n",
    "            \"title\": batch_title,\n",
    "        }\n",
    "\n",
    "        if save_to_pkl:\n",
    "            #append_to_pickle(output_path, batch_results)\n",
    "            #save_batch_pickle(batch_data=batch_results, output_dir=output_path, batch_idx=i)\n",
    "            pass\n",
    "        else:\n",
    "            for mode, acts in activations.items():\n",
    "                if mode not in batch_activations:\n",
    "                    batch_activations[mode] = []\n",
    "                if isinstance(acts, list):\n",
    "                    batch_activations[mode].extend(acts)\n",
    "                else:\n",
    "                    batch_activations[mode].extend([a for a in acts])\n",
    "\n",
    "            #batch_activations.extend(activations)\n",
    "        \n",
    "    if not save_to_pkl:\n",
    "        return batch_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "def load_llama(model_name: str = \"meta-llama/Llama-2-7b-chat-hf\"):\n",
    "    model_name = model_name  # fine-tuned version of LLaMA for conversational uses\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')\n",
    "    tokenizer.model_max_length = 1024  # LLaMA-2’s max length tokens is 4096\n",
    "    \n",
    "    # For llama, pad_token is not defined by default:\n",
    "    # The convention is to use tokenizer.pad_token = tokenizer.eos_token \n",
    "    # However, this causes an issue when outputting attention maps during model.generate()\n",
    "    if tokenizer.pad_token is None:\n",
    "        # add \"<pad>\" to vocab as a special token\n",
    "        tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
    "        print(\"<pad> token not defined by default, add it to vocabulary.\")\n",
    "       \n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16, \n",
    "            device_map= \"auto\",          # load model to device \n",
    "            low_cpu_mem_usage=True,     # reduce RAM usage during loading\n",
    "            attn_implementation=\"eager\",\n",
    "            #output_hidden_states=True, # to hidden activations -> memory overload since we access ALL hidden states \n",
    "            #force_download=True        # redo complete download \n",
    "        )\n",
    "    \n",
    "        # required for the model to accept the new vocabulary.\n",
    "        model.resize_token_embeddings(len(tokenizer)) \n",
    "\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16, \n",
    "            device_map=\"auto\",          # load model to device \n",
    "            low_cpu_mem_usage=True,     # reduce RAM usage during loading\n",
    "        )\n",
    "\n",
    "    # Ensures that during generation all sequences are aligned with the PAD token, and not with random tokens. \n",
    "    model.config.pad_token_id = tokenizer.pad_token_id \n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "def load_llama(model_name: str = \"meta-llama/Llama-2-7b-chat-hf\"):\n",
    "    model_name = model_name  # fine-tuned version of LLaMA for conversational uses\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')\n",
    "    tokenizer.model_max_length = 1024  # LLaMA-2’s max length tokens is 4096\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # pad_token not defined by default: reuse the EOS token (</s>) as the padding token.\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16, \n",
    "        device_map=\"cuda:0\", #,\"auto\",          # load model to device \n",
    "        low_cpu_mem_usage=True,     # reduce RAM usage during loading\n",
    "        attn_implementation=\"eager\", # \"\" \"flex_attention\"\n",
    "        #output_hidden_states=True, # to hidden activations -> memory overload since we access ALL hidden states \n",
    "        #force_download=True        # redo complete download \n",
    "    )\n",
    "    print(\"attn_implementation changed\")\n",
    "    model.config.pad_token_id = model.config.eos_token_id # ensures that during generation all sequences are aligned with the EOS token, and not with random tokens. \n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for layer in model.model.layers:\n",
    "    layer.self_attn.forward = patched_LlamaAttention_forward.__get__(layer.self_attn, layer.self_attn.__class__)\n",
    "\n",
    "'''\n",
    "attention_module = model.model.layers[-1].self_attn\n",
    "print(\"attention_module._attn_implementation:\", attention_module.config._attn_implementation)\n",
    "\n",
    "# attention_module.type <class 'transformers.models.llama.modeling_llama.LlamaAttention'>\n",
    "\n",
    "for layer in model.model.layers:\n",
    "    layer.self_attn._attn_implementation = \"eager\"\n",
    "\n",
    "model.model.layers[-1].self_attn\n",
    "\n",
    "\n",
    "\n",
    "'''from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "model_name = \"gpt2\"  # ou \"gpt2-medium\", \"gpt2-large\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# GPT-2 a un pad_token natif\n",
    "tokenizer.pad_token = tokenizer.eos_token'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n",
    "        attention_mask: Optional[torch.Tensor],\n",
    "        past_key_value: Optional[Cache] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        **kwargs: Unpack[FlashAttentionKwargs],\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "        \n",
    "        print(\"=== DEBUG ATTENTION FORWARD ===\")\n",
    "    \n",
    "        # 1. Vérifier les inputs\n",
    "\n",
    "        print(f\"   INPUT hidden_states:\")\n",
    "        print(f\"   Shape: {hidden_states.shape}\")\n",
    "        print(f\"   Dtype: {hidden_states.dtype}\")\n",
    "        print(f\"   Has NaN: {torch.isnan(hidden_states).any()}\")\n",
    "        print(f\"   Has Inf: {torch.isinf(hidden_states).any()}\")\n",
    "        print(f\"   Range: [{hidden_states.min():.4f}, {hidden_states.max():.4f}]\")\n",
    "        num_nans = torch.isnan(hidden_states).sum().item()\n",
    "        print(f\"Nombre de NaNs dans hidden_states : {num_nans}\")\n",
    "        # Masque des NaNs (True là où c'est NaN)\n",
    "        nan_mask = torch.isnan(hidden_states)\n",
    "        # Affichage d'exemples de positions NaN\n",
    "        print(\"Positions des premiers NaNs :\", nan_mask.nonzero(as_tuple=False)[:10])\n",
    "        # Nombre de NaNs par dimension (par exemple par token ou par batch)\n",
    "        nans_par_token = nan_mask.view(hidden_states.size(0), -1).sum(dim=1)\n",
    "        print(\"Nombre de NaNs par exemple du batch :\", nans_par_token)\n",
    "        if num_nans > 0:\n",
    "            return  \n",
    "\n",
    "         # 2. Vérifier les poids des projections\n",
    "        print(f\"\\n  PROJECTION WEIGHTS:\")\n",
    "        for name, param in [(\"q_proj\", self.q_proj.weight), (\"k_proj\", self.k_proj.weight), (\"v_proj\", self.v_proj.weight)]:\n",
    "            print(f\"   {name}.weight:\")\n",
    "            print(f\"     Has NaN: {torch.isnan(param).any()}\")\n",
    "            print(f\"     Has Inf: {torch.isinf(param).any()}\")\n",
    "            print(f\"     Range: [{param.min():.4f}, {param.max():.4f}]\")\n",
    "            print(f\"     Norm: {param.norm().item():.4f}\")\n",
    "        # 3. Vérifier les biais (s'ils existent)\n",
    "        for name, proj in [(\"q_proj\", self.q_proj), (\"k_proj\", self.k_proj), (\"v_proj\", self.v_proj)]:\n",
    "            if hasattr(proj, 'bias') and proj.bias is not None:\n",
    "                print(f\"   {name}.bias:\")\n",
    "                print(f\"     Has NaN: {torch.isnan(proj.bias).any()}\")\n",
    "                print(f\"     Has Inf: {torch.isinf(proj.bias).any()}\")\n",
    "                print(f\"     Range: [{proj.bias.min():.4f}, {proj.bias.max():.4f}]\")\n",
    "\n",
    "        input_shape = hidden_states.shape[:-1]\n",
    "        hidden_shape = (*input_shape, -1, self.head_dim)\n",
    "\n",
    "        print(f\"\\n PROJECTIONS:\")\n",
    "\n",
    "        # Q projection\n",
    "        print(f\"   Computing Q projection...\")\n",
    "        q_raw = self.q_proj(hidden_states)\n",
    "        print(f\"   Q raw - Has NaN: {torch.isnan(q_raw).any()}, Range: [{q_raw.min():.4f}, {q_raw.max():.4f}]\")\n",
    "        \n",
    "        query_states = q_raw.view(hidden_shape).transpose(1, 2)\n",
    "        print(f\"   Q reshaped - Has NaN: {torch.isnan(query_states).any()}\")\n",
    "        \n",
    "        # K projection\n",
    "        print(f\"   Computing K projection...\")\n",
    "        k_raw = self.k_proj(hidden_states)\n",
    "        print(f\"   K raw - Has NaN: {torch.isnan(k_raw).any()}, Range: [{k_raw.min():.4f}, {k_raw.max():.4f}]\")\n",
    "        \n",
    "        key_states = k_raw.view(hidden_shape).transpose(1, 2)\n",
    "        print(f\"   K reshaped - Has NaN: {torch.isnan(key_states).any()}\")\n",
    "        \n",
    "        # V projection\n",
    "        print(f\"   Computing V projection...\")\n",
    "        v_raw = self.v_proj(hidden_states)\n",
    "        print(f\"   V raw - Has NaN: {torch.isnan(v_raw).any()}, Range: [{v_raw.min():.4f}, {v_raw.max():.4f}]\")\n",
    "        \n",
    "        value_states = v_raw.view(hidden_shape).transpose(1, 2)\n",
    "        print(f\"   V reshaped - Has NaN: {torch.isnan(value_states).any()}\")\n",
    "        \n",
    "\n",
    "        \"\"\"\n",
    "        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"Q max\", query_states.max(), \"min\", query_states.min())\n",
    "        print(\"K max\", key_states.max(), \"min\", key_states.min())\n",
    "\n",
    "        cos, sin = position_embeddings\n",
    "\n",
    "        print(f\"\\n  POSITION EMBEDDINGS:\")\n",
    "        print(f\"   cos - Has NaN: {torch.isnan(cos).any()}, Range: [{cos.min():.4f}, {cos.max():.4f}]\")\n",
    "        print(f\"   sin - Has NaN: {torch.isnan(sin).any()}, Range: [{sin.min():.4f}, {sin.max():.4f}]\")\n",
    "\n",
    "        print(f\"\\n  APPLYING RoPE...\")\n",
    "        query_states_before_rope = query_states.clone() ######\n",
    "        key_states_before_rope = key_states.clone() #######\n",
    "\n",
    "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "\n",
    "        print(f\"   Q after RoPE - Has NaN: {torch.isnan(query_states).any()}\")\n",
    "        print(f\"   K after RoPE - Has NaN: {torch.isnan(key_states).any()}\")\n",
    "        \n",
    "        if torch.isnan(query_states).any() or torch.isnan(key_states).any():\n",
    "            print(\"  NaN detectes après RoPE!\")\n",
    "            print(f\"   Q before RoPE had NaN: {torch.isnan(query_states_before_rope).any()}\")\n",
    "            print(f\"   K before RoPE had NaN: {torch.isnan(key_states_before_rope).any()}\")\n",
    "        \n",
    "\n",
    "        if past_key_value is not None:\n",
    "            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n",
    "            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n",
    "            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n",
    "\n",
    "        attention_interface: Callable = eager_attention_forward\n",
    "        if self.config._attn_implementation != \"eager\":\n",
    "            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n",
    "                logger.warning_once(\n",
    "                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n",
    "                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n",
    "                )\n",
    "            else:  \n",
    "                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n",
    "        \n",
    "        print(\"attention_interface:\", attention_interface)\n",
    "        print(\"attention_mask:\", attention_mask)\n",
    "\n",
    "        attn_output, attn_weights = attention_interface(\n",
    "            self,\n",
    "            query_states,\n",
    "            key_states,\n",
    "            value_states,\n",
    "            attention_mask,\n",
    "            dropout=0.0 if not self.training else self.attention_dropout,\n",
    "            scaling=self.scaling,\n",
    "            **kwargs,\n",
    "        )\n",
    "        \n",
    "        #print(\"attention weights dtype:\", attn_weights.dtype)\n",
    "\n",
    "        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "        return attn_output, attn_weights\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n",
    "        attention_mask: Optional[torch.Tensor],\n",
    "        past_key_value: Optional[Cache] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        **kwargs: Unpack[FlashAttentionKwargs],\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "        input_shape = hidden_states.shape[:-1]\n",
    "        hidden_shape = (*input_shape, -1, self.head_dim)\n",
    "\n",
    "        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "\n",
    "        print(\"Q max\", query_states.max(), \"min\", query_states.min())\n",
    "        print(\"K max\", key_states.max(), \"min\", key_states.min())\n",
    "\n",
    "        cos, sin = position_embeddings\n",
    "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "\n",
    "        if past_key_value is not None:\n",
    "            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n",
    "            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n",
    "            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n",
    "\n",
    "        attention_interface: Callable = eager_attention_forward\n",
    "        \n",
    "        if self.config._attn_implementation != \"eager\":\n",
    "            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n",
    "                logger.warning_once(\n",
    "                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n",
    "                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n",
    "                )\n",
    "            else:  \n",
    "                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n",
    "        \n",
    "        print(\"attention_interface:\", attention_interface)\n",
    "        print(\"attention_mask:\", attention_mask)\n",
    "\n",
    "        attn_output, attn_weights = attention_interface(\n",
    "            self,\n",
    "            query_states,\n",
    "            key_states,\n",
    "            value_states,\n",
    "            attention_mask,\n",
    "            dropout=0.0 if not self.training else self.attention_dropout,\n",
    "            scaling=self.scaling,\n",
    "            **kwargs,\n",
    "        )\n",
    "        \n",
    "        print(\"attention weights dtype:\", attn_weights.dtype)\n",
    "\n",
    "        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "        return attn_output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.llama.modeling_llama import LlamaAttention, eager_attention_forward, sdpa_attention_forward\n",
    "\n",
    "def patched_forward(\n",
    "    self,\n",
    "    hidden_states,\n",
    "    position_embeddings,\n",
    "    attention_mask,\n",
    "    past_key_value=None,\n",
    "    cache_position=None,\n",
    "    **kwargs,\n",
    "):\n",
    "    # Codes classiques (projections etc.)\n",
    "    input_shape = hidden_states.shape[:-1]\n",
    "    hidden_shape = (*input_shape, -1, self.head_dim)\n",
    "\n",
    "    query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "    key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "    value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "\n",
    "    cos, sin = position_embeddings\n",
    "    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "\n",
    "    if past_key_value is not None:\n",
    "        cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n",
    "        key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n",
    "\n",
    "    # --- 1. Passe principale avec SDPA ---\n",
    "    attn_output, _ = sdpa_attention_forward(\n",
    "        self,\n",
    "        query_states,\n",
    "        key_states,\n",
    "        value_states,\n",
    "        attention_mask,\n",
    "        dropout=0.0 if not self.training else self.attention_dropout,\n",
    "        scaling=self.scaling,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    # --- 2. Calcul des attn_weights en EAGER (mais pas utilisés, juste pour logging) ---\n",
    "    # À ce stade, attention: le calcul de attn_weights peut planter si instable ou NaN\n",
    "    try:\n",
    "        _, attn_weights = eager_attention_forward(\n",
    "            self,\n",
    "            query_states, key_states, value_states, attention_mask,\n",
    "            dropout=0.0 if not self.training else self.attention_dropout,\n",
    "            scaling=self.scaling, **kwargs,\n",
    "        )\n",
    "    except Exception as ex:\n",
    "        print(f\"!!! Eager NaN/inf during attn_weights calculation: {ex}\")\n",
    "        attn_weights = None\n",
    "\n",
    "    attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n",
    "    attn_output = self.o_proj(attn_output)\n",
    "    return attn_output, attn_weights  # NB: seule la sortie SDPA est utilisée downstream !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n",
    "import torch\n",
    "import torch\n",
    "import sys\n",
    "import os \n",
    "# Add the path to the src directory\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "set_seed(1234)\n",
    "\n",
    "from src.model_loader.llama_loader import load_llama\n",
    "model, tok = load_llama(MODEL_NAME)\n",
    "model.eval()\n",
    "\n",
    "encoding = tok([\"Hi there, how are you?\"], return_tensors=\"pt\").to(\"cuda:0\")\n",
    "with torch.no_grad():\n",
    "    generation_output = model.generate(**encoding, return_dict_in_generate=True, output_logits=True)\n",
    "\n",
    "sequences = generation_output.sequences\n",
    "sanity_check_logits = generation_output.logits\n",
    "\n",
    "with torch.no_grad():\n",
    "    model_output = model(input_ids=encoding['input_ids'])\n",
    "\n",
    "# Vérification des logits\n",
    "prompt_len = encoding['input_ids'].shape[1]\n",
    "total_len = sequences.shape[1]\n",
    "generated_len = total_len - prompt_len\n",
    "\n",
    "print(f\"Longueur du prompt: {prompt_len}\")\n",
    "print(f\"Longueur totale des sequences: {total_len}\")\n",
    "print(f\"Longueur générée: {generated_len}\")\n",
    "print(f\"Nombre de logits générés: {len(generation_output.logits)}\")\n",
    "\n",
    "# Vérification que sequences = prompt + génération\n",
    "print(f\"\\nPrompt original: {encoding['input_ids']}\")\n",
    "print(f\"Séquence complète: {sequences}\")\n",
    "print(f\"Partie prompt de sequences: {sequences[0, :prompt_len]}\")\n",
    "print(f\"Partie générée de sequences: {sequences[0, prompt_len:]}\")\n",
    "print(f\"Prompt == partie prompt ? {torch.equal(encoding['input_ids'][0], sequences[0, :prompt_len])}\")\n",
    "\n",
    "# Le dernier logit du prompt (à la position prompt_len-1) prédit le premier token généré\n",
    "last_prompt_logit = model_output.logits[:, -1, :].float()  # Dernier logit du prompt\n",
    "\n",
    "# Le premier logit de génération correspond au premier token généré\n",
    "first_gen_logit = generation_output.logits[0].float()  # Premier logit de la génération\n",
    "\n",
    "# Comparaison\n",
    "diff = torch.max(torch.abs(first_gen_logit - last_prompt_logit)).cpu().item()\n",
    "are_close = torch.allclose(first_gen_logit, last_prompt_logit, rtol=1e-5, atol=1e-8)\n",
    "\n",
    "print(f\"\\nLes logits sont-ils identiques ? {are_close}\")\n",
    "print(f\"Différence maximale: {diff:.10f}\")\n",
    "\n",
    "# Affichage pour debug\n",
    "print(f\"\\nShape du dernier logit du prompt: {last_prompt_logit.shape}\")\n",
    "print(f\"Shape du premier logit généré: {first_gen_logit.shape}\")\n",
    "\n",
    "# Vérification que le premier token généré correspond bien\n",
    "first_generated_token_id = sequences[0, prompt_len]  # Premier token après le prompt\n",
    "predicted_token_id = torch.argmax(last_prompt_logit, dim=-1)\n",
    "print(f\"\\nPremier token généré (ID): {first_generated_token_id}\")\n",
    "print(f\"Token prédit par le dernier logit du prompt (ID): {predicted_token_id.item()}\")\n",
    "print(f\"Les tokens correspondent-ils ? {first_generated_token_id == predicted_token_id.item()}\")\n",
    "\n",
    "# Décodage pour visualisation\n",
    "print(f\"\\nPremier token généré: '{tok.decode(first_generated_token_id)}'\")\n",
    "print(f\"Token prédit: '{tok.decode(predicted_token_id)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def compute_perplexity(\n",
    "        prompt_logits: torch.Tensor, \n",
    "        gen_logits: torch.Tensor,\n",
    "        prompt_input_ids: torch.Tensor, \n",
    "        gen_input_ids: torch.Tensor,\n",
    "        prompt_attention_mask: torch.Tensor,\n",
    "        gen_attention_mask: torch.Tensor,\n",
    "        mode: Literal[\"prompt\", \"generation\", \"promptGeneration\"] = \"prompt\",\n",
    "        min_k: float = None\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Computes the per-sample perplexity of language model outputs using logits \n",
    "    and corresponding input token IDs. Logits maked by 0 in the attention mask \n",
    "    are ignored in the computation of the perplexity. \n",
    "\n",
    "    Perplexity is defined as:\n",
    "        Perplexity = exp(- mean(log P(token_i | context)))\n",
    "\n",
    "    NOTE: This implementation is inspired by:\n",
    "    \"LLM-Check: Investigating Detection of Hallucinations in Large Language Models\"\n",
    "    (Sriramanan et al., 2024)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    prompt_logits : torch.Tensor\n",
    "        Tensor of shape (batch_size, prompt_len, vocab_size) \n",
    "        These are the model's output logits obtained from a standard forward pass over the prompt sequence.\n",
    "    gen_logits : torch.Tensor\n",
    "        Tensor of shape (batch_size, gen_len, vocab_size).\n",
    "        These are the logits obtained during autoregressive decoding using `model.generate()`.\n",
    "    prompt_input_ids : torch.Tensor\n",
    "        Tensor of shape (batch_size, prompt_len), containing the input token IDs for the prompt.\n",
    "    gen_input_ids : torch.Tensor\n",
    "        Tensor of shape (batch_size, gen_len), containing the token IDs generated by the model.\n",
    "    prompt_attention_mask: torch.Tensor\n",
    "        Tensor of shape (batch_size, Tensor), 1 where token valid, 0 for padding.\n",
    "    gen_attention_mask: torch.Tensor  \n",
    "        Tensor of shape (batch_size, gen_len), 1 where token valid, 0 for padding.\n",
    "    mode : str, optional\n",
    "        One of {\"prompt\", \"generation\", \"promptGeneration\"}:\n",
    "        - \"prompt\": compute perplexity only over the prompt.\n",
    "        - \"generation\": compute perplexity only over the generated tokens.\n",
    "        - \"promptGeneration\": compute perplexity over both prompt and generation.\n",
    "    min_k : float, optional\n",
    "        Optional value between 0 and 1. If specified, only the bottom-k lowest-probability\n",
    "        tokens are used for perplexity calculation.\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "        np.ndarray: Per-sample perplexity scores of shape (batch_size,)\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    About token shifting in autoregressive models:\n",
    "\n",
    "    In a standard autoregressive forward pass:\n",
    "        - At step *t*, the model predicts the token at position *t* based on the tokens up to *t-1*.\n",
    "        - Thus, the logit at position *t* predicts the token at position *t+1*.\n",
    "        - The first token has no preceding context and is not predicted.\n",
    "        - When computing log-probabilities, we must **shift the targets one position to the left** \n",
    "        to correctly align logits with target tokens.\n",
    "        \n",
    "        Example: Suppose we have a sequence of tokens (with their token IDs):\n",
    "        | Index | Token | ID  |\n",
    "        |-------|-------|-----| - The model produces logits at positions 0, 1, \n",
    "        | 0     | A     | 10  | and 2 to predict the tokens B, C, and D, respectively.\n",
    "        | 1     | B     | 29  |\n",
    "        | 2     | C     | 305 |  - The logits at position 0 are used to predict\n",
    "        | 3     | D     | 24  |  token B (ID 29).\n",
    "\n",
    "    During generation (e.g., using model.generate()):\n",
    "        - The logit at time step *t* predicts the token generated at position *t*.\n",
    "        - Each logit already corresponds to the prediction of the token at this step \n",
    "        - No shifting is needed in this case.\n",
    "\n",
    "    Summary of alignment:\n",
    "        - Prompt: logit at position *t* predicts token at position *t+1* -> shift targets left.\n",
    "        - Generation: logit at position *t* predicts token at position *t* -> no shift.\n",
    "\n",
    "    NOTE: help from issue https://github.com/huggingface/transformers/issues/29664\n",
    "    \"\"\"\n",
    "\n",
    "    softmax = torch.nn.Softmax(dim=-1)\n",
    "\n",
    "    # Apply softmax over vocabulary dimension and take log to get log-probabilities\n",
    "    prompt_log_probs = torch.log(softmax(prompt_logits))  # shape: (batch_size, prompt_len, vocab_size)\n",
    "    gen_log_probs = torch.log(softmax(gen_logits))        # shape: (batch_size, gen_len, vocab_size)\n",
    "\n",
    "    if mode in (\"prompt\", \"promptGeneration\"):\n",
    "        # In prompt: logit at position t predicts token at t+1 (requires shifting)\n",
    "        # Remove first token from target (no context to predict it)\n",
    "        prompt_target_tokens = prompt_input_ids[:, 1:] # (batch_size, prompt_len - 1)\n",
    "        \n",
    "        # Remove last logit position (since it predicts next token)\n",
    "        prompt_pred_log_probs = prompt_log_probs[:, :-1, :] # shape: (batch_size, prompt_len - 1, vocab_size)\n",
    "        \n",
    "        # Retrieves, for each position and each batch, the log-probability corresponding to the next token \n",
    "        # (the one in target_tokens) from all the probas on the vocabulary.\n",
    "        prompt_token_log_probs = prompt_pred_log_probs.gather(\n",
    "            dim=2, index=prompt_target_tokens.unsqueeze(-1)\n",
    "            ).squeeze(-1) # shape: (batch_size, prompt_len - 1)\n",
    "    \n",
    "    if mode in (\"prompt\", \"promptGeneration\"):\n",
    "        # In generation: logit at position t predicts token at position t (no shift needed)\n",
    "        gen_token_log_probs = gen_log_probs.gather(\n",
    "            dim=2, index=gen_input_ids.unsqueeze(-1)\n",
    "            ).squeeze(-1)  # shape: (batch_size, gen_len)\n",
    "        \n",
    "    if mode == \"promptGeneration\":\n",
    "        # Last logit of prompt from the forward pass == first logit of generation from `model.generate()`. \n",
    "        # To compute perplexity over the full sequence:\n",
    "        # - Use prompt_token_log_probs (excluding final prompt token)\n",
    "        # - Use gen_token_log_probs from generation\n",
    "        # Concatenate both to form a complete sequence of predicted log-probs\n",
    "        prompt_gen_token_log_probs = torch.cat(\n",
    "            [prompt_token_log_probs, gen_token_log_probs], dim=1\n",
    "        )  # shape: (batch_size, prompt_len - 1 + gen_len)\n",
    "\n",
    "    # Select the appropriate token log-probabilities based on mode\n",
    "    if mode == \"prompt\":\n",
    "        token_log_probs = prompt_token_log_probs  # (batch_size, prompt_len - 1)\n",
    "    elif mode == \"generation\":\n",
    "        token_log_probs = gen_token_log_probs # (batch_size, gen_len)\n",
    "    elif mode == \"promptGeneration\":\n",
    "        token_log_probs = prompt_gen_token_log_probs # (batch_size, prompt_len - 1 + gen_len)\n",
    "\n",
    "    # Optionally focus only on the k% hardest tokens (lowest log-probs)\n",
    "    if min_k is not None:\n",
    "        # Keep only the min_k fraction of tokens with the lowest log-probs \n",
    "        k = int(min_k * token_log_probs.size(1))  # number of tokens to keep per sample\n",
    "        \n",
    "        # Use topk with largest=False to get the k tokens with the lowest log-probabilities\n",
    "        topk_vals, _ = torch.topk(token_log_probs, k=k, dim=1, largest=False)\n",
    "\n",
    "        # Compute perplexity using only the selected subset\n",
    "        ppls = torch.exp(-topk_vals.mean(dim=1))\n",
    "    else:\n",
    "        # Compute perplexity over all predicted tokens\n",
    "        ppls = torch.exp(-token_log_probs.mean(dim=1))\n",
    "\n",
    "    return ppls.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def compute_attn_eig_prod(\n",
    "    prompt_attentions: torch.Tensor,\n",
    "    generation_attentions: List[torch.Tensor],\n",
    "    attentions: List[torch.Tensor], \n",
    "    mode: Literal[\"prompt\", \"generation\", \"promptGeneration\"] = \"promptGeneration\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute a mean log-diagonal attention score (eigenvalue-inspired) for a single layer's attention map.\n",
    "    NOTE: Implementation inspired from \n",
    "    \"LLM-Check: Investigating Detection of Hallucinations in Large Language Models\"\n",
    "    (Sriramanan et al. 2024)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    attentions : list of torch.Tensor: [attn_prompt, attn_gen1, attn_gen2, ...]\n",
    "        - attentions[0]: Tensor of shape (batch_size, n_heads, prompt_len, prompt_len)\n",
    "            Self-attention over the prompt tokens. \n",
    "        - attentions[1:]: List of tensors of shape(batch_size, n_heads, 1, prompt_len + t)\n",
    "            Self-attention for each generated token at generation step t (t >= 1).\n",
    "     mode : str, optional\n",
    "        Specifies which part of the attention map to use for the score computation.\n",
    "        Must be one of the following:\n",
    "        - \"prompt\":\n",
    "            Only uses the prompt self-attention matrix (attentions[0]).\n",
    "            The diagonal (i.e., self-attention values per token) is extracted,\n",
    "            then the log is taken, followed by a mean over prompt tokens and sum over heads.\n",
    "        - \"generation\":\n",
    "            Only uses the generated self-attention maps (attentions[1:]).\n",
    "            Each tensor in attentions[1:] has shape (batch_size, n_heads, 1, prompt_len + t),\n",
    "            where t is the generation step. \n",
    "            Intead of concatenating these tensors to obtain the generation attention matrix, \n",
    "            for each step, we directly take the last value along the last axis (i.e., the self-attention\n",
    "            of the newly generated token). These values are stacked across time steps, then we take the log,\n",
    "            compute the mean over time, and sum over heads.\n",
    "        - \"prompt+generation\":\n",
    "            Combines the diagonals from both the prompt and generation attention maps as described above\n",
    "            for \"prompt\" and \"generation\" mode. The two diagonals are concatenated along the token/time axis, \n",
    "            then the log is taken, followed by a mean across all tokens and a sum over heads.\n",
    "            Note: we do **not** concatenate the full prompt and generation attention matrices,\n",
    "            since the diagonal of the combined matrix would only include values from the prompt attention\n",
    "            due to mismatched matrix shapes.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        A NumPy array of shape (batch_size,), where each value is the per-sample attention score.\n",
    "        The score is summed across heads and averaged across tokens (in log-space).\n",
    "    \"\"\"\n",
    "    assert mode in (\"prompt\", \"generation\", \"promptGeneration\"), \"Invalid mode.\"\n",
    "    \n",
    "    prompt_attentions = attentions[0]\n",
    "    gen_attentions = attentions[1:]\n",
    "\n",
    "    batch_size, n_heads = prompt_attentions.shape[:2]\n",
    "    n_generated = len(attentions) - 1\n",
    "\n",
    "    diag_blocks = []\n",
    "\n",
    "    if mode in (\"prompt\", \"promptGeneration\"):\n",
    "        prompt_diag = torch.diagonal(prompt_attentions, dim1=-2, dim2=-1) # (batch_size, n_heads, prompt_len)\n",
    "        diag_blocks.append(prompt_diag)\n",
    "        print(\"prompt_diag.shape\", prompt_diag.shape)\n",
    "\n",
    "    if mode in (\"generation\", \"promptGeneration\") and n_generated > 0:\n",
    "        # For each generation step, take the (batch_size, n_heads, 1, prompt_len + t)\n",
    "        # => keep last value of last axis (self-attention of generated token)\n",
    "        gen_diag_steps = [attn[..., -1].squeeze(-1) for attn in gen_attentions]  # list of (batch_size, n_heads)\n",
    "        gen_debug = [attn[..., -1] for attn in gen_attentions] \n",
    "        print(\"len(gen_diag_steps): \", len(gen_diag_steps))\n",
    "        print(\"gen_diag_steps[0].shape:\", gen_diag_steps[0].shape)\n",
    "        print(\"gen_debug[0].shape:\", gen_debug[0].shape)\n",
    "        # Stack along newly generated tokens\n",
    "        gen_diag = torch.stack(gen_diag_steps, dim=-1) if gen_diag_steps else None # (batch_size, n_heads, n_generated)\n",
    "        if gen_diag is not None:\n",
    "            diag_blocks.append(gen_diag)\n",
    "            print(\"gen_diag.shape: \", gen_diag.shape)\n",
    "\n",
    "    # Now concatenate along token axis\n",
    "    all_diags = torch.cat(diag_blocks, dim=-1) # (batch_size, n_heads, N) where N = prompt_len + n_generated (or a subset)\n",
    "    print(\"all_diags.shape:\", all_diags.shape)\n",
    "\n",
    "    # Take log, mean over N tokens, sum over heads\n",
    "    all_diags = all_diags.clamp(min=1e-6)\n",
    "    log_diag = torch.log(all_diags).mean(dim=-1) # (batch_size, n_heads)\n",
    "    print(\"log_diag.shape: \", log_diag.shape)\n",
    "    scores = log_diag.sum(dim=-1).cpu().numpy()  # sum over n_heads\n",
    "    print(\"scores.shape: \", scores.shape)\n",
    "\n",
    "    return scores # (batch_size,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def compute_logit_entropy(\n",
    "    prompt_logits: torch.Tensor,\n",
    "    gen_logits: torch.Tensor,\n",
    "    prompt_attention_mask: torch.Tensor,\n",
    "    gen_attention_mask: torch.Tensor,\n",
    "    mode: str = \"prompt\",\n",
    "    top_k: int = None\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the per-sample entropy of a language model's output distributions\n",
    "    using its logits and attention masks.\n",
    "    For each token position, the function computes the entropy of the softmax distribution\n",
    "    over the vocabulary. Entropy is averaged over the valid tokens (i.e., those marked\n",
    "    as 1 in the attention mask). If `top_k` is specified, the entropy is computed only\n",
    "    over the top-k logits (highest values) for each position.\n",
    "\n",
    "    Entropy is defined as:\n",
    "        Entropy = -Sum_i p_i * log(p_i)\n",
    "        where p_i = softmax(logits)_i\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    prompt_logits : torch.Tensor\n",
    "        Tensor of shape (batch_size, prompt_len, vocab_size).\n",
    "    gen_logits : torch.Tensor\n",
    "        Tensor of shape (batch_size, gen_len, vocab_size).\n",
    "    prompt_attention_mask : torch.Tensor\n",
    "        Tensor of shape (batch_size, prompt_len). Contains 1 where valid token, 0 for padding.\n",
    "    gen_attention_mask : torch.Tensor  \n",
    "        Tensor of shape (batch_size, gen_len). Contains 1 where valid token, 0 for padding.\n",
    "    mode : str, optional\n",
    "        One of {\"prompt\", \"generation\", \"promptGeneration\"}:\n",
    "        - \"prompt\": compute entropy only over the prompt tokens.\n",
    "        - \"generation\": compute entropy only over generated tokens.\n",
    "        - \"promptGeneration\": compute entropy over both prompt and generated tokens.\n",
    "    top_k : int, optional\n",
    "        If specified, compute entropy only over the top-k logits per token.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Per-sample entropy values, shape (batch_size,).\n",
    "    \"\"\"\n",
    "\n",
    "    def entropy_from_logits(logits, attention_mask, top_k=None):\n",
    "        print(f\"[DEBUG] Computing entropy from logits of shape {logits.shape}\")\n",
    "        print(f\"[DEBUG] Attention mask shape: {attention_mask.shape}\")\n",
    "\n",
    "        # Convert float12 -> float32 for better accuracy during computations\n",
    "        logits = logits.float()\n",
    "        attention_mask = attention_mask.float()\n",
    "\n",
    "        # AJOUT : Vérifier les logits d'entrée\n",
    "        print(f\"[DEBUG] Logits sample (batch0, first 5 tokens, first 10 vocab): {logits[0, :5, :10]}\")\n",
    "        print(f\"[DEBUG] Logits min/max: {logits.min()}, {logits.max()}\")\n",
    "        print(f\"[DEBUG] Attention mask sample (batch0, first 10): {attention_mask[0, :10]}\")\n",
    "\n",
    "        if top_k is not None:\n",
    "            topk_vals = torch.topk(logits, k=top_k, dim=-1).values  # (batch_size, seq_len, top_k)\n",
    "            print(f\"[DEBUG] Selected top_k={top_k} logits shape: {topk_vals.shape}\")\n",
    "            probs = F.softmax(topk_vals, dim=-1)  # (batch_size, seq_len, top_k)\n",
    "        else:\n",
    "            probs = F.softmax(logits, dim=-1)  # (batch_size, seq_len, vocab_size)\n",
    "            print(f\"[DEBUG] Softmax probabilities shape: {probs.shape}\")\n",
    "\n",
    "        # AJOUT : Vérifier les probabilités\n",
    "        print(f\"[DEBUG] Probs sample (batch0, first 5 tokens, first 10 vocab): {probs[0, :5, :10]}\")\n",
    "        print(f\"[DEBUG] Probs sum per token (should be ~1): {probs[0, :5].sum(dim=-1)}\")\n",
    "\n",
    "        epsilon = 1e-12  # Plus petit epsilon 1e-9\n",
    "        log_probs = torch.log(probs + epsilon)  # numerical stability\n",
    "        print(f\"[DEBUG] probs sample values (batch0, first 5 tokens): {probs[0, :5]}\")\n",
    "        print(f\"[DEBUG] log_probs sample values (batch0, first 5 tokens): {log_probs[0, :5]}\")\n",
    "        # AJOUT : Vérifier les log_probs\n",
    "        print(f\"[DEBUG] Log_probs sample (batch0, first 5 tokens, first 10 vocab): {log_probs[0, :5, :10]}\")\n",
    "        print(f\"[DEBUG] Log_probs min/max: {log_probs.min()}, {log_probs.max()}\")\n",
    "\n",
    "        product = (probs * log_probs)\n",
    "        print(f\"[DEBUG] product per token shape: {product.shape}\")\n",
    "        print(f\"HERE [DEBUG] product sample values (batch0, first 5 tokens): {product[0, :5]}\")\n",
    "        #entropy = -(probs* log_probs).sum(dim=-1)  # (batch_size, seq_len)\n",
    "\n",
    "        # Use torch.special.entr, which automatically handles edge cases.\n",
    "        # entropy(x) = -x * log(x) with entropy(0) = 0\n",
    "        entropy = torch.special.entr(probs).sum(dim=-1) # (batch_size, seq_len)\n",
    "\n",
    "        print(f\"[DEBUG] Entropy per token shape: {entropy.shape}\")\n",
    "        print(f\"HERE [DEBUG] Entropy sample values (batch0, first 5 tokens): {entropy[0, :5]}\")\n",
    "        # AJOUT : Vérifier l'entropie avant masquage\n",
    "        print(f\"[DEBUG] Entropy before masking (batch0, first 5): {entropy[0, :5]}\")\n",
    "        print(f\"[DEBUG] Entropy min/max: {entropy.min()}, {entropy.max()}\")\n",
    "        print(f\"[DEBUG] NaN count in entropy: {torch.isnan(entropy).sum()}\")\n",
    "    \n",
    "        entropy_masked = entropy * attention_mask  # Zero out padded tokens\n",
    "        print(f\"[DEBUG] entropy_masked per token shape: {entropy_masked.shape}\")\n",
    "        print(f\"[DEBUG] entropy_masked sample values (batch0, first 5 tokens): {entropy_masked[0, :5]}\")\n",
    "        total_entropy = entropy_masked.sum(dim=-1)  # sum over seq_len, (batch_size,)\n",
    "        valid_token_count = attention_mask.sum(dim=-1)  # (batch_size,)\n",
    "        print(f\"[DEBUG] Total entropy per sample: {total_entropy}\")\n",
    "        print(f\"[DEBUG] Valid token counts per sample: {valid_token_count}\")\n",
    "\n",
    "        return total_entropy.cpu().numpy(), valid_token_count.cpu().numpy()\n",
    "\n",
    "    if mode == \"prompt\":\n",
    "        total_entropy, count = entropy_from_logits(prompt_logits, prompt_attention_mask, top_k)\n",
    "        avg_entropy = total_entropy / (count + 1e-9)\n",
    "        print(f\"[INFO] Mode 'prompt': avg_entropy shape {avg_entropy.shape}\")\n",
    "        print(f\"[INFO] Sample avg_entropy: {avg_entropy}\")\n",
    "        return avg_entropy\n",
    "\n",
    "    elif mode == \"generation\":\n",
    "        total_entropy, count = entropy_from_logits(gen_logits, gen_attention_mask, top_k)\n",
    "        avg_entropy = total_entropy / (count + 1e-9)\n",
    "        print(f\"[INFO] Mode 'generation': avg_entropy shape {avg_entropy.shape}\")\n",
    "        print(f\"[INFO] Sample avg_entropy: {avg_entropy}\")\n",
    "        return avg_entropy\n",
    "\n",
    "    elif mode == \"promptGeneration\":\n",
    "        ent_prompt, count_prompt = entropy_from_logits(prompt_logits, prompt_attention_mask, top_k)\n",
    "        ent_gen, count_gen = entropy_from_logits(gen_logits, gen_attention_mask, top_k)\n",
    "        total_ent = ent_prompt + ent_gen\n",
    "        total_count = count_prompt + count_gen\n",
    "        avg_entropy = total_ent / (total_count + 1e-9)\n",
    "        print(f\"[INFO] Mode 'promptGeneration': avg_entropy shape {avg_entropy.shape}\")\n",
    "        print(f\"[INFO] Sample avg_entropy: {avg_entropy}\")\n",
    "        return avg_entropy\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown mode: {mode}. Must be 'prompt', 'generation' or 'promptGeneration'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logit_entropy(\n",
    "    prompt_logits: torch.Tensor,\n",
    "    gen_logits: torch.Tensor,\n",
    "    prompt_attention_mask: torch.Tensor,\n",
    "    gen_attention_mask: torch.Tensor,\n",
    "    mode: str = \"prompt\",\n",
    "    top_k: int = None,\n",
    "    window_size: int = None,\n",
    "    stride: int = None\n",
    ") -> np.ndarray:\n",
    "    def entropy_from_logits(logits, attention_mask, top_k=None):\n",
    "        print(f\"\\n[DEBUG] Input logits shape: {logits.shape}\")\n",
    "        print(f\"[DEBUG] Input logits: {logits}\")\n",
    "        print(f\"[DEBUG] Input attention_mask shape: {attention_mask.shape}\")\n",
    "        print(f\"[DEBUG] Input attention_mask: {attention_mask}\")\n",
    "        \n",
    "        logits = logits.float()\n",
    "        attention_mask = attention_mask.float()\n",
    "        \n",
    "        # >>>>> masquer avant de sélectionner le top k non ? non car c'est sur la dim vocab size \n",
    "\n",
    "        if top_k is not None:\n",
    "            print(f\"[DEBUG] top_k activated: {top_k}\")\n",
    "            topk_vals = torch.topk(logits, k=top_k, dim=-1).values\n",
    "            print(f\"[DEBUG] topk_vals:\\n{topk_vals}\")\n",
    "            probs = F.softmax(topk_vals, dim=-1)\n",
    "        else:\n",
    "            print(f\"[DEBUG] Using full softmax\")\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        entropy = torch.special.entr(probs).sum(dim=-1)\n",
    "        print(f\"[DEBUG] Entropy shape: {entropy.shape}\")\n",
    "        print(f\"[DEBUG] Entropy example:\\n{entropy}\")\n",
    "        return entropy, attention_mask\n",
    "\n",
    "    def average_entropy(entropy, mask):\n",
    "        print(\"\\n[DEBUG] === AVERAGE ENTROPY ===\")\n",
    "        entropy_masked = entropy * mask\n",
    "        print(f\"[DEBUG] entropy_masked: {entropy_masked}\")\n",
    "        total_entropy = entropy_masked.sum(dim=-1)\n",
    "        valid_count = mask.sum(dim=-1)\n",
    "        print(f\"[DEBUG] total_entropy: {total_entropy}\")\n",
    "        print(f\"[DEBUG] valid_count: {valid_count}\")\n",
    "        avg_entropy = total_entropy / (valid_count + 1e-9)\n",
    "        print(f\"[DEBUG] avg_entropy: {avg_entropy}\")\n",
    "        return avg_entropy\n",
    "\n",
    "    def max_sliding_window_entropy(entropy, mask, w, stride):\n",
    "        print(\"\\n[DEBUG] === SLIDING WINDOW ENTROPY ===\")\n",
    "        entropy = entropy.unsqueeze(1)  # (B, 1, T)\n",
    "        mask = mask.unsqueeze(1)        # (B, 1, T)\n",
    "        print(f\"[DEBUG] entropy: \\n{entropy}\")\n",
    "        print(f\"[DEBUG] mask: \\n{mask}\")\n",
    "        kernel = torch.ones(1, 1, w, device=entropy.device) / w\n",
    "        print(f\"[DEBUG] kernel: \\n{kernel}\")\n",
    "        moving_avg = F.conv1d(entropy, kernel, stride=stride, padding=0)\n",
    "        valid_counts = F.conv1d(mask, kernel, stride=stride, padding=0)\n",
    "        valid_mask = (valid_counts == 1.0)\n",
    "\n",
    "        print(f\"[DEBUG] moving_avg shape: {moving_avg.shape}\")\n",
    "        print(f\"[DEBUG] moving_avg :\\n{moving_avg}\")\n",
    "        print(f\"[DEBUG] valid_counts:\\n{valid_counts}\")\n",
    "        print(f\"[DEBUG] valid_mask:\\n{valid_mask}\")\n",
    "        print(f\"[DEBUG] moving_avg (before masking):\\n{moving_avg}\")\n",
    "        \n",
    "        moving_avg = moving_avg.masked_fill(~valid_mask, float('-inf'))\n",
    "\n",
    "        print(f\"[DEBUG] moving_avg (after masking):\\n{moving_avg}\")\n",
    "        max_avg_entropy, _ = moving_avg.max(dim=-1)\n",
    "        return max_avg_entropy.squeeze(1)\n",
    "\n",
    "    print(\"\\n[DEBUG] compute_logit_entropy called\")\n",
    "    print(f\"[DEBUG] mode: {mode}, top_k: {top_k}, window_size: {window_size}, stride: {stride}\")\n",
    "    \n",
    "    if top_k is not None:\n",
    "        top_k = int(top_k)\n",
    "        if top_k <= 0 or top_k > prompt_logits.shape[2]:\n",
    "            raise ValueError(\"top_k must be a positive integer less or equal to vocab size\")\n",
    "\n",
    "    if window_size is not None:\n",
    "        if stride is None:\n",
    "            stride = window_size\n",
    "        else:\n",
    "            stride = int(stride)\n",
    "            if stride <= 0 or stride > window_size:\n",
    "                raise ValueError(\"stride must be a positive integer <= window_size.\")\n",
    "    else:\n",
    "        stride = None\n",
    "\n",
    "    if mode == \"prompt\":\n",
    "        entropy, mask = entropy_from_logits(prompt_logits, prompt_attention_mask, top_k)\n",
    "    elif mode == \"generation\":\n",
    "        entropy, mask = entropy_from_logits(gen_logits, gen_attention_mask, top_k)\n",
    "    elif mode == \"promptGeneration\":\n",
    "        ent_p, mask_p = entropy_from_logits(prompt_logits, prompt_attention_mask, top_k)\n",
    "        ent_g, mask_g = entropy_from_logits(gen_logits, gen_attention_mask, top_k)\n",
    "        entropy = torch.cat([ent_p, ent_g], dim=1)\n",
    "        mask = torch.cat([mask_p, mask_g], dim=1)\n",
    "        print(f\"[DEBUG] After concat: entropy shape = {entropy.shape}, mask shape = {mask.shape}\")\n",
    "    else:\n",
    "        raise ValueError(\"mode must be in {'prompt','generation','promptGeneration'}\")\n",
    "\n",
    "    if window_size is None:\n",
    "        result = average_entropy(entropy, mask)\n",
    "    else:\n",
    "        if window_size <= 0:\n",
    "            raise ValueError(\"window_size must be a positive integer\")\n",
    "        if window_size > entropy.shape[1]:\n",
    "            raise ValueError(\"window_size greater than sequence length\")\n",
    "        result = max_sliding_window_entropy(entropy, mask, window_size, stride)\n",
    "\n",
    "    print(f\"\\n[DEBUG] Final entropy per sample: {result}\")\n",
    "    return result.cpu().numpy()\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Dimensions\n",
    "batch_size = 2\n",
    "prompt_len = 5\n",
    "gen_len = 4\n",
    "vocab_size = 3\n",
    "\n",
    "# Logits aléatoires\n",
    "torch.manual_seed(42)\n",
    "prompt_logits = torch.randn(batch_size, prompt_len, vocab_size)\n",
    "gen_logits = torch.randn(batch_size, gen_len, vocab_size)\n",
    "\n",
    "# Masques d’attention avec padding (0 = padding)\n",
    "prompt_mask = torch.tensor([\n",
    "    [0, 0, 1, 1, 1],  # 3 tokens valides\n",
    "    [1, 1, 1, 1, 1],  # tous valides\n",
    "], dtype=torch.float32)\n",
    "\n",
    "gen_mask = torch.tensor([\n",
    "    [1, 1, 0, 0],     # 2 tokens valides\n",
    "    [1, 1, 1, 1],     # 3 valides\n",
    "], dtype=torch.float32)\n",
    "\n",
    "# Appel avec top_k et fenêtre\n",
    "result = compute_logit_entropy(\n",
    "    prompt_logits=prompt_logits,\n",
    "    gen_logits=gen_logits,\n",
    "    prompt_attention_mask=prompt_mask,\n",
    "    gen_attention_mask=gen_mask,\n",
    "    mode=\"promptGeneration\",\n",
    "    top_k=2,\n",
    "    window_size=1,\n",
    "    stride=1\n",
    ")\n",
    "\n",
    "print(\"\\n[TEST RESULT] Entropy scores:\\n\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def compute_logit_entropy(\n",
    "    prompt_logits: torch.Tensor,\n",
    "    gen_logits: torch.Tensor,\n",
    "    prompt_attention_mask: torch.Tensor,\n",
    "    gen_attention_mask: torch.Tensor,\n",
    "    mode: str = \"prompt\",\n",
    "    top_k: int = None\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the per-sample entropy of a language model's output distributions\n",
    "    using its logits and attention masks.\n",
    "    For each token position, the function computes the entropy of the softmax distribution\n",
    "    over the vocabulary. Entropy is averaged over the valid tokens (i.e., those marked\n",
    "    as 1 in the attention mask). If `top_k` is specified, the entropy is computed only\n",
    "    over the top-k logits (highest values) for each position.\n",
    "\n",
    "    Entropy is defined as:\n",
    "        Entropy = -Sum_i p_i * log(p_i)\n",
    "        where p_i = softmax(logits)_i\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    prompt_logits : torch.Tensor\n",
    "        Tensor of shape (batch_size, prompt_len, vocab_size).\n",
    "       These are the model's output logits obtained from a standard forward pass over the prompt sequence.\n",
    "    gen_logits : torch.Tensor\n",
    "        Tensor of shape (batch_size, gen_len, vocab_size).\n",
    "        These are the logits obtained during autoregressive decoding using `model.generate()`.\n",
    "    prompt_attention_mask : torch.Tensor\n",
    "        Tensor of shape (batch_size, prompt_len). Contains 1 where the token is valid and 0 for padding.\n",
    "    gen_attention_mask : torch.Tensor  \n",
    "        Tensor of shape (batch_size, gen_len). Contains 1 where the token is valid and 0 for padding.\n",
    "    mode : str, optional\n",
    "        One of {\"prompt\", \"generation\", \"promptGeneration\"}:\n",
    "        - \"prompt\": compute entropy only over the prompt tokens.\n",
    "        - \"generation\": compute entropy only over the generated tokens.\n",
    "        - \"promptGeneration\": compute entropy over both prompt and generated tokens.\n",
    "          In this case, entropies are summed over all valid tokens and averaged globally.\n",
    "    top_k : int, optional\n",
    "        If specified, compute entropy only over the top-k logits per token.\n",
    "        Useful for estimating uncertainty in the most likely predictions.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Per-sample entropy values as a tensor of shape (batch_size,).\n",
    "        Each value is the average entropy over valid tokens for that sample.\n",
    "    \"\"\"\n",
    "\n",
    "    def entropy_from_logits(logits, attention_mask, top_k=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ---------\n",
    "        logits: (batch_size, seq_len, vocab_size)\n",
    "        attention_mask: (batch_size, seq_len)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        total_entropy: (batch_size,)\n",
    "        valid_token_count: (batch_size,)\n",
    "        \"\"\"\n",
    "        # Convert float16 -> float32 for better accuracy during computations\n",
    "        logits = logits.float()\n",
    "        attention_mask = attention_mask.float()\n",
    "\n",
    "        if top_k is not None:\n",
    "            topk_vals = torch.topk(logits, k=top_k, dim=-1).values # (batch_size, seq_len, top_k)\n",
    "            probs = F.softmax(topk_vals, dim=-1) # (batch_size, seq_len, top_k)\n",
    "        else:\n",
    "            probs = F.softmax(logits, dim=-1) # (batch_size, seq_len, vocab_size)\n",
    "\n",
    "        # Use torch.special.entr, which automatically handles edge cases\n",
    "        # entropy(x) = -x * log(x) with entropy(0) = 0\n",
    "        entropy = torch.special.entr(probs).sum(dim=-1) # (batch_size, seq_len)\n",
    "\n",
    "        entropy_masked = entropy * attention_mask       # (batch_size, seq_len)\n",
    "        total_entropy = entropy_masked.sum(dim=-1)      # (batch_size,)\n",
    "        valid_token_count = attention_mask.sum(dim=-1)  # (batch_size,)\n",
    "\n",
    "        return total_entropy.cpu().numpy(), valid_token_count.cpu().numpy()  # both are (batch_size,)\n",
    "    \n",
    "    if top_k is not None:\n",
    "        top_k = int(top_k)\n",
    "        if top_k < 0 or top_k > prompt_logits.shape[2]: raise ValueError(\"top_k must be an integer between 0 and vocab_size\")\n",
    "\n",
    "    if mode == \"prompt\":\n",
    "        total_entropy, count = entropy_from_logits(prompt_logits, prompt_attention_mask, top_k)\n",
    "        return total_entropy / (count + 1e-9) # (batch_size,)\n",
    "\n",
    "    elif mode == \"generation\":\n",
    "        total_entropy, count = entropy_from_logits(gen_logits, gen_attention_mask, top_k)\n",
    "        return total_entropy / (count + 1e-9) # (batch_size,)\n",
    "\n",
    "    elif mode == \"promptGeneration\":\n",
    "        # Combine prompt and gen entropies\n",
    "        ent_prompt, count_prompt = entropy_from_logits(prompt_logits, prompt_attention_mask, top_k)\n",
    "        ent_gen, count_gen = entropy_from_logits(gen_logits, gen_attention_mask, top_k)\n",
    "\n",
    "        total_ent = ent_prompt + ent_gen         # (batch_size,)\n",
    "        total_count = count_prompt + count_gen   # (batch_size,)\n",
    "        return total_ent / (total_count + 1e-9)  # (batch_size,)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown mode: {mode}. Must be 'prompt', 'generation' or 'promptGeneration'\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ood_env",
   "language": "python",
   "name": "ood_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
