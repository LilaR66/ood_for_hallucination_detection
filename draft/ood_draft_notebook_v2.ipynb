{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OOD detection applied to Hallucination Detection\n",
    "\n",
    " The goal is to predict if an INPUT prompt  is going to produce an hallucination or not (using OOD detection methods). For now, we don’t look at the output generated by the model, we may consider this in a second time. Retrieve ID samples:  To do this, take a general (easy) QA dataset containing questions along with their true hallucination-free answers. Feed the questions to the model. Let the model generate responses and check if the a given generated response is the same as the real hallucination-free answer. All the correct generated responses will be considered ID. More precisely, the ID dataset will consist of the embeddings of the last token of the last layer of the input (or maybe middle layer) of the correct generated responses.  Test a new sample to see if this is going to be OOD=hallucination: Take another dataset containing questions susceptible to trigger hallucinations along with the true hallucination-free answers (or no answer if the model cannot know the answer by any way and all response that the model might produce will necessarily be hallucinated). Feed a question to the model and let it generate a response. Check by comparing to the hallucination-free answer is that generated response is hallucinated or not. At the same time, apply an OOD detection method on the input question (at the last token last layer) and see if there is a correspondence between a high OOD score and a generated hallucination. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "# -----------------------------------\n",
    "import torch\n",
    "import sys\n",
    "import time \n",
    "import os \n",
    "# Add the path to the src directory\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define global variables\n",
    "# -----------------------------------\n",
    "SEED = 44\n",
    "BATCH_SIZE = 16 #32\n",
    "MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "OUTPUT_DIR = \"../results/raw/\"\n",
    "PLOT_DIR   = \"../results/figures/\"\n",
    "LAYER = 16    # integer\n",
    "TOKENS = \"Avg\"  # string\n",
    "\n",
    "\n",
    "from src.inference.activation_utils import (\n",
    "    extract_last_token_activations,\n",
    "    extract_average_token_activations,\n",
    "    extract_max_token_activations\n",
    "    )\n",
    "if TOKENS==\"-1\":\n",
    "    EXTRACT_TOKEN_ACTIVATIONS = extract_last_token_activations\n",
    "elif TOKENS==\"Avg\":\n",
    "    EXTRACT_TOKEN_ACTIVATIONS = extract_average_token_activations\n",
    "elif TOKENS==\"Max\":\n",
    "    EXTRACT_TOKEN_ACTIVATIONS = extract_max_token_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear memory to avoid \"CUDA out of memory\"\n",
    "# -----------------------------------\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\n",
      "Cuda version: 12.6\n",
      "Number of available de GPU : 2\n",
      "GPU 1 : NVIDIA GeForce RTX 4090\n",
      "GPU 2 : NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "# Visualize setup \n",
    "# -----------------------------------\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Cuda version: {torch.version.cuda}\")\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Number of available de GPU : {num_gpus}\")\n",
    "for i in range(num_gpus):\n",
    "    print(f\"GPU {i + 1} : {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed everything\n",
    "# -----------------------------------\n",
    "from src.utils.general import seed_all\n",
    "seed_all(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f3c973fa9df4f099bc9363668e097c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model\n",
    "# -----------------------------------\n",
    "from src.model_loader.llama_loader import load_llama\n",
    "\n",
    "model, tokenizer = load_llama(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ID dataset\n",
    "\n",
    "For the ID general dataset, we are going to use the SQUAD 1.1 dataset: \n",
    "\n",
    "***SQuAD 1.1:** Comprises over 100,000 question-answer pairs derived from more than 500 Wikipedia articles. Each question is paired with a specific segment of text (a span) from the corresponding article that serves as the answer.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Dataset Information =====\n",
      "Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers', 'original_index', 'is_impossible'],\n",
      "    num_rows: 100\n",
      "})\n",
      "Mean ground-truth answer length: 2.75, Max length: 16\n",
      "Mean context + question length: 124.98, Max length: 269\n"
     ]
    }
   ],
   "source": [
    "# Load ID dataset\n",
    "# -----------------------------------\n",
    "from src.data_reader.squad_loader import load_id_fit_dataset\n",
    "\n",
    "id_fit_dataset = load_id_fit_dataset()\n",
    "id_fit_dataset = id_fit_dataset.shuffle(SEED) \n",
    "id_fit_dataset = id_fit_dataset.slice(idx_start=0, idx_end=100)\n",
    "id_fit_dataset.print_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= Analyze one generation  =========\n",
      "----- Prompt construction: 0.000 sec\n",
      "----- Tokenization: 0.003 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Token extraction: 0.391 sec\n",
      "----- Generation: 0.609 sec\n",
      "----- Decoding: 0.000 sec\n",
      "----- Similarity scoring: 0.042 sec\n",
      "\n",
      "=== Prompt ===\n",
      "[INST] <<SYS>>\n",
      "Just give the answer, without a complete sentence. Reply with 'Impossible to answer' if answer not in context.\n",
      "<<SYS>>\n",
      "\n",
      "Context:\n",
      "Under a front page headline \"The Truth\", the paper printed allegations provided to them that some fans picked the pockets of crushed victims, that others urinated on members of the emergency services as they tried to help and that some even assaulted a police constable \"whilst he was administering the kiss of life to a patient.\" Despite the headline, written by Kelvin MacKenzie, the story was based on allegations either by unnamed and unattributable sources, or hearsay accounts of what named individuals had said – a fact made clear to MacKenzie by Harry Arnold, the reporter who wrote the story.\n",
      "\n",
      "Question:\n",
      "What was the story based on?\n",
      "\n",
      "Answer:\n",
      "[/INST]\n",
      "\n",
      "=== Shapes ===\n",
      "Shape - number of tokens: torch.Size([1, 205])\n",
      "Shape - selected_layer: torch.Size([1, 205, 4096])\n",
      "\n",
      "=== Generated Answer ===\n",
      "Allegations from unnamed and unattributable sources or hearsay accounts.\n",
      "\n",
      "=== Ground-truth Answer ===\n",
      "allegations either by unnamed and unattributable sources, or hearsay accounts of what named individuals had said\n",
      "\n",
      "=== Similarity Scores ===\n",
      "ROUGE-L F1: 0.6400\n",
      "Sentence-BERT Cosine Similarity: 0.8867\n",
      "Is generated answer correct: True\n"
     ]
    }
   ],
   "source": [
    "# Visualize one generation with the ID dataset\n",
    "# -----------------------------------\n",
    "from src.inference.inference_utils import analyze_single_generation, build_prompt, get_layer_output, extract_last_token_activations\n",
    "\n",
    "_ = analyze_single_generation(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer, \n",
    "    dataset=id_fit_dataset,\n",
    "    sample_idx=0,\n",
    "    build_prompt_fn=build_prompt,\n",
    "    get_layer_output_fn=get_layer_output,\n",
    "    layer_idx=LAYER,\n",
    "    extract_token_activations_fn=extract_last_token_activations,\n",
    "    end_offset=-5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve ID embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Input text =====\n",
      "[INST] <<SYS>>\n",
      "Just give the answer, without a complete sentence. Reply with 'Impossible to answer' if answer not in context.\n",
      "<<SYS>>\n",
      "\n",
      "Context:\n",
      "Many of the leading universities associated with Enlightenment progressive principles were located in northern Europe, with the most renowned being the universities of Leiden, Göttingen, Halle, Montpellier, Uppsala and Edinburgh. These universities, especially Edinburgh, produced professors whose ideas had a significant impact on Britain's North American colonies and, later, the American Republic. Within the natural sciences, Edinburgh's medical also led the way in chemistry, anatomy and pharmacology. In other parts of Europe, the universities and schools of France and most of Europe were bastions of traditionalism and were not hospitable to the Enlightenment. In France, the major exception was the medical university at Montpellier.\n",
      "\n",
      "Question:\n",
      "Which university, especially, produced professors whose ideas had a significant impact on the colonies?\n",
      "\n",
      "Answer:\n",
      "[/INST]\n",
      "\n",
      "===== Decoded text between `start_offset` and `end_offset` =====\n",
      "----START TEXT---\n",
      "Context:\n",
      "Many of the leading universities associated with Enlightenment progressive principles were located in northern Europe, with the most renowned being the universities of Leiden, Göttingen, Halle, Montpellier, Uppsala and Edinburgh. These universities, especially Edinburgh, produced professors whose ideas had a significant impact on Britain's North American colonies and, later, the American Republic. Within the natural sciences, Edinburgh's medical also led the way in chemistry, anatomy and pharmacology. In other parts of Europe, the universities and schools of France and most of Europe were bastions of traditionalism and were not hospitable to the Enlightenment. In France, the major exception was the medical university at Montpellier.\n",
      "\n",
      "Question:\n",
      "Which university, especially, produced professors whose ideas had a significant impact on the colonies?\n",
      "\n",
      "Answer:\n",
      "----END TEXT---\n",
      "\n",
      "start_offset: 40, end_offset:-4\n"
     ]
    }
   ],
   "source": [
    "# Compute offsets to select the tokens to give to the model\n",
    "# -----------------------------------\n",
    "from src.inference.inference_utils import compute_token_offsets, build_prompt\n",
    "idx = 67\n",
    "text = build_prompt(id_fit_dataset[idx][\"context\"], id_fit_dataset[idx][\"question\"])\n",
    "start_offset, end_offset = compute_token_offsets(\n",
    "    text=text,\n",
    "    tokenizer=tokenizer,\n",
    "    start_phrase=\"\\nContext:\",\n",
    "    end_phrase=\"Answer:\\n\",\n",
    "    debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start retrieving ID embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:13<00:00,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...end!\n",
      "ID embeddings: Time elapsed: 00 min 13 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Retrieve ID embeddings and save results \n",
    "# -----------------------------------\n",
    "from src.inference.inference_utils import (batch_extract_token_activations_with_generation, build_prompt, get_layer_output)\n",
    "from src.utils.general import print_time_elapsed\n",
    "\n",
    "# Runs batched inference on a dataset using a decoder-only language model.\n",
    "# For each batch, generates answers, computes semantic similarity scores, extracts token-level activations,\n",
    "# and appends the results to a pickle file.\n",
    "print(\"\\nStart retrieving ID embeddings...\")\n",
    "t0 = time.time()\n",
    "batch_extract_token_activations_with_generation(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=id_fit_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    idx_start_sample=0,\n",
    "    max_samples=len(id_fit_dataset),\n",
    "    output_path = OUTPUT_DIR + f\"id_fit_results_layer{LAYER}_token{TOKENS}.pkl\",\n",
    "    build_prompt_fn=build_prompt,\n",
    "    get_layer_output_fn=get_layer_output,\n",
    "    layer_idx=LAYER,  \n",
    "    extract_token_activations_fn=EXTRACT_TOKEN_ACTIVATIONS,\n",
    "    start_offset=start_offset,\n",
    "    end_offset=end_offset\n",
    ")\n",
    "t1 = time.time()\n",
    "print(\"...end!\")\n",
    "print_time_elapsed(t0, t1, label=\"ID embeddings: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free memory \n",
    "del id_fit_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load OOD/Hallucinations test datasets \n",
    "\n",
    "To evaluate the Hallucination detection in question answering using OOD detection methods, we will use datasets in the SQuAD style:\n",
    "\n",
    "***SQuAD 2.0:** SQuAD 2.0 extends the original **SQuAD 1.1** dataset by adding around 50,000 unanswerable questions. These questions are carefully designed to look similar to answerable ones, making it more challenging for models to determine when there isn’t enough information in the context to provide an answer.*\n",
    "\n",
    "**Test Dataset Composition** \\\n",
    "Our test set will include two types of samples:\n",
    "- ***Impossible samples***: Questions that cannot be answered based on the provided context (i.e., the answer is not present in the text). These are taken from the training split of SQuAD 2.0, selecting only the unanswerable questions.\n",
    "- ***Possible samples***: Questions where the answer is explicitly present in the context. These are drawn from the validation split of SQuAD 1.1. This ensures there is no overlap with the in-distribution (ID) data from the SQuAD 1.1 training split.\n",
    "\n",
    "**Note on Evaluation Scope**\\\n",
    "Currently, our evaluation focuses on whether the model can answer questions using only the information provided in the input context. We do not test the model’s internal knowledge or ability to answer questions without supporting context. However, this setup closely matches the OOD scenario: if the information is not in the text, the model should recognize and indicate this.\n",
    "\n",
    "**Additional Dataset: TriviaQA**\\\n",
    "To further test generalization, we will also use the TriviaQA dataset. Like SQuAD, TriviaQA provides question-answering prompts with supporting context. The model must either extract the correct answer from the context or correctly identify when the answer is not present.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Dataset Information =====\n",
      "Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers', 'original_index', 'is_impossible'],\n",
      "    num_rows: 1000\n",
      "})\n",
      "Mean ground-truth answer length: 2.94, Max length: 25\n",
      "Mean context + question length: 132.44, Max length: 513\n",
      "\n",
      "===== Dataset Information =====\n",
      "Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers', 'original_index', 'is_impossible'],\n",
      "    num_rows: 1000\n",
      "})\n",
      "No valid ground-truth answers to compute length stats.\n",
      "Mean context + question length: 129.01, Max length: 451\n"
     ]
    }
   ],
   "source": [
    "# Load test datasets\n",
    "# -----------------------------------\n",
    "from src.data_reader.squad_loader import load_id_test_dataset, load_od_test_dataset\n",
    "\n",
    "# Load possible test dataset \n",
    "id_test_dataset = load_id_test_dataset()\n",
    "id_test_dataset = id_test_dataset.shuffle(SEED) \n",
    "id_test_dataset = id_test_dataset.slice(idx_start=0, idx_end=1000)\n",
    "id_test_dataset.print_info()\n",
    "\n",
    "# Load impossible test dataset \n",
    "od_test_dataset = load_od_test_dataset()\n",
    "od_test_dataset = od_test_dataset.shuffle(SEED) \n",
    "od_test_dataset = od_test_dataset.slice(idx_start=0, idx_end=1000)\n",
    "od_test_dataset.print_info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= Analyze one generation  =========\n",
      "----- Prompt construction: 0.000 sec\n",
      "----- Tokenization: 0.018 sec\n",
      "----- Token extraction: 0.038 sec\n",
      "----- Generation: 0.076 sec\n",
      "----- Decoding: 0.000 sec\n",
      "----- Similarity scoring: 0.004 sec\n",
      "\n",
      "=== Prompt ===\n",
      "[INST]\n",
      "\n",
      "Just give the answer, without a complete sentence. Reply with 'Impossible to answer' if answer not in context.\n",
      "\n",
      "Context:\n",
      "In the Catholic Church, canon law is the system of laws and legal principles made and enforced by the Church's hierarchical authorities to regulate its external organization and government and to order and direct the activities of Catholics toward the mission of the Church.\n",
      "\n",
      "Question:\n",
      "What mission this Canon law directly activities of all Christians towards?\n",
      "\n",
      "Answer:\n",
      "[/INST]\n",
      "\n",
      "=== Shapes ===\n",
      "Shape - number of tokens: torch.Size([1, 117])\n",
      "Shape - selected_layer: torch.Size([1, 117, 4096])\n",
      "\n",
      "=== Generated Answer ===\n",
      "Mission\n",
      "\n",
      "=== Ground-truth Answer ===\n",
      "\n",
      "\n",
      "=== Similarity Scores ===\n",
      "ROUGE-L F1: 0.0000\n",
      "Sentence-BERT Cosine Similarity: 0.3657\n",
      "Is generated answer correct: False\n"
     ]
    }
   ],
   "source": [
    "# Visualize one generation with the test impossible dataset\n",
    "# -----------------------------------\n",
    "from src.inference.inference_utils import analyze_single_generation, build_impossible_prompt, get_layer_output, extract_last_token_activations\n",
    "\n",
    "_ = analyze_single_generation(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer, \n",
    "    dataset=od_test_dataset,\n",
    "    sample_idx=500,\n",
    "    build_prompt_fn=build_impossible_prompt,\n",
    "    get_layer_output_fn=get_layer_output,\n",
    "    layer_idx=LAYER,\n",
    "    extract_token_activations_fn=extract_last_token_activations,\n",
    "    end_offset=-5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve test embeddings \n",
    "\n",
    "Test embeddings which may be OOD/Hallucinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start retrieving test impossible embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:53<00:00,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...end!\n",
      "Test impossible embeddings: Time elapsed: 00 min 53 sec\n",
      "\n",
      "\n",
      "Start retrieving test possible embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:53<00:00,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...end!\n",
      "Test possible embeddings: Time elapsed: 00 min 53 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Retrieve test embeddings and save results \n",
    "# -----------------------------------\n",
    "from src.inference.inference_utils import batch_extract_token_activations, build_prompt, get_layer_output, extract_last_token_activations\n",
    "from src.utils.general import print_time_elapsed\n",
    "\n",
    "# Runs batched inference on a dataset using a decoder-only language model.\n",
    "# For each batch, gextracts token-level activations, and appends the results to a pickle file.\n",
    "print(\"\\nStart retrieving test impossible embeddings...\")\n",
    "t2 = time.time()\n",
    "batch_extract_token_activations(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=od_test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    idx_start_sample=0,\n",
    "    max_samples=len(od_test_dataset),\n",
    "    save_to_pkl = True,\n",
    "    output_path = OUTPUT_DIR + f\"od_test_results_layer{LAYER}_token{TOKENS}.pkl\",\n",
    "    build_prompt_fn=build_prompt,\n",
    "    get_layer_output_fn=get_layer_output,\n",
    "    layer_idx=LAYER,  \n",
    "    extract_token_activations_fn=EXTRACT_TOKEN_ACTIVATIONS,\n",
    "    start_offset=start_offset,\n",
    "    end_offset=end_offset\n",
    ")\n",
    "t3 = time.time()\n",
    "print(\"...end!\")\n",
    "print_time_elapsed(t2, t3, label=\"Test impossible embeddings: \")\n",
    "\n",
    "\n",
    "print(\"\\nStart retrieving test possible embeddings...\")\n",
    "t4 = time.time()\n",
    "batch_extract_token_activations(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=id_test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    idx_start_sample=0,\n",
    "    max_samples=len(id_test_dataset),\n",
    "    save_to_pkl = True,\n",
    "    output_path = OUTPUT_DIR + f\"id_test_results_layer{LAYER}_token{TOKENS}.pkl\",\n",
    "    build_prompt_fn=build_prompt,\n",
    "    get_layer_output_fn=get_layer_output,\n",
    "    layer_idx=LAYER,  \n",
    "    extract_token_activations_fn=EXTRACT_TOKEN_ACTIVATIONS,\n",
    "    start_offset=start_offset,\n",
    "    end_offset=end_offset\n",
    ")\n",
    "t5 = time.time()\n",
    "print(\"...end!\")\n",
    "print_time_elapsed(t4, t5, label=\"Test possible embeddings: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free memory \n",
    "del od_test_dataset \n",
    "del id_test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load extracted embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10000 samples from: ../results/raw/id_fit_results_layer16_tokenAvg.pkl\n",
      "Size before filtering incorrect samples: 10000.\n",
      "Size after filtering: 7420. Filtered 2580 samples.\n"
     ]
    }
   ],
   "source": [
    "# Load ID embeddings \n",
    "# -----------------------------------\n",
    "from src.data_reader.pickle_io import load_pickle_batches\n",
    "from src.utils.general import filter_correct_entries\n",
    "\n",
    "# Load extracted embeddings \n",
    "id_fit_embeddings = load_pickle_batches(OUTPUT_DIR + f\"id_fit_results_layer{LAYER}_token{TOKENS}.pkl\")\n",
    "# Only keep rows with simiar generated and ground-truth answers\n",
    "id_fit_embeddings = filter_correct_entries(id_fit_embeddings) \n",
    "# Concatenate all embeddings \n",
    "#id_fit_embeddings = torch.cat(id_fit_embeddings['activations'], dim=0) # shape: [N, D]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1000 samples from: ../results/raw/id_test_results_layer16_tokenAvg.pkl\n",
      "Loaded 1000 samples from: ../results/raw/od_test_results_layer16_tokenAvg.pkl\n"
     ]
    }
   ],
   "source": [
    "# Load test embeddings \n",
    "# -----------------------------------\n",
    "from src.data_reader.pickle_io import load_pickle_batches\n",
    "\n",
    "# Load extracted possible and impossible embeddings \n",
    "od_test_embeddings = load_pickle_batches(OUTPUT_DIR + f\"id_test_results_layer{LAYER}_token{TOKENS}.pkl\")\n",
    "id_test_embeddings = load_pickle_batches(OUTPUT_DIR + f\"od_test_results_layer{LAYER}_token{TOKENS}.pkl\")\n",
    "# Concatenate possible and impossible all embeddings \n",
    "od_test_embeddings = torch.cat(od_test_embeddings['activations'], dim=0) # shape: [N, D]\n",
    "id_test_embeddings = torch.cat(id_test_embeddings['activations'], dim=0) # shape: [N, D]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform DKNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear memory to avoid \"CUDA out of memory\"\n",
    "# -----------------------------------\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructs the FAISS index from ID data.\n",
    "# -----------------------------------\n",
    "from src.analysis.dknn import fit_to_dataset\n",
    "index = fit_to_dataset(id_fit_embeddings, use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute DKNN scores on test data\n",
    "# -----------------------------------\n",
    "from src.analysis.dknn import score_tensor\n",
    "k=5\n",
    "dknn_scores_id  = score_tensor(index, id_test_embeddings, nearest=k, batch_size=BATCH_SIZE)\n",
    "dknn_scores_ood = score_tensor(index, od_test_embeddings, nearest=k, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auROC: 0.5670\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC: 0.5670\n"
     ]
    }
   ],
   "source": [
    "# Compute and plot auroc \n",
    "# -----------------------------------\n",
    "from src.analysis.dknn import compute_auroc\n",
    "auroc, fpr, tpr, thresholds = compute_auroc(\n",
    "    dknn_scores_id, \n",
    "    dknn_scores_ood, \n",
    "    plot=True,\n",
    "    save_path= PLOT_DIR + f\"roc_layer{LAYER}_token{TOKENS}_k{k}.png\"\n",
    ")\n",
    "print(f\"AUROC: {auroc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA00AAAIoCAYAAAC1TQBxAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAAmbhJREFUeJzs3XlcVOX+B/DPOWcYFkVABUVBQFwwE1yg8pqK5prXa6utZla2Z1277S6UW/u1ffuVdrvtdVutzFQ0y1JyT9FUXBFBZVMQmHOe3x84IwMzAwg45xk+79eLl/KcMzPfZ+bLM/Od55znKEIIASIiIiIiInJJ9XYAREREREREZsaiiYiIiIiIyAMWTURERERERB6waCIiIiIiIvKARRMREREREZEHLJqIiIiIiIg8YNFERERERETkAYsmIiIiIiIiD1g0ERERERERecCiiYh8RlpaGhRFQXp6urdD8Xnp6elQFAVpaWleefzY2FjExsY6tXn79d+zZw8URcGNN97olcdvDJdccgl69OgBXde9HQoBUBQFqampTm3ezvOqbrzxRiiKgj179jjafvrpJyiKgu+++857gRE1ARZNRCZk//BV9ScoKAgdOnTARRddhBkzZmDXrl0ub2v/MHv77bfX2CaEwP333w9FUZCUlIScnBwAp9/4FEXB6tWrXd7vqFGjarw5Vo1z5MiRLm/322+/1euD5MKFC536raoqWrVqhbi4OIwbNw4vvfQSjh07Vqf7qi9XH1B8WUPyrKFSU1OhKEqT3HdTclWs+YoVK1bgq6++wsyZM6FpmqPd/iH9o48+8nh7IQS+//573HHHHUhMTERISAiCgoKQlJSEuXPn4uTJk/WKx56TPXv2dFnE5eTkNLu/WRkMGzYMF154IR588EEW3+RTLN4OgIjci4+Px/XXXw8AKCsrQ25uLtasWYNZs2Zh7ty5ePDBBzFnzpw6ffjUdR2TJ0/GggULMGDAAHz77bcIDQ2tsd9DDz2ElStX1jvWH3/8EcuWLcPQoUPrfVtXLrroIlx44YUAgOPHj+PgwYP4+eef8fXXX2PmzJl44403cOWVVzrd5u6778bVV1+NTp06NUoMzcWZ5Nl5552Hbdu2oW3btl6JeenSpV55XE86duyIbdu2ISQkxNuhnJHp06cjJiYG48ePP6Pbl5WV4eKLL4a/vz9SU1MxcuRInDx5EosXL8Zjjz2GL7/8Eunp6QgKCqrX/W7duhULFy7EzTfffEZx+RoZxrkHH3wQ//jHP/DRRx/huuuu83Y4RI2CRRORiXXp0sXl4U+rVq3ChAkTMG/ePGiahlmzZnm8n7KyMlx99dX48ssvcfHFF+Ozzz5DYGBgjf3i4+Px888/45tvvsHYsWPrHGdsbCz27duHhx56CGvWrGmUGYRhw4bh4YcfdmrTdR3vvvsu7r77blxzzTUICQnBiBEjHNvbtm3rtQ/xMjuTPAsKCkJCQsJZjNJZfHy81x7bHT8/P68+Jw3x559/4ueff8Zjjz0GVT2zg1A0TcPs2bNx5513IiwszNFeUVGByy+/HN988w1eeeUVPPDAA3W+z4iICJSUlCAtLQ3XXXcdAgICzig2XyLDODdq1Ci0bdsWr7/+Oosm8hk8PI9IQhdeeCF++OEH+Pv74+mnn8b+/fvd7ltcXIyLL74YX375Ja699lp8+eWXLgsmAJg5cyYsFgseffRRGIZR53i6d++OCRMmICMjA5988km9+1NXmqbhpptuwmuvvQZd1zF16lQIIRzb3R3rv3z5cowePRodOnSAv78/2rVrh4EDB+LNN98EcPqQRqDyEKWqh6stXLgQAFBYWIinnnoKgwcPRocOHWC1WtGhQwfccMMNLg9hqxrLBx98gN69eyMwMBCRkZG49957UVpa6rKPK1euxCWXXIJ27drB398f0dHRuOyyy7Bq1Sqn/YQQeOeddzBgwAC0atUKQUFBSE5OxjvvvHOmT28NnvLM3TlNf/31FyZNmoS4uDj4+/ujdevWSEpKwn333ed4rRRFwYoVKxz/t//YD+Gsem7Qtm3bcOmll6JNmzZOh4fWdpjc22+/jV69eiEgIAAdO3bEP//5TxQXFzvt4+m8rOrnJ9l/37t3L/bu3esUt/32ns5p2rt3L26++WZ07NgRVqsVUVFRuPnmm7Fv374a+9oPXayoqEBaWhpiY2Ph7++Pbt264dVXX62x/8mTJ/Hcc88hKSkJISEhaNGiBWJjYzF+/Hhs3LjR7XNU1YIFCwCgxuxtffj5+eGxxx5zKpjs7Y888ggAOF73ugoLC8P999+PAwcO4IUXXqjz7Y4cOYL77rvPkYcREREYP348tmzZUmNf++HJu3fvxnPPPYdzzjkH/v7+jtfRnmuFhYW44447EBkZiRYtWmDQoEFYt24dACA7OxvXX389IiIiEBgYiBEjRuCvv/6q8VhffPEFrrnmGnTp0gVBQUEICQnBwIED8fnnn9e5b67GOXvOuPupnpPFxcWYOXMmevbsicDAQISGhmLkyJE1xhm7P//8E3//+98RHByMkJAQXHzxxS6fSzs/Pz9ccsklWLVqFXbu3FnnvhGZGWeaiCTVvXt3jB8/Hu+99x6+/PJL3HPPPTX2OXLkCEaPHo2MjAzcfffdePHFFz3OAnXt2hWTJ0/Ga6+9hnfffReTJk2qczxPPPEEPvroI0ybNg2XXXYZ/Pz8zqhfdTFhwgTMnDkTf/75J7Zs2YJevXq53XfRokUYO3YsQkNDMW7cOERGRiIvLw8bN27Ee++9h1tvvRWxsbGYOXMmHn/8ccTExDh9wOjduzcAYNu2bZgxYwaGDBmCSy+9FC1atEBmZiY++OADLFq0COvWrUNMTEyNx3/55Zfxww8/YNy4cRg6dCh++OEHvPjiizhy5Ajef/99p31feOEF/POf/0RgYCAuvfRSdOrUCQcPHsSqVavw2WefOQ5XFELguuuuw4cffoiuXbvi2muvhdVqxZIlS3DzzTdj69atePbZZxv+RKNueWaXnZ2N8847DydOnMCYMWNw1VVX4cSJE/jrr7/w6quv4tlnn4XFYsHMmTOxcOFC7N27FzNnznTc3v5c2+3cuRMXXHABevXqhRtvvBFHjx6F1WqtNebnn38eS5cuxVVXXYUxY8bgp59+wvz58/Hbb79h5cqVZ5SboaGhmDlzJubPnw8AuO+++xzbajunZseOHbjwwguRl5eHsWPHomfPntiyZQveeecdfPPNN1i1ahW6detW43bXXHMN1qxZg9GjR0PTNHzyySe466674Ofnh8mTJzv2mzhxIj755BMkJiZi0qRJ8Pf3x/79+7F8+XKsXbsWSUlJtfZv6dKlaNGiBc4999w6PR/1ZX/OLZb6f+z417/+hddeew1PPvkkJk+ejNatW3vcPy8vD/3798euXbuQmpqKq6++GllZWfjss8+waNEiLF682PG3VNU999yD3377DWPGjMHYsWMRERHh2FZeXo7hw4fj5MmTuOqqq3D48GF88sknGDZsGH799VeMHDkSkZGRuP7667Fz50588803GDNmDLZt2+Z0ftgjjzwCq9WKCy+80DEWff3117jiiivw4osvevz78uTGG290mYfff/891qxZ43RI5LFjxzBo0CD8+eefGDBgAG6//XYUFRXhq6++wpAhQ/Dpp5/ikksucey/ZcsWDBgwAMePH8dll12Grl27Ys2aNRgwYIDH3Orfvz/+7//+D8uWLUOXLl3OqF9EpiKIyHSysrIEADFy5EiP+7399tsCgJgwYYKjbfny5QKA+Pvf/y569OghAIgZM2Z4vJ+JEycKAGL16tUiJydHtGzZUkRFRYnS0lLHPiNHjhQARFZWlts4//WvfwkA4qWXXnLss3r1agFATJw4sU59X7BggQAg5s2b53G/CRMmCADi7bffdrTNnDlTABDLly93tF122WUCgNiwYUON+zhy5IjT7wDE4MGDXT5eQUGBOHr0aI32ZcuWCVVVxS233OLUbo8lJCREZGZmOtpLSkpEt27dhKqq4uDBg472DRs2CFVVRYcOHZyeYyGEMAzDad8333xTABCTJk0S5eXljvaysjIxduxYAUBkZGS47EdVjZFnM2fOdLS9+OKLAoCYP39+jfuo/twNHjxYuHsLssflKXdjYmJETEyMU5v9ObdarWLjxo2OdsMwxLXXXisAiGeffdZjH6rHUD1vXT1ubbcZMmSIACDeeOMNp/ZXXnlFABBDhw51arc/N+eff74oLCx0tGdmZgqLxSK6d+/uaCsoKBCKooh+/foJm83mdD82m03k5+e7jLWq4uJioaqqGDBggMvt9uf1ww8/rPW+3LnjjjsEAPHKK6/U+TYAHH19+eWXBQBx//33O7YfOnTI5d/spEmTBADxyCOPOLUvWrRIABBdunQRuq472u3jX1RUlNi7d2+NOGJiYgQAceWVV4qKigpH+1NPPSUAiNDQUPHPf/5TGIZRo7+ff/65033t2rWrxv0XFxeLXr16iZCQEHHixIkaz0H1/rka51xZuXKlsFqtonPnziIvL8/Rbv9beOutt5z2P3z4sIiOjhbh4eFOY789H//73/867f/II484/k6rj1lCCLFx40YBQNxwww0e4ySSBQ/PI5JYhw4dAFTOKFX37bffYtu2bbj66qvx+OOP1/k+27Vrh6lTp+LAgQN48cUX6xXPo48+itDQUMyaNQvHjx+v123ry1PfXXF1SGKbNm3q/HghISEuv+EeMmQIevbsiZ9++snl7e699150797dKY5rrrkGhmHgjz/+cLS/8cYbMAwDs2fPrnHYmaIojv4ClbNXLVq0wCuvvOI0a2K1WjFnzhwAwIcffljnvtWmMZ7r2mYHXGnfvj0ee+yxet/uhhtuQGJiouN3RVEwd+5caJrmONzybNm3bx+WL1+Oc845x2l2CABuv/12JCQkYNmyZS4PsZ03bx5atWrl+L179+4YMGAAtm/f7jjUUFEUCCEQEBBQ41wkTdNcLvZSXXZ2NgzDQLt27c6gh7X7/vvv8cYbb6BHjx5nvJjDrbfeii5duuCVV17xeDhyeXk5PvzwQ7Rp0wbTpk1z2nbxxRdj+PDh2LlzJ3755Zcat33ggQc8Lq5gnym1u+aaawAANpsNs2fPdprFt2+rfnhk586da9xvy5YtceONN6KwsBBr1651+/j1sXPnTlx66aUICgrCokWLHOdAHTlyBB9//DGGDh2KW265xek2EREReOCBB5CXl+cYz/bt24cVK1YgMTGxxrlJ9vHeHXs+HThwoFH6RORtLJqIfNT555+PVq1a4ZNPPqn3B8V//etfCA8Px5NPPon8/Pw63y4sLAwPP/wwcnNzG+3wsIa6+uqrAQAXXHAB7r77bnzxxRd1/vBfXXp6Oi655BJERkbCz8/Pcb7A5s2bkZ2d7fI2/fr1q9EWFRUFACgoKHC0rVmzBgCcFrZwpaSkBJs3b0ZoaCieeuoppKWlOf3Yl4XOzMw8ky42yNixY9GiRQvcdddduOqqq7BgwQLs3r37jO8vKSmpTofjVTdw4MAabTExMYiOjsaff/6J8vLyM46pvjZs2AAAGDx4cI1DY1VVxaBBg5z2q6ouudOqVStcfPHF+OWXX9C3b1/MnTsXv/76KyoqKuoc49GjRwGgTgVWfa1duxZXXXUVQkJC8Omnn8Lf3/+M7sfPzw+zZ8/GyZMnMX36dLf7ZWZm4uTJkzjvvPNcrtI3ZMgQAK6f7/POO8/t/YaFhdUoqCIjIwFUHtZc/bHs26qPC7m5uZg6dSp69OiBoKAgxxhy//33u9z/TOTn52PMmDEoLCzEZ5995rQ4ydq1a6HrOsrKymqMHWlpafjtt98AnB4/7EWfq8MZW7ZsWeOQ2qrsX5Sc6XhLZDY8p4lIYvY32PDw8BrbevfujRdeeAEjR47ETTfdBF3X6/wtb3BwMKZPn44pU6Zg3rx5ePrpp+sc05QpU/Dyyy/jueeew5133lnn29WXp75XdeWVV+LLL7/E888/j9dffx2vvPIKFEXBkCFD8Nxzz3l806/q008/xVVXXYWWLVti5MiRiI2NdXzosZ+f40rVmQI7+7fVVa9hUlhYCEVRHB+23MnPz4cQAgcPHvQ4g3jixIm6dKtO6vpcx8bG4rfffkNaWhq+++47x6IgCQkJeOKJJ+q9yMCZzny4u127du2wZ88eFBcX12uWsSGKioo8xmR/ve37VVXX3Pn0008xd+5cfPDBB46ZuVatWmHSpEmYO3durUt822cG63sdpdpkZGRgxIgRUFUVixcvRs+ePRt0f+PHj8ezzz6L9957D/fff7/LfGzI8+0p3zy9Fp62VS1ejx07hpSUFOzbtw8DBgzAsGHDEBoaCk3TsGHDBnz11VcoKytzG0NdVFRU4LLLLsOOHTvw5ptv4qKLLnLabr/G3S+//OJyts3OPn4UFhYCgNP5XVV5es7si93Ud4l5IrPiTBORxOyrJ6WkpLjcfv7552PJkiUICQnB5MmTHavF1cXtt9+O+Ph4vPTSSx4Ph6kuMDAQjz/+OI4fP16vwwLrwzAMx7Wk3PW9qnHjxmHFihXIz8/H999/j1tuuQXp6ekYNWqU02yPJ2lpaQgICMAff/yBTz/9FM888wwef/xxR3tDhYaGQgiBQ4cOedzP/gGtX79+EEK4/Vm+fHmDY7KrLc+qOvfcc/HZZ5/h2LFjWL16NWbMmIGcnBxcddVVHj+kuXKmS9cfPnzYbbuiKAgODgYAx+FsNputxr72D4sNZX+93MVkv8C0qw/edRUUFITZs2dj9+7d2L17N95++210797dsbBIbezFR2NeNDojIwPDhw+HYRhYvHhxnXKnNoqi4KmnnoJhGDUuR2DXkOe7qS+2/Pbbb2Pfvn2YNWsWVq1ahZdeegmzZs1CWloaLrjggkZ5jNtuuw3p6em4//77axwOCpzu9/333+9x/LAv0GK/5lhubq7Lx3P3PAOn86m2L1uIZMGiiUhSO3bswCeffAJ/f39ceumlbvdLSUnBTz/9hNDQUNx+++147bXX6nT/VQ+HmTFjRr1imzhxInr27Im33nqrSZabfe+997B371706tWrXt9eBwcHY9SoUXjzzTdx44034vDhw/j9998d21VVdXsF+127dqFHjx7o2rWrU/uhQ4cadAianf3QoB9//NHjfsHBwejRowe2bdtW54KvIeqaZ9X5+fnhggsuwOOPP44XX3wRQgh8++23ju32FcXcPd8N8fPPP9do27t3L/bv34+ePXs6DvmzL4198ODBGvuvX7/e5X1rmlavmO0zmStXrnRaHh+oXAXRXvzXdcazNnFxcbjpppuwYsUKtGzZEl9//XWtt+nQoQPatGmD7du3N0oM9oJJ13X88MMPOP/88xvlfgFg6NChGDlyJL777juXF+FOSEhAQEAA1q5di5KSkhrb7V8ANNbzXR/2SxOMGzeuxjZXOVtf8+bNw4IFCzBu3Di3RwekpKRAURSsXr26TvdpXx3P1VLkx48fd3mYo509nzytbkokExZNRBL65ZdfMHLkSJSVleHhhx9Gx44dPe7fr18/LF26FK1bt8add96Jl19+uU6Pc9VVV6Ffv374z3/+gx07dtQ5Pk3TMHfuXMd1ZhqLrutYsGAB7rjjDmiahueff77Wb4dXrlzp8kOu/ZvTqrNErVu3dnvSckxMDHbu3On0zerJkydxxx131Ov8EXduv/12aJqGadOm1TjUTwjhdK7DlClTUFJSgsmTJ7s8DC8rK8txPaOGqG+e/fHHHy4Pe7I/Z9WfawD1msWsq//85z/YtGmT43chBB599FHouu60nHz37t0RHByMr7/+2mmW5fDhw5g9e7bL+27dujWOHDlS50PZOnXqhCFDhuDPP/+scQ2tN998E9u2bcPQoUMRHR1djx6elpeX5/J6Ofn5+SgrK6vTLKiiKBg4cCCysrKQl5d3RnHY/fHHHxg+fDhsNhu+//579O/fv0H358qTTz4JRVHw6KOP1thmtVpxzTXX4MiRI5g3b57Tth9++AGLFy9Gly5dMGDAgEaPqzb2SxJUL0A++OADfPfddw26788++wyPPfYY+vbti/fff9/tBYrbt2+P8ePH49dff8UzzzxTo5AHgN9//91RcHbq1AmDBg3Cpk2balwiYe7cuR6/uLF/ITV48OAz7BWRufCcJiIT27lzp6PoKC8vR25uLtasWYPNmzc7PmBXvc6NJ3369MGyZctw0UUX4Z577oGu67j33ns93kZRFDz55JMYPnw4srKy6hX7P/7xD1x44YVuL5ZYm59++snxwbSkpAQHDhzAypUrcfDgQbRu3Rrvvfcehg0bVuv9TJkyBdnZ2bjwwgsRGxsLRVGwatUqrFmzBhdccIHTCc5Dhw7FJ598gksuuQR9+vSBpmn4xz/+gcTERNxzzz2455570KdPH1xxxRWw2WxYsmQJhBBISkqq80VE3enVqxfmz5+PKVOmoGfPnrjkkksQExODnJwcrFy5EmPGjHFcI+i2227Db7/9hnfffRe//PILhg0bhg4dOuDw4cPIzMzE77//jg8++MDjxV+raqw8e++99/DGG29g0KBBiI+PR6tWrbB161Z89913aN26tdN1v4YOHYrPPvsMl19+OUaPHo2AgAAkJSVh7Nix9X3qahg5ciT69++Pq6++GuHh4Vi6dCkyMjJwwQUXOF0Hx2q14p577sHcuXPRt29fjBs3DsXFxfjmm28wePBglxctHjp0KDIyMjB69GgMHDgQVqsVgwYNcizo4Mprr72GCy+8EJMnT8Y333yDc845B3/++Se+/vprhIeH13n215WDBw+iT58+SEpKQmJiIjp27IijR4/iq6++QkVFBf71r3/V6X4uvfRSfPnll1iyZAmuvfZat/344YcfXG675ZZbcM4552D48OEoKCjAqFGjsGTJEixZssRpv9DQUKdrXJ2J3r1749prr63xId7uqaeewooVKzB79mz8+uuvOP/887Fnzx58+umnCAoKwoIFC9wWFU1pwoQJeOqpp3DPPfdg+fLliImJwcaNG7F06VJcdtll+N///nfG933DDTdACIG+ffvimWeeqbG9d+/ejmsvvfrqq9i+fTsefPBBvPfee+jfvz9CQ0Oxf/9+ZGRk4K+//sKhQ4cc5yK98sorGDBgAG644QZ8+eWXjus0rV27FgMHDnQ7S7ZkyRKEhYV5/NsgkspZW9yciOqs6nVq7D+BgYEiMjJSDBkyREyfPl3s3LnT5W3t15657bbbXG7fvHmziIiIEADEc889J4Rwvk6TKyNGjHB5PY7arvPzyy+/OG5X3+s02X8URREtW7YUsbGxYuzYseKll14Sx44dc3lbV9cv+eijj8T48eNFfHy8CAoKEiEhISIpKUk89dRTori42On2hw4dEuPHjxdt27YVqqoKAGLBggVCiMpr/bz++uuiZ8+eIiAgQLRv317cfPPNIjc31+U1hzxdS8XeR/t9V7V8+XLx97//XbRu3VpYrVYRFRUlLr/8cvHLL7/U2Pfjjz8Ww4YNE2FhYcLPz0907NhRpKamiueee87puizuNEaeVb3G0W+//SZuu+02ce6554rQ0FARGBgounbtKu6+++4a17+pqKgQDz74oOjUqZOwWCxOOeLuekdVebpO0/Lly8Vbb70levbsKfz9/UVkZKS49957RVFRUY370XVdpKWliejoaGG1WkW3bt3ECy+8IHbv3u0yhuLiYjF58mQRGRkpNE1zeg48xb1nzx4xadIkERkZKSwWi4iMjBSTJk0Se/bsqbGvp2tY2f9W7X+H+fn5Ii0tTQwaNEhERkYKq9UqOnToIEaNGiW+//57t89fdaWlpaJ169Zi9OjRNbbZn1dPPwsWLHCZT9V/3F3jyhVUuU5TdVlZWcJqtbq9tlpeXp6YMmWKiImJEX5+fqJt27biiiuuEJs3b66xb/XntDpP1+Zy9/jucmHDhg1ixIgRIiwsTAQHB4vBgweLn376ye2Y4Or+XY0ttT3v1eMoKSkRTz/9tOjXr59o0aKFCAwMFHFxceKSSy4R//nPf5yuRyVE5fvGxRdfLFq2bCmCg4PF6NGjxebNm90+d1lZWUJRFHHfffe5fN6IZKQI4WJuloiIiJqV6dOn48knn8TOnTsdh5IRnYlp06bh6aefxrZt2xAfH+/tcIgaBYsmIiIiQnFxMbp06YJx48bVa6VNoqry8/MRGxuLG2+8ES+88IK3wyFqNFwIgoiIiBAcHIz33nsPsbGxTbKqITUPWVlZ+Oc//1nvVVeJzI4zTURERERERB5wpomIiIiIiMgDFk1EREREREQesGgiIiIiIiLyoNld3NYwDGRnZyM4OBiKong7HCIiIiIi8hIhBIqLi9GhQwePF75udkVTdnY2oqOjvR0GERERERGZxP79+xEVFeV2e7MrmoKDgwFUPjGtWrXycjRUVzabDevXr0efPn1gsTS7tCVJME9JBsxTkgHzlM6WoqIiREdHO2oEd5pdFtoPyWvVqhWLJonYbDa0aNECrVq14uBJpsU8JRkwT0kGzFM622o7bafZXaepqKgIISEhKCwsZNEkESEESktLERgYyHPRyLSYpyQD5inJgHlKZ0tdawOunkfSsFqt3g6BqFbMU5IB85RkwDwlM2HRRFLQdR0ZGRnQdd3boRC5xTwlGTBPSQbMUzIbHiRKRERERGRSQgjYbDYWkGdI0zRYLJYGH+bJoomIiIiIyITKy8tx6NAhlJSUeDsUqQUFBSEyMrJBh3yyaCIiIiIiMhnDMJCVlQVN09ChQwdYrVYuilFPQgiUl5cjLy8PWVlZ6Nq1q8cL2HrC1fNICkII6LoOTdM4YJBpMU9JBsxTkgHzFDh58iSysrIQExODoKAgb4cjtZKSEuzduxdxcXEICAhw2sbV88jnlJeXezsEoloxT0kGzFOSAfO00pnOjNBpjfEc8lUgKei6jk2bNvEkSDI15inJgHlKMmCektmwaCIiIiIiIvKAC0EQEREREUkkLc23H8+MONNE0tA0zdshENWKeUoyYJ6SDJin8rrxxhtxySWXOP6vKAoURYGfnx/atWuH4cOH45133oFhGN4NtB5YNJEULBYLUlJSYLFwcpTMi3lKMmCekgyYp75l1KhROHToEPbs2YPvv/8eQ4YMwb333ou///3vsNls3g6vTlg0kRSEECgoKEAzWyGfJMM8JRkwT0kGzFPf4u/vj/bt26Njx47o27cvHn30UXz11Vf4/vvvsXDhQm+HVyfNrny3//EZhiHVlGBzZ7PZsHXrViQnJ/v0t0726WuSk67ryMzM9Pk8JbkxT0kGzFPfN3ToUCQlJeF///sfbrnlFm+HU6tmkYU2mw05OTkoKCjA0aNHERwcjI0bN6Jly5beDo3qyGazYefOndA0zecHz4CAAISFhaFdu3Y8npuIiIh8VkJCAjZt2uTtMOrEtz99ovLD9l9//YWysjK0bt0aLVu2RFxcHOLi4lg0ScRms6GiogJxcXE+XTQZhoHjx4/j8OHDKC4uRteuXXlROyIiIvJJQghpjrDx3U+fp+Tl5eHkyZNISEhAYGAgjh8/DovF4iigSA66riMiIgJt2rTx+dmXtm3bIjw8HNu3b8fRo0cRHh7u7ZCojhRFQWBgoDRvANQ8MU9JBszT5mHbtm2Ii4vzdhh14vNfYRcUFCA0NBSBgYHeDoUaQNM0dOvWzecLJrsWLVogODgY+fn53g6F6kHTNCQlJTWbPCU5MU9JBsxT37ds2TJs3rwZl19+ubdDqRNTzTStXLkSzzzzDP744w8cOnQIX3zxhWONd6ByCm/mzJl46623UFBQgAEDBuC1115D165d3d6n/bA8kpthGI4CuLkcrhYUFIRjx455Owyf4+kCfQ29eJ9hGDhy5Ajatm3bbPKU5MM8JRkwT31LWVkZcnJyoOs6Dh8+jB9++AHz5s3D3//+d9xwww3eDq9OTFU0nThxAklJSbjppptw2WWX1dj+9NNP48UXX8S7776LuLg4TJ8+HSNHjsTWrVsREBDg8j5lOlaS3DMMAwcPHkSrVq2azeCpKAqXWpWMYRjYvXs3Wrdu3WzylOTDPCUZME89a+iXfGfbDz/8gMjISFgsFoSFhSEpKQkvvvgiJk6cKM3ra6ooR48ejdmzZ+PSSy+tsU0Igfnz52PatGkYN24cEhMT8Z///AfZ2dn48ssv6/1Yt912m9Mslq9crfhM7du3D2PGjEFQUBAiIiLwwAMPeLzY2J49e3DzzTcjLi4OgYGBiI+Px8yZM1FeXu6036ZNmzBw4EAEBAQgOjoaTz/9dFN3pd59AYBjx47huuuuQ6tWrRAaGoqbb74Zx48fd2zfs2ePIz+q/vz2229N3R0iIiIiqSxcuNDx+XzhwoUQQkAIgYqKCuTm5mLJkiWYNGmSNAUTYLKZJk+ysrKQk5ODYcOGOdpCQkJw/vnnY/Xq1bj66qtd3q68vBwnTpxwfAA+ceIEADhePJvN5njBRo4cif/7v/9zTB0uWbIE9957Lz799FN88cUXjlXbVFWFqqrQdd1pJkDTNCiKUuMDuv14XF3X69RusVgghHBqVxQFmqbVuL6Uu3Z7jO7aq8au6zrGjBmD9u3b4+eff8ahQ4cwadIkaJqGefPmuezTtm3bYBgGXn31VcTHx+PPP//E7bffjuPHj+PZZ5+FrusoKirCiBEjMHToULz++uvYtGkTbr75ZgQHB2Py5Ml16tPy5ctx8803Y8eOHW6vsVW1TzabDRdffDHat2+PVatW4fDhw7jhhhugaRpmz57t9nW69tprkZOTgx9//BEnT57E5MmTMXnyZLz33nvQNM3x2IsXL8Y555zjeJ1at27tdD+N+TrZX5uq9+9rueeNPlVOPCsQQgNgQFFOx6jrDeuT/V/7Pnyd2Ccz9qlqvvpKn+oSO/skX5+q9sFX+lRVbX2y2WyOz6v2I6dcHYHS1O314a0Ya2uv+rm/6udfALV+sW4nTdGUk5MDAGjXrp1Te7t27RzbXPnvf/+LFStWwM/Pz6m9qKgIJSUl2LZtG0JDQwFUFlL2E+8DAwNxyy23oH///rjooovw5JNPOk5U69ixI1q3bo1du3ahrKwM06ZNQ3FxMQYOHIjXX38dpaWlmDBhAm655Ra88MIL+PrrrxEUFFRjdqtly5b417/+hSVLlkBRFPTt2xePPPIIRowYgePHj+Pbb7/Fiy++iMzMTNhsNvTt2xdpaWlo27at4z4SExPx1ltv4X//+x+WL1+OiIgI3H///bj00ksRHR2NgwcPoqCgwLF/REQE2rVrh3379jkKyZ9//hlbt27FTz/9hMLCQlitVtx+++2YP38+HnjgAbRp0wbbt293GmBSU1MxfPhwbNu2DWVlZejSpQuuu+46fPHFF5g9ezZ27tyJjz/+GKWlpXjwwQfRs2dPdOrUCddccw2efvppXHjhhfD390e3bt1QUFCAgwcPOj0vcXFxyMvLw969e1FRUYHMzEzHgOGpT99//z22bduGl19+GbGxsejbty/uvvtuPPvssxg/fjz8/PwQGxuL4OBgR592796NxYsX4+eff0ZycjK2bduGqVOn4q677sItt9yCwYMHo6ysDABQWFiI/Px8qKqKnj17ori4GHv27HHEUpc+5ebmOtpDQ0Pdvk5A5ayZ/bEBoHPnzoiIiMCWLVtQWlrqaE9ISEBoaCjWr1/vNPgmJibCarUiIyPDKf+Tk5NRXl7udG0ETdOQkpKCwsJCZGZmOtoDAwORlJSEI0eOYPfu3Y72kJAQ9OjRA9nZ2Thw4ICjPTw8HPHx8cjKykJeXp6jPSoqClFRUdixYwcKCwu91qe4OKC8PBAHDiQhOPgIwsNP92nHjob1SQiBEydO4NixY2jfvj1fJ/bJlH2y2Ww4ceIE1q1bh6SkJJ/oky++Ts29Txs3bnTkqcVi8Yk+ncnrFBAQgJKSEse2iooKp6N6LBYLAgICUFZW5vTh32q1wmq14uTJk04x+vv7w8/PD6WlpU6f6wICAmCxWFBSUuJUfAQGBkJVVcekg12LFi1gGIbT86IoClq0aAFd13Hy5ElHu6qqCAoKgs1mc/pMo2naWevTyZMnUV5eji1bttR4nar3zR1FmPSkCUVRnBaC+PXXXzFgwABkZ2cjMjLSsd/48eOhKAo+/vhjl/fz+++/IyIiwrFs84kTJ3DxxReje/fuOHHiBD7//HOoqoqbbroJ+fn5+Pzzzx23tX/D0Lt3b0RGRuKbb75xard/83DTTTfhyy+/xIQJEzBlyhSsXLkSt956K0aMGIGBAwdi/Pjx+PjjjzFr1izs2LEDUVFRqKioQL9+/XDBBRfgnnvugcViwdy5c7F+/Xps2rQJfn5++Omnn5CdnY1+/foBAObPn49vv/0W27ZtQ3BwMADAz88PUVFRePLJJ9GvXz+88sorWLhwIbKystC2bVsYhoHOnTvjhhtuwIwZM1x+a5KWloZvv/0WGzZscLRnZWWhW7du+OOPP9C3b986fZsyY8YM/Pjjj1i7di10XceNN96I4uJifP75545vU5YuXYrhw4cjNzcXrVu3rvNM086dO+v0DdHMmTPxzTff4I8//nC079y5E127dsWaNWvQp0+fGt8QLViwAA8++KBj0QX77E7Lli3x0Ucf4fLLL8eePXvQuXNnREdH4+TJk+jatSseeughjB07tsm+9crJycHhw4fRq1cvl33lt5Nn1qc5cwB3M03TpsnZJ8D3Xif2iX1in9in5t6nkpIS7Nu3D3FxcQgICOBMUwPaT548iaysLHTq1AktWrQAcPr1KCoqQps2bVBYWIhWrVq57Zs0M03t27cHABw+fNipaDp8+DB69+7t9nZWqxUtWrSocU0m+zkpVS+UWv13O/vViqtvs/8RqKqK1q1b46WXXoKqqujevTuee+45lJaWYtq0aQCARx99FE899RR+++03XH311fjoo49gGAbefvttx0IV7777LkJDQ5Geno4RI0Zg+PDhTo/35ptvIjQ0FL/88gv+/ve/O9pvvPFGXHfddQCAJ598Ei+//DIyMjIwatQoqKqK+Ph4REREOMVfdQnP3Nxcxwyevb1jx46O5xeA2wvK2tt37tyJV155Bc8++6zjeczNzXW6GK2iKOjQoQMA4MiRI45C1j4o2VV9rXRdR1lZGUJDQx0zTddffz1ef/31GrFomobc3Fy0b9/eKd6qj1m13f7/vLw8REREOF4Hi8XiOPQuLy8PiqIgODgYzz33HAYMGABVVfH555/jkksuwZdffol//OMfNWKp3qczbdc0zeVzX/X1q6q216ku7e7+DhqzT66crT45j6kqhDgdoz2EM+2TYRjIzs525BxfJ/bJjH2qmqdVxz13+1dnxj41tJ19Ml+fVFV15Kn98WXvU31fJ4vF4nQetX1/V5q6vT68FaOn9qqf+6uPe+5er+qkKZri4uLQvn17LF261FEkFRUV4ffff8cdd9zRpI9dlxX4evbs6fQH2K5dO5x77rmO3zVNQ5s2bRyHZm3cuBE7d+50zBjZnTx5Ert27QJQWbBMmzYN6enpyM3Nha7rjm8dqkpMTHT8v0WLFmjVqpXTIWBLly6tZ4/r5+DBgxg1ahSuvPJKTJ48ucH3t2HDBsf/f//9dzz00EP46aefsGvXLsTHx3tlCfm2bdti6tSpjt9TUlKQnZ2NZ555xmXRRM2TYRg4cOAA2rdv7/INmcgMmKckA+YpmY2piqbjx49j586djt+zsrKwYcMGtG7dGp06dcJ9992H2bNno2vXro4lxzt06OB0nlBTqMvViqufM6Uoiss2+3Tw8ePH0a9fP7z//vs17ss+AzNx4kQcPXoUL7zwAmJiYuDv74/+/fvXWKHO0+PURfv27bFmzRqnNvsMk32Gz53s7GwMGTIEf/vb3/Dmm2/WuF/7/dTnfrt06eL4/4EDB2CxWNClSxdUVFSgS5cuHr8ROJO+tG/f3qnIBCpPCrSfl+LO+eefjyVLlrjdTkRERES+wVSle0ZGBvr06YM+ffoAAKZOnYo+ffpgxowZAIAHH3wQ99xzD2699VakpKTg+PHj+OGHH9xeo6kxNNXVivv27Yu//voLERER6NKli9NPSEgIAOCXX37BlClTcPHFF6Nnz57w9/fHkSNHGjUOAOjfvz82b97sVDgsWbIErVq1cqwU58rBgweRmpqKfv36YcGCBTW+Cerfvz9WrlyJiooKp/vt3r07wsLCGr0f9sesb1/69++PgoIC/PHHH462ZcuWwTAMnH/++W4fa8OGDU6HihIRERGRbzJV0ZSamuq0LKD9Z+HChQAqZ1CeeOIJ5OTk4OTJk/jpp5/QrVu3Rnt8+9WKDx48iHXr1mHu3LkYN25ck1yt+LrrrkPbtm0xbtw4/Pzzz8jKykJ6ejqmTJniWOmla9eueO+997Bt2zb8/vvvuO666xAYGFjvx7rooovw8ssvu90+YsQInHPOOZgwYQI2btyIxYsXY9q0abjrrrvg7+8PAFizZg0SEhIcK8LZC6ZOnTrh2WefRV5eHnJycpxWMrz22mthtVpx8803488//8THH3+MF154wekwN1fs95OTk4OEhAT89ttvyM3NRXl5OXJzc51WtmmMvvTo0QOjRo3C5MmTsWbNGvzyyy+4++67cfXVVzvOTXn33Xfx4YcfIjMzE5mZmZg7dy7eeecd3HPPPXV4Bai5UFUV4eHhPJSETI15SjJgnpLZmOrwPG87m1crDgoKwsqVK/HQQw/hsssuQ3FxMTp27IiLLrrIsXLH22+/jVtvvRV9+/ZFdHQ05s6di3/961/1fqxdu3Z5nKHSNA3ffvst7rjjDvTv3x8tWrTAxIkT8cQTTzj2KSkpwfbt2x2zRkuWLMHOnTuxc+dOREVFOd2fffWSkJAQ/Pjjj7jrrrvQr18/tG3bFjNmzMCtt97qMd7aZm8mTpzoKKQboy8A8P777+Puu+/GRRddBFVVcfnll+PFF190uu9Zs2Zh7969sFgsSEhIwMcff4wrrrjCY6zUvNgXXiEyM+YpyYB5SmZj2iXHG8v69evRsWNHx3Vvjh8/jsGDB2PFihU1VtQj8zIMAwcPHkTHjh2bzbdO2dnZOHLkiNNCH9RwaWlntq0uDMNAVlYW4uLimk2eknyYpyQD5ikcy2Tblxx30tA3rPo624/XyDw9l0VFRQgJCal1yfHmmYUkHcMwUFBQUK8FLojONsMwkJeXxzwlU2OekgyYp75h//79uOmmm9ChQwdYrVbExMTg3nvvxdGjR532+/PPPzF+/HiEh4fD398f3bp1w4wZMxwX9rWLjY11LB8eGBiI2NhYjB8/HsuWLWvyvrBoIiIiIiKiRrV7924kJyfjr7/+wocffoidO3fi9ddfx9KlS9G/f38cO3YMAPDbb7/h/PPPR3l5ORYtWoQdO3Zgzpw5WLhwIYYPH15j1egnnngChw4dwvbt2/Gf//wHoaGhGDZsGOZUXr2+yfCcJiIiIiIialR33XUXrFYrfvzxR8dCZp06dUKfPn0QHx+Pxx57DK+++ipuvvlm9OjRA//73/8ch2LGxMSgW7du6NOnD/7973/joYcectxvcHCw45IwnTp1wqBBgxAZGYkZM2bgiiuuQPfu3ZukP5xpIimoqoqIiIhme1wzyUFVVURFRTFPydSYpyQD5qncjh07hsWLF+POO++ssfJz+/btcd111+Hjjz/Ghg0bsHXrVkydOrXGa52UlIRhw4bhww8/rPXx7r33Xggh8NVXXzVqP6piJpIUVFVFu3btOHiSqfFNnmTAPCUZME/l9tdff0EIgR49erjc3qNHD+Tn52PHjh2O393tZ9/Hk9atWyMiIgJ79uw545hrw8PzSAq6rmPfvn3o1KkTNE3zdjjkTU25/F0D6bqOHTt2oFu3bsxTMi3mKcmAeeob6rpId2Ms5i2EgKIoDb4fd5p1+V7XFT0AOVb1MIP09HT07dsX/v7+6NKli9vrKVXdf9y4cYiMjESLFi3Qu3dvvP/++077/Pnnn7jyyitx4YUXwmKxYP78+U3XgSo2bdqEgQMHIiAgANHR0Xj66adrvc2+ffswZswYBAUFISIiAg888ABsNpvTPmVlZXjssccQExMDf39/xMbG4p133mmqbtBZJIRAYWFhowz+RE2FeUoyYJ7KrUuXLlAUBdu2bXO5fdu2bQgLC0O3bt0cv7vbz76PJ0ePHkVeXh7i4uLOPOhaNNuiKSsrq04regDyrOrhbVlZWRgzZgyGDBmCDRs24L777sMtt9yCxYsXu73Nr7/+isTERHz++efYtGkTJk2ahBtuuAHffvutY5+SkhLExcXh3nvvdZz4V18LFy5EampqnfcvKirCiBEjEBMTgz/++APPPPMM0tLS8Oabb7q9ja7rGDNmDMrLy/Hrr7/i3XffxcKFCzFjxgyn/caPH4+lS5fi7bffxvbt2/Hhhx822UmLRERERGdbmzZtMHz4cLz66qsoLS112paTk4P3338fV111FXr37o2EhAT8+9//rrG8/MaNG/HTTz/hmmuuqfXxXnjhBaiqiksuuaQxu+Gk2RZN999/v2NFj8GDB6NTp04YPXo0fvrpJxw8eBCPPfYYgMpvOqqu6nHeeechJiYGV155Jb755husXr0a//73v53u276qh31FjzfffBPTp0/HjBkzsH37drcxxcbGYvbs2bjhhhvQsmVLxMTE4Ouvv0ZeXh7GjRuHli1bIjExERkZGU63W7VqFQYOHIjAwEBER0djypQpOHHihGP7e++9h+TkZEdc1157LXJzcx3b09PToSgKli5diuTkZAQFBeFvf/ubx1hdef311xEXF4fnnnsOPXr0wN13340rrriixvNT1aOPPopZs2bhb3/7G+Lj43Hvvfdi1KhR+N///ufYJyUlBU899RRGjx4Nf3//esV0pt5//32Ul5fjnXfeQc+ePXH11VdjypQpeP75593e5scff8TWrVvx3//+F71798bo0aMxa9YsvPLKK47C+ocffsCKFSvw3XffYdiwYYiNjUX//v0xYMCAs9IvIiIiorPh5ZdfRllZGUaOHImVK1di//79+OGHHzB8+HB07NgRc+bMgaIoePvtt7F161ZcfvnlWLNmDfbt24dPP/0UY8eORf/+/XHfffc53W9xcTFycnKwf/9+rFy5Erfeeitmz56NOXPmoEuXLk3XIeHj1q1bJw4fPuz4vbi4WCQmJgpFUcTcuXNd3mby5MkiLCxMGIYh1q1bJwCIDz74wOW+w4cPF0lJSY7fY2JixL///e8a+x09elQoiiKeeuopt7HGxMSI1q1bi9dff13s2LFD3HHHHaJVq1Zi1KhR4pNPPhHbt28Xl1xyiejRo4cwDEMIIcTOnTtFixYtxL///W+xY8cO8csvv4g+ffqIG2+80XG/b7/9tvjuu+/Erl27xOrVq0X//v3F6NGjHduXL18uAIjzzz9fpKeniz///FMMHDhQ/O1vf3Psk5WVJQCI5cuXu41/4MCB4t5773Vqe+edd0SrVq3c3saVAQMGiPvvv9+pTdd1cfToUbfPb20WLFggBg8eXOf9J0yYIMaNG+fUtmzZMgFAHDt2zOVtpk+f7pQLQgixe/duAUCsW7dOCCHEHXfcIS666CLx0EMPiQ4dOoiuXbuK+++/X5SUlNS4v4MHD4qNGzfWOeZmY+ZM9z9Nf3OPdF0Xhw8fFrquN/zOiJoI85RkwDwVorS0VGzdulWUlpZ6O5QztmfPHjFx4kTRrl074efnJ6Kjo8U999wjjhw54rTfpk2bxOWXXy5at24t/Pz8RHx8vJg2bZo4ceKE034xMTECgAAgrFar6NSpkxg/frxYtmyZxzg8PZeFhYUCgCgsLPR4H81yIYiysrI6reiRl5dXp1U9Vq1aVetj1nVVj4svvhi33XYbAGDGjBl47bXXkJKSgiuvvBIA8NBDD6F///44fPgw2rdvj3nz5uG6665zVOFdu3bFiy++iMGDB+O1115DQEAAbrrpJsf9d+7cGS+++CJSUlJw/PhxtGzZ0rFtzpw5GDx4MADg4YcfxpgxY3Dy5EkEBATAz88P3bt3R1BQkNvYc3Jy0K5dO6e2du3aoaioCKWlpTWWnHTlk08+wdq1a/HGG284tauqitatW9d6e7t9+/bhnHPOcfxus9lQUVHh1N9HH30Ujz76qNu+VD8u1t63nJwchIWFubyNq/7btwGVF3pbtWoVAgIC8MUXX+DIkSO48847cfToUSxYsKDO/SNzsi+NT2RmzFOSAfPUN8TExNR6fjsA9OrVC5999lmt+zXl6ni1aZZFk52ox8mF9dnX033UtqpHYmKi4//2D9y9evWq0Zabm4v27dtj48aN2LRpk9PiCUIIGIaBrKws9OjRA3/88QfS0tKwceNG5OfnO44ZrV5YVH3syMhIx+N06tQJHTt2RGZm5pl2vU6WL1+OSZMm4a233kLPnj2dtum6jl27dtX5vjp06IANGzY4fv/f//6Hzz//3Ol5qk8R1lgMw4CiKHj//fcREhICAHj++edxxRVX4NVXX61TYUnmpes6tmzZgnPPPZerPZFpMU9JBsxTMptmWTT5+/s7VvS49NJLa2y3r+gRHh7utKpHnz59XO7bmKt6+Pn5Of5vL7BctdkLn+PHj+O2227DlClTatxXp06dcOLECYwcORIjR47E+++/j/DwcOzbtw8jR46ssYCFp8epi/bt2+Pw4cNObYcPH0arVq1qLQZWrFiBsWPH4t///jduuOGGGtuFECgrK6tzLBaLxem41oiICAQGBtb5WFd3fbFvc3ebNWvWeLxNZGQkOnbs6CiYgMrZSiEEDhw4gK5du9YpPjInIQRKS0u52hOZGvOUZMA8JbNplgtBWCwWDB06tNYVPRRFMf2qHn379sXWrVvRpUuXGj9WqxWZmZk4evQonnzySQwcOBAJCQlOi0A0pv79+2Pp0qVObUuWLEH//v093i49PR1jxozBU089hVtvvbVJYquv/v37Y+XKlaioqHC0LVmyBN27d3d5aJ79Nps3b3Z6fpcsWYJWrVo5ZvQGDBiA7OxsHD9+3LHPjh07HBfxIyIiIiLzaZZFEwA8++yzta7oAcD0q3o89NBD+PXXX3H33Xdjw4YN+Ouvv/DVV1/h7rvvBlA522S1WvHSSy9h9+7d+PrrrzFr1qx6P87BgweRkJBQYyalqttvvx27d+/Ggw8+iMzMTLz66qv45JNP8M9//tOxz8svv4yLLrrI8fvy5csxZswYTJkyBZdffjlycnKQk5PjtOR7eXk5NmzYgMzMTJSXl+PgwYPYsGEDdu7c6TYWXdcd95WTk4NRo0bho48+cmqrWrhUd+2118JqteLmm2/Gn3/+iY8//hgvvPACpk6d6tjniy++QEJCguP3ESNG4JxzzsGECROwceNGLF68GNOmTcNdd93lWPXv2muvRZs2bTBp0iRs3boVK1euxAMPPICbbrqJh+YRERERmVSzLZq6dOmCjIwMdO7cGePHj0d8fDxuvfVWDBkyBKtXr3Y63+Vvf/sbfvvtN2iahtGjR6NLly545JFHMHHiRCxZsqTGMtgzZsxAZGQkunTpggkTJqCwsBBLly7FQw891Oj9SExMxIoVK7Bjxw4MHDgQffr0wYwZM9ChQwcAQHh4OBYuXIhPP/0U55xzDp588kk8++yz9X6ciooKbN++vcbFfKuKi4vDokWLsGTJEiQlJeG5557D//3f/2HkyJGOfY4cOeJ0btK7776LkpISzJs3D5GRkY6fyy67zLFPdnY2UlJSMH78eBw6dAjPPvss+vTpg1tuucVtLPv373e6P1c/np6HkJAQ/Pjjj8jKykK/fv1w//33Y8aMGU4zYYWFhU7Lsmuahm+//RaapqF///64/vrrccMNN+CJJ55w7NOyZUssWbIEBQUFSE5OxnXXXYexY8fixRdfdBsLyUPTNCQkJPD4ezI15inJgHl6Gg9RbLjGeA4V4eOvxPr169GxY0fHCizHjx/H4MGDsWLFCqeV1IjMJjs7G0eOHHFaoIMApKWd2bbGuTkREdFZoes6duzYgYiICLRp08bb4Ujt6NGjyM3NRbdu3WoU4kVFRQgJCUFhYSFatWrl9j6a5UIQJB+bzYbt27eje/fusFiYtmRONpsN69evR58+fZinZFrMU5IB87Ryti00NNRxrnRQUFCtqzCTMyEESkpKkJubi9DQ0AbNXPp8FiqKUq8V4Mi8mtvraBgGVLXZHkErLV3XvR0CUa2YpyQD5unp1XebahGv5iI0NNTt6sd15fNFU2BgoMcT/onM6vjx4wgICPB2GGeFu8PiPB0ul57uoi2t9tsRERHJQlEUREZGIiIiwmlFX6o7Pz+/Rjk3zueLprCwMBw4cABFRUUej1MkMpP8/HycOHGi1ut6ERERke/TNI2LYniZzxdN4eHhKCoqwl9//YWWLVtCURSUl5fj0KFDaNGihbfDozoSQiAoKAiHDx/26eN5DcPA8ePHceLECbRu3drtNaHInDRNQ2JiIt/YyNSYpyQD5imZjc8XTYqiID4+HkePHkVBQQGOHDmCv/76C0eOHMHJkye9HR7VkRACuq7jxIkTPl00KYqCwMBAxMXFISwszKf76qusVqu3QyCqFfOUZMA8JTPx+aIJqPwg2rZtW7Rt2xYREREoKytDz549ebieRGw2GzIyMpCcnNxsV9Eh89N1nXlKpsc8JRkwT8lsuDQXERERERGRByyaiIiIiIiIPOB8JxGRTDytp8611omIiJoEZ5pICpqmITk5mavokKkxT0kGzFOSAfOUzIZFE0mjvLzc2yEQ1Yp5SjJgnpIMmKdkJiyaSAq6rmPTpk3Qdd3boRC5xTwlGTBPSQbMUzIbntNERKbl6hSd1PSzHQURERE1d5xpIiIiIiIi8oBFE0mDJ4OSDJinJAPmKcmAeUpmwsPzSAoWiwUpKSneDoPII+YpyYB5SjJgnpLZcKaJpCCEQEFBAYQQ3g6FyC3mKcmAeUoyYJ6S2bBoIinouo7MzEyuokOmxjwlGTBPSQbMUzIbFk1EREREREQe8JwmIiJvcLWeel22ERER0VnHmSaSgqIoCAwMhKIo3g6FyC3mKcmAeUoyYJ6S2XCmiaSgaRqSkpK8HQaRR8xTkgHzlGTAPCWz4UwTScEwDOTm5sIwDG+HQuQW85RkwDwlGTBPyWxYNJEUDMPA7t27OXiSqTFPSQbMU5IB85TMhkUTERERERGRByyaiIiIiIiIPGDRRFJQFAUhISFcRYdMjXlKMmCekgyYp2Q2XD2PpKBpGnr06OHtMIg8Yp6SDJinJAPmKZkNZ5pICoZh4MCBAzwhlEyNeUoyYJ6SDJinZDacaSIp2AfP9u3bQ1VZ61PjSk1Pq/xPmouNaa4aXWOekgyYpyQD5imZDbOQiIiIiIjIAxZNREREREREHrBoIimoqorw8HBO0ZOpMU9JBsxTkgHzlMyG5zSRFFRVRXx8vLfDoFp4Ov2nHqcGSYt5SjJgnpIMmKdkNizfSQqGYWDXrl1cRYdMjXlKMmCekgyYp2Q2LJpICoZhIC8vj4MnmRrzlGTAPCUZME/JbFg0ERERERERecCiiYiIiIiIyAMWTSQFVVURFRXFVXTI1JinJAPmKcmAeUpmw9XzSAr2wZPIzJinJAPmKcmAeUpmw6KJpKDrOnbs2IFu3bpB0zRvh0PNST3WUWeekgyYpyQD5imZDec8SQpCCBQWFkII4e1QiNxinpIMmKckA+YpmQ2LJiIiIiIiIg9YNBEREREREXnAc5pICqqqonPnzlxFh0zNVZ66OyUqNR1ITT0bURE543hKMmCektmwaCIpqKqKiIgIb4dB5BHzlGTAPCUZME/JbFi+kxR0XcfGjRuh67q3QyFyi3lKMmCekgyYp2Q2LJpICkIIlJaWchUdMjXmKcmAeUoyYJ6S2bBoIiIiIiIi8oBFExERERERkQcsmkgKmqYhISGBVwUnU2OekgyYpyQD5imZDVfPIykoioLQ0FBvh0HkEfOUZMA8JRkwT8lsONNEUrDZbFi7di1sNpu3QyFyi3lKMmCekgyYp2Q2LJpIGlx2lGTAPCUZME9JBsxTMhMWTURERERERB6waCIiIiIiIvJAqqJJ13VMnz4dcXFxCAwMRHx8PGbNmsULnzUDmqYhMTGRq+iQqTFPSQbMU5IB85TMRqrV85566im89tprePfdd9GzZ09kZGRg0qRJCAkJwZQpU7wdHjUxq9Xq7RCIasU8JRkwT0kGzFMyE6lmmn799VeMGzcOY8aMQWxsLK644gqMGDECa9as8XZo1MR0XUdGRgZPCiVTY56SDJinJAPmKZmNVDNNf/vb3/Dmm29ix44d6NatGzZu3IhVq1bh+eefd3ubsrIylJWVOX4vKioCULmUpX0ZS1VVoaoqDMOAYRiOfe3tuq47HQLorl3TNCiKUmN5TPvUcvU/fHftFosFQgindkVRoGlajRjdtftan6rG5St9qhqjr/RJUQBAhRAqFMUAcHp/w3DfJ0CFougATrcLoQFQoCjOfXLsYVGc2mETAAQURUfVp6F6nypjVE7dvwFFMSBO3ZeuAJoQMAAYyun7VwGoQsBQlCo9AlTDcOqT/d+qeasorvskLApspx5CO/V86PbHPNUBl6+TosAiBETV/QEoADTU/PtoLrnHPtW9T1Xz1Vf6VJfY2Sf5+lS1D77Sp6rYJ3P0qa7L2ktVND388MMoKipyXCFa13XMmTMH1113ndvbzJs3D48//niN9vXr16NFixYAgPDwcMTHxyMrKwt5eXmOfaKiohAVFYUdO3agsLDQ0d65c2dERERgy5YtKC0tdbQnJCQgNDQU69evd0qWxMREWK1WZGRkOMWQnJyM8vJybNq0ydGmaRpSUlJQWFiIzMxMR3tgYCCSkpJw5MgR7N6929EeEhKCHj16IDs7GwcOHHC0+1qfhBCOuHylT4DvvU5xcUBxcTjy8uLRtm0WgoNP9yk7232fgAh07LgFVuvpPh06lIDS0lDExKyHqp7uk8jwg1JqQ8mYOKc+BS3Kgp9fKaKjN8HeXVd9iosDyssDceBAEoKDjyA8fDdKWlXe147AEvTIyUF2WBgOhIWdfp2KixGfl4estm2RFxx8+nXKznbqkxACBQUFOHr0KCIjI7FlyxbExbnuU0mrOGS0OfU67d8Pq82GjLhTfTrVAZevU0wMUvbsQWFgIDIjI0+/TuXlSAKabe6xT3Xvk81mQ0FBAdatW4ekpCSf6JMvvk7NvU8bN2505KnFYvGJPvni6+QLfTpx4gTqQhESraLw0Ucf4YEHHsAzzzyDnj17YsOGDbjvvvvw/PPPY+LEiS5v42qmKTo6GkePHkWrVq0AsEqXoU+6rmPdunVISUlx9Ev2PlWN0VdepzlzAHczTdOnu+/TE0/UfaZp0PLZp4KtOdO0InUmFEXHY4+571NljM4zTYN+nlN53wPrOdM0fXqNmaZ169ahX79+sFqt0HUds2e77tOgn+dg4MDK9hozTac64PJ1mjPH/UzTzJnNNvfYp7r3yZ6nffv2dZwzInuf6hI7+yRXn8rLyx15qmmaT/TJF18nX+hTUVER2rRpg8LCQkdt4IpUM00PPPAAHn74YVx99dUAgF69emHv3r2YN2+e26LJ398f/v7+NdotFgssFufu25/06uxPbl3bq9/vmbQriuKy3V2M9W2XrU/2by/sfwC+0Ke6tMvWp6pfwQhRedjd6f08x15ZULhqd47FUSbYXH3fo0AIC6qHX7VPzl8TnSrwTt2XJuytlUVSdaoQzieCVslP+7/2PLX/7uprKSEsUGwClmrbLPadq3XA6fU4tY9SdX+nkJpn7tWlnX2qjLH6eOoudnftZuxTQ9vZJ/P1yWq11shT2fvki6+TL/TJ3fYa8dRpL5MoKSmp8eTZK1TyfeXl5d4OgahWzFOSAfOUZMA8JTORqmgaO3Ys5syZg0WLFmHPnj344osv8Pzzz+PSSy/1dmjUxHRdx6ZNm2pMAxOZCfOUZMA8JRkwT8lspDo876WXXsL06dNx5513Ijc3Fx06dMBtt92GGTNmeDs0IiIiIiLyUVIVTcHBwZg/fz7mz5/v7VCIiIiIiKiZkOrwPGre3J3gR2QmzFOSAfOUZMA8JTORaqaJmi/7NRqIzIx5SjJgnpIMmKdkNpxpIinYLxoq0WXFqBlinpIMmKckA+YpmQ2LJpKCruvIzMzkKjpkasxTkgHzlGTAPCWzYdFERERERETkAYsmIiIiIiIiD1g0kRQURUFgYCAURfF2KERuMU9JBsxTkgHzlMyGq+eRFDRNQ1JSkrfDIPKIeUoyYJ6SDJinZDacaSIpGIaB3NxcGIbh7VCI3GKekgyYpyQD5imZDWeaSAqGYWD37t1o3bo1VJW1vozS0rwdQcOkp7toS6v81963s5WnrmIBgNQme0TyJRxPSQbMUzIbZiEREREREZEHLJqIiIiIiIg8YNFEUlAUBSEhIVxFh0yNeUoyYJ6SDJinZDY8p4mkoGkaevTo4e0wiDxinpIMmKckA+YpmQ1nmkgKhmHgwIEDXEWHTI15SjJgnpIMmKdkNiyaSAocPEkGzFOSAfOUZMA8JbNh0UREREREROQBiyYiIiIiIiIPWDSRFFRVRXh4OC9wR6bGPCUZME9JBsxTMhuunkdSUFUV8fHx3g6DyCPmKcmAeUoyYJ6S2bBoIikYhoGsrCzExcXxWydqMunpZ3a7tLTKfxXFQNu2WThyJA5CnP08tcdR323UvHA8JRkwT8lsmIUkBcMwkJeXx1V0yOQMBAfnAWCeknlxPCUZME/JbFg0ERERERERecCiiYiIiIiIyAMWTSQFVVURFRXF45rJ1IRQkZ8f5ZXzmYjqiuMpyYB5SmbDhSBICvbBk8jcKosmIjPjeEoyYJ6S2bBoIinouo4dO3agW7du0DTN2+FQHaWmp7ndlp7qfluDH8/VXTfB8nHV+yc0BWXntYP/msNQdNEkfWwSXHavWeF4SjJgnpLZcM6TpCCEQGFhIYQQ3g6FyD0F0COCAMXbgRC5x/GUZMA8JbNh0UREREREROQBiyYiIiIiIiIPWDSRFFRVRefOnbmKDpmbLmBdnwfoPJyEzIvjKcmAeUpmw4UgSAqqqiIiIsLbYRC4LoAnigD89hV7OwwijziekgyYp2Q2LN9JCrquY+PGjdB13duhELklNAWlQ6MgNK4EQebF8ZRkwDwls2HRRFIQQqC0tJSr6JC5KYARbOXqeWRqHE9JBsxTMhsWTURERERERB6waCIiIiIiIvKAC0GQFDRNQ0JCAq8KbkKp6WlS3OdZoQsErD7U8NXzuNoGNSGOpyQD5imZDYsmkoKiKAgNDfV2GEQeKQLQcku9HQaRRxxPSQbMUzIbHp5HUrDZbFi7di1sNpu3QyFyS1gUlIyJhbBwJQgyL46nJAPmKZkNiyaSBpcdJRkIC4dVMj+OpyQD5imZCd/diYiIiIiIPOA5TURUg+zrEKSnu2hLO9tR1M5VnHapqWcrCiIiIqoNZ5pICpqmITExkavokLnZBAKX7QdsvBgjmRfHU5IB85TMhkUTScNqtXo7BKJaKaU8aZnMj+MpyYB5SmbCoomkoOs6MjIyeFIomZtFQcmYOICr55GJcTwlGTBPyWxYNBEREREREXnAoomIiIiIiMgDrp5HRM1CqhmXzyMiIiIpcKaJpKBpGpKTk7mKDpmbTSBoURZXzyNT43hKMmCektmwaCJplJeXezsEolqJQE7gk/lxPCUZME/JTFg0kRR0XcemTZu4ig6Zm0VB6dBorp5HpsbxlGTAPCWzYdFERERERETkAYsmIiIiIiIiD1g0kTR4MijJQLEZ3g6BqFYcT0kGzFMyE56xTFKwWCxISUnxdhhEHik2gaBFe7wdBpFHHE9JBsxTMhvONJEUhBAoKCiAEFzKmcxLKIAeEQjBdSDIxDiekgyYp2Q2LJpICrquIzMzk6vokLlpCk72jwQ0Vk1kXhxPSQbMUzIbFk1EREREREQesGgiIiIiIiLygEUTSUFRFAQGBkJReNgTmZgA1OJygIfgk4lxPCUZME/JbLh6HklB0zQkJSV5OwwijxRdIHDZAW+HQeQRx1OSAfOUzIYzTSQFwzCQm5sLw+A1cMi8hAJUdArm6nlkahxPSQbMUzIbFk0kBcMwsHv3bg6eZG6agvI+4Vw9j0yN4ynJgHlKZsOiiYiIiIiIyAMWTURERERERB6waCIpKIqCkJAQrqJD5iYALbeEq+eRqXE8JRkwT8lsuHoeSUHTNPTo0cPbYRB5pOgCAatzvB0GkUccT0kGzFMyG840kRQMw8CBAwd4QiiZmlCB8u5hEBxZycQ4npIMmKdkNpxpIinYB8/27dtDVfmJ9GxLTU/zdghyUBVUJITBb1cBYPjIMXppaWd3GzU5jqckA+YpmQ2zkIiIiIiIyAMWTURERERERB6waCIpqKqK8PBwTtGTuRmAZW8xwEPwycQ4npIMmKdkNjyniaSgqiri4+O9HQaRR4oh4L8hz9thEHnE8ZRkwDwls2HRRFIwDANZWVmIi4vjt071wHPxzy6hKihPbAvrpiNQJFoIIj3d/bbU1LMVBZ0tHE9JBsxTMhtmIUnBMAzk5eVx6VEyNxWwxQRzZCVT43hKMmCektnwrZ2IiIiIiMgD6YqmgwcP4vrrr0ebNm0QGBiIXr16ISMjw9thERERERGRj5LqnKb8/HwMGDAAQ4YMwffff4/w8HD89ddfCAsL83Zo1MRUVUVUVBSPayZzMwT8MvN958K25JM4npIMmKdkNlIVTU899RSio6OxYMECR1tcXJwXI6KzxT54UhOqsjJEarrXopCaYgDW7fneDsP8uEKJV3E8JRkwT8lspCqavv76a4wcORJXXnklVqxYgY4dO+LOO+/E5MmT3d6mrKwMZWVljt+LiooAADabDTabDUDlH6aqqjAMw+mEQ3u7rusQQtTarmkaFEVx3G/VdgDQdb1O7RaLBUIIp3ZFUaBpWo0Y3bX7Wp8Mw8DOnTvRvXt3KIriE32qGmNTvU6AdurxnWMXQoMQ1fqkKLAIAQFAWJQqOwOKLiAUAJqLdhWAWqXdqFx6W6iK8wHAhoBiAEJTAAVQFNupWFQA6qkYK/skLAqgCyiiWiwAYDvV7zq2KzYPsTdyn4SmoCw5Av6/H4ZqE059quyrBkCBotic++UidpsCaKdeY12p1le4jt0erKJUPXFagRCec88plmp9sp3apAoB9VQsjoey2dz/PZ36t3rs7vpkzz29yv2Y7e/JV8YIwzDw119/oWvXrvDz8/OJPtUldvZJrj5VVFQ48lRVVZ/oky++Tr7Qp5qfnVyTqmjavXs3XnvtNUydOhWPPvoo1q5diylTpsBqtWLixIkubzNv3jw8/vjjNdrXr1+PFi1aAADCw8MRHx+PrKws5OWdvsZKVFQUoqKisGPHDhQWFjraO3fujIiICGzZsgWlpaWO9oSEBISGhmL9+vVOyZKYmAir1Vrj3Kvk5GSUl5dj06ZNjjZN05CSkoLCwkJkZmY62gMDA5GUlIQjR45g9+7djvaQkBD06NED2dnZOHDggKPd1/okhEBpaSm6deuGPXv2+ESfgKZ/nfz8EmGzWREX59ynrKxklJZW61NMDFL27EFhYCBKxkQ62tXicgQuOwBbdDDK+4Sf3j+3BAGrc1DRNQwVCacPkbXsLYb/hjyUJ7atXEnOHktmPqzb81F2XjvoEUGIa1MZU15eZxQXR6Bjxy2wWiv7VNIqDgGrD0HLLUXpyBgIy+lKJXDZfiilNpSMcZ5lDlqUBRFoQenQaEebYjMQtGgPjPBAnOzf9H2CAugRQbBkl0DNKnLqEwAcOpSA0tJQxMSsR0mr0/G76lNGGyA5KwvlFgs2RZ/uk2YYAFz3CQYQHHwE4eGnc6+kJAQ5OZ5zr+rjVu9TRpvK9s55eYgoLsaWjh1RarWeCjLD/d+Tnx+sNhsyqh0N4K5P9tzLrPL3Z7a/J18ZI2w2GwoKClBUVISkpCSf6JMvvk7NvU8bN2505KnFYvGJPvni6+QLfTpx4gTqQhFVSzKTs1qtSE5Oxq+//upomzJlCtauXYvVq1e7vI2rmabo6GgcPXoUrVq1AsAqXYY+6bqOdevWISUlxdEv2ftUNcamep1mz3Y/0zRzZrU+zZnj+LY/fVXTz8qsHPjYqVhqzjQN+nmOnDNNFgUlo2IRtCgLaoXAiiHT4W6madDPczzGPnCg+1mZVctdx75i4Ey4m2maMcN97q0YMcttnwYOPLW/q5mmxx5z//c0e7bL2GudaZo27XTkJvt78pUxwj6e9u3bF9ZTBbDsfapL7OyTXH0qLy935KmmaT7RJ198nXyhT0VFRWjTpg0KCwsdtYErUs00RUZG4pxzznFq69GjBz7//HO3t/H394e/v3+NdovFAovFufv2J706+5Nb1/bq93sm7YqiuGx3F2N922Xsk3LqQ5Yv9am29sbqkxCuYq+2/6mBRkFloVFjf4HTH+6rthtwufCBYgjAxeU1FF24jKmyoDi1T5XHcRUL4DoWd+1uY2/kPtnvUzn1a9U+VSWExXW/qrRZqmy2uPhuy13sgHqqEK3W6iH3XL7ep/pkqbZJqxpLlfxx+3fj5ns5l31ycz9m+3vyhTHC/mHFPq76Qp8a0s4+ma9P9vy0F0yA/H3yxdfJF/rkbnuNeOq0lxujR4/GBx984DQV1pQGDBiA7du3O7Xt2LEDMTExZ+XxyXtUVUXnzp1d/vEQmYYuYF2fB+huCjoiE+B4SjJgnpLZNGimaffu3bj++uvRsmVLXHrppZgwYQIuuugixzdXje2f//wn/va3v2Hu3LkYP3481qxZgzfffBNvvvlmkzwemYeqqoiIiPB2GEQeKQLw21fcKPeVnt4od0NUA8dTkgHzlMymQeX79u3b8fvvv2PSpEn48ccfMXLkSERFReGBBx7Ahg0bGinE01JSUvDFF1/gww8/xLnnnotZs2Zh/vz5uO666xr9schcdF3Hxo0baxw7S2QmQlNQOjSq8nwgIpPieEoyYJ6S2TR4zjMlJQUvvPACDh48iO+++w5Dhw7FG2+8gX79+uHcc8/F008/7bQiRkP9/e9/x+bNm3Hy5Els27bN43Lj5Dvsq+dJtG4JNUcKYARbK0/OITIpjqckA+YpmU2jHSiqqipGjhyJ9957D/v27cMVV1yBrVu34uGHH0ZsbCyGDRuGRYsWNdbDERERERERnRWNenbdqlWrcPvtt6NLly749NNPHTNNzz33HPLy8vCPf/wDM2bMaMyHJCIiIiIialINXnJ869at+O9//4sPP/wQ+/btQ0REBCZOnIgJEyagd+/ejv3uvfde3HrrrXjllVfwxBNPNPRhqZnRNA0JCQlul5Mk+aSmp3k7hManCwSsPuS11fM8P6eetlFzwvGUZMA8JbNpUNHUu3dvbN68Gf7+/hg3bhxeffVVjBw50u3ykEOGDMH//d//NeQhqZlSFAWhoaHeDoPII0UAWu7ZuQQD0ZnieEoyYJ6S2TTo8LzQ0FC8+eabyMnJwYcffojRo0d7XE9/3LhxyMrKashDUjNls9mwdu3aGld1JjITYVFQMiYWwsKVIMi8OJ6SDJinZDYNmmn6z3/+g/DwcAQGBrrcXlpairy8PHTq1AkAEBQUxAvR0hnjsqMkA2HhhRjJ/DiekgyYp2QmDXp3j4uLwxdffOF2+9dff424uLiGPAQREREREZFXNahoqm3t/IqKCo+H6xEREREREZldvQ/PKyoqQkFBgeP3o0ePYt++fTX2KygowEcffYTIyMgGBUgEVK6ik5iYyFV0yNxsAoHL9gM2XoyRzIvjKcmAeUpmU++i6d///rdjyXBFUXDffffhvvvuc7mvEAKzZ89uUIBEdlar1dshENVKKeVJy2R+HE9JBsxTMpN6F00jRoxAy5YtIYTAgw8+iGuuuQZ9+/Z12kdRFLRo0QL9+vVDcnJyowVLzZeu68jIyEBycjIslgZfXoyoaVgUlIyJQ9CiLM42kWlxPCUZME/JbOqdhf3790f//v0BACdOnMBll12GXr16NXpgREREREREZtCg0n3mzJmNFQcREREREZEp1atoeuKJJ6AoCh577DGoquo4t8kTRVEwffr0Mw6QiIiIiIjIm+pVNKWlpUFRFDz00EOwWq1IS0ur9TYsmqgxaJqG5ORkrqJD5mYTPJ+JTI/jKcmAeUpmU6+LKBmGAV3XHauZGIZR6w+v5kyNpby83NshENVKBPKEZTI/jqckA+YpmQmvPEtS0HUdmzZtYhFO5mZRUDo0GrAo3o6EyC2OpyQD5imZTaN/JVpSUoKPPvoIZWVluPjiixETE9PYD0FERERERHTWNKhouvnmm/H7779jy5YtACqnUS+44ALH7yEhIVi2bBn69OnT8EiJiIiIiIi8oEGH5y1fvhyXXXaZ4/cPPvgAW7Zswfvvv48tW7agffv2ePzxxxscJBEAngxKUlBshrdDIKoVx1OSAfOUzKRBRVNOTg5iY2Mdv3/55ZdITk7GNddcg3POOQeTJ0/G77//3tAYiWCxWJCSksKrgpOpKTaBoEV7oHD1PDIxjqckA+YpmU2DiqYWLVqgoKAAAGCz2ZCeno6RI0c6tgcHB6OwsLBBARIBgBACBQUFEIIfRsm8hALoEYEQXAeCTIzjKcmAeUpm06CiqW/fvnjrrbewfv16zJkzB8XFxRg7dqxj+65du9CuXbsGB0mk6zoyMzO5ig6Zm6bgZP9IQGPVRObF8ZRkwDwls2nQnOecOXMwcuRIJCcnQwiBK664Auedd55j+xdffIEBAwY0OEgiIhmlpqd5O4SmVYcLnBMREfmCBhVNycnJyMzMxK+//orQ0FAMHjzYsa2goAB33nmnUxsREREREZFsGnx2XXh4OMaNG1ejPTQ0FPfee29D754IAKAoCgIDA6EoPOyJTEwAanE5wEPwycQ4npIMmKdkNo2yJElxcTH27t2L/Px8lyfsDRo0qDEehpoxTdOQlJTk7TCIPFJ0gcBlB7wdBpFHHE9JBsxTMpsGFU1Hjx7F3Xffjc8//9zliXpCCCiKwpP4qMEMw8CRI0fQtm1bqGqD1i8hajJCAWzRwbDsL4bC2SYyKY6nJAPmKZlNg4qmyZMn45tvvsGUKVMwcOBAhIWFNVZcRE4Mw8Du3bvRunVrDp5kXpqC8j7hsGQfB3itJjIpjqckA+YpmU2DiqYff/wR//znP/H00083VjxEzZanhciaYpGy6veZmt74j0FkBu7+frj4HxER1VWDSvegoCDExsY2UihERERERETm06Ci6frrr8cXX3zRWLEQuaUoCkJCQriKDpmbALTcEq6eR6bG8ZRkwDwls2nQ4XlXXHEFVqxYgVGjRuHWW29FdHQ0NE2rsV/fvn0b8jBE0DQNPXr08HYYRB4pukDA6hxvh0HkEcdTkgHzlMymQUXThRde6Pj/kiVLamzn6nnUWAzDQHZ2Njp06MATQsm0hApUdA2D31/5UAxvR1MNT+ChUziekgyYp2Q2DSqaFixY0FhxEHlkGAYOHDiA9u3bc/Ak81IVVCSEwW9XAWDwGD0yJ46nJAPmKZlNg4qmiRMnNlYcREREREREptRopfuhQ4ewceNGnDhxorHukoiIiIiIyOsaXDR99dVXSEhIQFRUFPr27Yvff/8dAHDkyBH06dOHq+tRo1BVFeHh4ZyiJ3MzAMveYsBs5zMRVcHxlGTAPCWzaVAmfvPNN7jsssvQtm1bzJw5E0KcPoa/bdu26NixIxYuXNjQGImgqiri4+M5eJKpKYaA/4Y8KDyfiUyM4ynJgHlKZtOgTHziiScwaNAgrFq1CnfddVeN7f3798f69esb8hBEACpPCN21axcMg1/hk3kJVUFZ73AIldcVIfPieEoyYJ6S2TRoIYgtW7bg+eefd7u9Xbt2yM3NbchDEAGoHDzz8vIQExPDb53IvFTAFhMM65YjpjtELz3d2xGQWXA8JRkwT8lsGpSFQUFBHhd+2L17N9q0adOQhyAiIiIiIvKqBhVNQ4YMwbvvvgubzVZjW05ODt566y2MGDGiIQ9BRERERETkVQ0qmubMmYMDBw4gJSUFb7zxBhRFweLFizFt2jT06tULQgjMnDmzsWKlZkxVVURFRXGKnszNEPDLzOeFbcnUOJ6SDJinZDYNysTu3btj1apVaNOmDaZPnw4hBJ555hnMnTsXvXr1ws8//4zY2NhGCpWaMw6eJAPFAKzb86GY7Hwmoqo4npIMmKdkNg1aCAIAevbsiZ9++gn5+fnYuXMnDMNA586dER4e3hjxEQEAdF3Hjh070K1bN2ia5u1wiFwSmoKy89rBf81hKDpnm8icOJ6SDJinZDZnXDSVlZXhv//9L3788Ufs2rULxcXFCA4ORpcuXTBq1Chce+21sFqtjRkrNWNCCBQWFjpdC4zIdBRAjwgCuOI4mRjHU5IB85TM5oyKps2bN2PcuHHYu3cvhBAICQlBy5YtkZubi3Xr1uHTTz/FnDlz8PXXX6NHjx6NHTMREREREdFZU+8DRY8fP45//OMfOHz4MObMmYP9+/cjPz/f6d/Zs2cjOzsbY8eO9bgkORERERERkdnVu2hasGAB9u3bh0WLFuHhhx9Gx44dnbZ37NgRjzzyCL755htkZWVh4cKFjRUrNWOqqqJz5848IZTMTRewrs8DeD4TmRjHU5IB85TMpt6ZuGjRIowYMQKpqake9xs6dCiGDx+Ob7755kxjI3JQVRUREREcPMnUFAH47SuGwpqJTIzjKcmAeUpmU+9M3Lx5c60Fk93QoUOxefPm+j4EUQ26rmPjxo3Qdd3boRC5JTQFpUOjILTmsRJEerr7HzIvjqckA+YpmU29i6Zjx46hffv2ddq3Xbt2OHbsWL2DIqpOCIHS0lKuokPmpgBGsJWr55GpcTwlGTBPyWzqXTSVlZXBz8+vTvtaLBaUl5fXOygiIiIiIiKzOKMlx/fs2YN169bVul9WVtaZ3D0RNURamuO/qelei4KIiIjIZ5xR0TR9+nRMnz691v2EEFAUHqdCDadpGhISEnhVcDI3XSBg9SGunkemxvGUZMA8JbOpd9G0YMGCpoiDyCNFURAaGurtMIg8UgSg5ZZ6OwwijziekgyYp2Q29S6aJk6c2BRxEHlks9mwfv169OnTBxbLGU2QEjU5YVFQOjIGgYv3QrFxtonMieMpyYB5SmbDxe9JGlx2lGQgLBxWyfw4npIMmKdkJnx3JyIiIiIi8oBFExERERERkQcsmkgKmqYhMTGRq+iQudkEApftB3g+E5kYx1OSAfOUzIZn1pE0rFart0MwpSqXZQLAazN5m1Jq83YIppee7n5b6tkKopnjeEoyYJ6SmXCmiaSg6zoyMjJ4UiiZm0VByZg4wMLr05F5cTwlGTBPyWxYNBEREREREXnAw/OIiIgaU/VjZuu6jYiITIszTURERERERB6waCIpaJqG5ORkrqJD5mYTCFqUxdXzyNQ4npIMmKdkNiyaSBrl5eXeDoGoViKQRz2T+XE8JRkwT8lMWDSRFHRdx6ZNm7iKDpmbRUHp0GiunkemxvGUZMA8JbNh0UREREREROQBiyYiIiIiIiIPePA9SaM5nwzKVYrlodgMb4dAVKvmPJ6SPJinZCYsmkgKFosFKSkp3g6DyCPFJhC0aI+3wyDyiOMpyYB5SmbDw/NICkIIFBQUQAgu5UzmJRRAjwiE4DoQZGIcT0kGzFMyGxZNJAVd15GZmclVdMjcNAUn+0cCGqsmMi+OpyQD5imZjdRF05NPPglFUXDfffd5OxQiIiIiIvJR0hZNa9euxRtvvIHExERvh0JERERERD5MyqLp+PHjuO666/DWW28hLCzM2+HQWaAoCgIDA6EoPOyJTEwAanE5wEPwycQ4npIMmKdkNlKunnfXXXdhzJgxGDZsGGbPnu1x37KyMpSVlTl+LyoqAgDYbDbYbDYAgKqqUFUVhmHAME4vF2xv13Xd6UREd+2apkFRFMf9Vm0HUOO4XHftFosFQgindkVRoGlajRjdtftin3r16uVzfar6OinK6XYhVAAqFEVH1U/g7ts1AAoUxQZhqfIGYzu1j6Xam46bdsUmKhcxqHpOjgAU3UO7CkCt0m4AiiEgVMX5axlDQDEAoSlA1YfVBRThob2OsZulTwErDwJGZSy+0CfbqZuoQkAFoCuKI/OExX2fxKm99GofeDRxaoubPlX9u2ysca+ygzX/bgyjicaIU322nOpr1edA0XVTjOU9e/aEEMKxD9+f2Cez9UkI4chTm83mE33yxdfJF/pUfbs70hVNH330EdatW4e1a9fWaf958+bh8ccfr9G+fv16tGjRAgAQHh6O+Ph4ZGVlIS8vz7FPVFQUoqKisGPHDhQWFjraO3fujIiICGzZsgWlpaWO9oSEBISGhmL9+vVOyZKYmAir1YqMjAynGJKTk1FeXo5NmzY52jRNQ0pKCgoLC5GZmeloDwwMRFJSEo4cOYLdu3c72kNCQtCjRw9kZ2fjwIEDjnZf7FNgYCB69erlU32q+jrFxZ3uU35+FPLzo9Cu3Q4EBZ3uU15eZxQXR6Bjxy2wWk/36dChBJSWhiImZj1KWsWdjmfZfiilNpSMOd0GAEGLsiACLSgdGu1oU2wGghbtgREeWLmYwSlqcTkClx2ALToY5X3CTz8HuSUIWJ2Diq5hqEg4PeNr2VsM/w15KE9sC1tMsKPdLzMf1u35KDuvHfSIIEe7dX0e/PYV4+TgjjCCrY72gNWHoOWWonRkDITl9Kd6s/fJCLQgYPUh+O31jT5ltKls75yXh4jiYmzp2BGl1so+lbRy36fSkv2w2mzIiHPuU3JWFkSw+z5V/ftrrHEvLMz139ORI000RsTFQTMMpOzZg8LAQGRGnn6dArdsMcVYXl5eDqvV6vVxzxfHcvap8fpkz1Nf6pMvvk6y9+nEiROoC0VItJbj/v37kZycjCVLljjOZUpNTUXv3r0xf/58l7dxNdMUHR2No0ePolWrVgBYpcvQJ13XsW7dOqSkpDj6JXufqsZoGAZmzWqcmaZBP885HaSJZzAcfGimSVgUlIyKRdCiLKgVwif6NHBgZbOrmaaff3bfp9QL3c80pae779PAJdNOtzXSuDd7tuu/m+nTm2iMmFP5N+hypmnaNK+P5fbxtG/fvrCeKoD5/sQ+ma1P5eXljjzVNM0n+uSLr5Mv9KmoqAht2rRBYWGhozZwRaqZpj/++AO5ubno27evo03XdaxcuRIvv/wyysrKalw92t/fH/7+/jXuy2KxwGJx7r79Sa/O3RWp3bVXv98zaVcUxWW7uxjr2y5jn+zHNftSn6q2VxZEziqLoZrct1ug2ETNDa7a3LQrop7tBhyHozm3C8Co0QxFdx2L2/Z6xO6u/Wz2SRGn7he+0SdLtZtoVd6QlCrvQ9X7ZC8TLC6+l1PgPvamHPeq/93Yd2n0MaLqc4Rqz8Gpx/L2WG7/sGIfV/n+xD6ZrU/2/LQXTID8ffLF18kX+uRue43967SXSVx00UXYvHmzU9ukSZOQkJCAhx56yO2TRUREREREdKakKpqCg4Nx7rnnOrW1aNECbdq0qdFOvkVRFISEhHAVHTI3UXkOEVfPIzPjeEoyYJ6S2UhVNFHzpWkaevTo4e0wiDxSdIGA1TneDoPII46nJAPmKZmN9EVTenq6t0Ogs8AwDGRnZ6NDhw4uj28lMgOhAhVdw+D3Vz4UF+cJUR2kpTX+Nnja1vxwPCUZME/JbJiFJAXDMHDgwIFq114hMhlVqVzWW+XhJGReHE9JBsxTMhsWTURERERERB6waCIiIiIiIvKARRNJQVVVhIeH87hmMjcDsOwtdnndIyKz4HhKMmCektlIvxAENQ+qqiI+Pt7bYRB5pBgC/hvyvB0GkUccT0kGzFMyG5bvJAXDMLBr1y6eEEqmJlQFZb3DIbgQBJkYx1OSAfOUzIZFE0nBMAzk5eVx8CRzUwFbTDBHVjI1jqckA+YpmQ0PzyMiokbXFJfQ83iZpsZ/OCIiIgd+H0pEREREROQBiyaSgqqqiIqK4io6ZG6GgF9mPmAIb0dC5BbHU5IB85TMhofnkRTsgyeRmSkGYN2e7+0wqLF4PB7QwzaT43hKMmCektmwfCcp6LqObdu2Qdd1b4dC5JbQFJzs3x5C4+p5ZF4cT0kGzFMyGxZNJAUhBAoLCyEED3siE1MAPSIIYM1EJsbxlGTAPCWzYdFERERERETkAYsmIiIiIiIiD1g0kRRUVUXnzp25ig6Zmy5gXZ8H6DychMyL4ynJgHlKZsPV80gKqqoiIiLC22EQeaQIwG9fsbfDIPKI4ynJgHlKZsPynaSg6zo2btzIVXTI1ISmoHRoFFfPI1PjeEoyYJ6S2bBoIikIIVBaWspVdMjcFMAItnL1PDI1jqckA+YpmQ2LJiIiIiIiIg94ThORhFLT07wdAhEREVGzwZkmkoKmaUhISICmad4Ohcg9XSBg9SGunkemxvGUZMA8JbPhTBNJQVEUhIaGejsMIo8UAWi5pd4Og8gjjqckA+YpmQ1nmkgKNpsNa9euhc1m83YoRG4Ji4KSMbEQFq4EQebF8ZRkwDwls2HRRNLgsqMkA2HhsErmx/GUZMA8JTPhuzsREREREZEHLJqIiIiIiIg8YNFEUtA0DYmJiVxFh8zNJhC4bD9g4+p5ZF4cT0kGzFMyG66eR9KwWq3eDoGoVkqpb520nJ7u7QjMJz0dcHeptDQ37WbD8ZRkwDwlM+FME0lB13VkZGTwpFAyN4uCkjFxAFfPIxPjeEoyYJ6S2bBoIiIiIiIi8oCH5xERkRRS3R0TBwCpZysKIiJqjjjTRERERERE5AGLJpKCpmlITk7mKjpkbjaBoEVZXD2PTI3jKcmAeUpmw6KJpFFeXu7tEIhqJQJ51DOZH8dTkgHzlMyERRNJQdd1bNq0iavokLlZFJQOjebqeWRqHE9JBsxTMhsWTURERERERB6waCIiIiIiIvKARRNJgyeDkgwUm+HtEIhqxfGUZMA8JTPhGcskBYvFgpSUFG+HQeSRYhMIWrTH22EQecTxlGTAPCWz4UwTSUEIgYKCAgjBpZzJvIQC6BGBEFwHgkyM4ynJgHlKZsOZJpKCruvIzMxEcnIyLBamLZmUpuBk/0heq8lkUtPTXG9IA5DmZpsP43hKMmCektlwpomIiIiIiMgDFk1EREREREQesGgiKSiKgsDAQCgKTxYhExOAWlwO8Mg8MjGOpyQD5imZDQ8SJSlomoakpCRvh0HkkaILBC474O0wiDzieEoyYJ6S2XCmiaRgGAZyc3NhGLwGDpmXUICKTsFcPY9MjeMpyYB5SmbDoomkYBgGdu/ezcGTzE1TUN4nHNBYNZF5cTwlGTBPyWxYNBEREREREXnAoomIiIiIiMgDLgRBUlAUBSEhIdKvotMMr6PZvAhAyy3h6nkNkJ7u7QgaJi0NSE13vS019SwG4oGvjKfk25inZDYsmkgKmqahR48e3g6DyCNFFwhYnePtMIg84nhKMmCektmwaCIpGIaB7OxsdOjQAarKo0rJnIQKVHQNg99f+VB47rJPS01P83YIZ4zjKcmAeUpmwywkKRiGgQMHDnAVHTI3VUFFQhig8nASMi+OpyQD5imZDYsmIiIiIiIiD1g0ERERERERecCiiaSgqirCw8N5XDOZmwFY9hYDPJqETIzjKcmAeUpmw4UgSAqqqiI+Pt7bYRB5pBgC/hvyvB0GkUccT0kGzFMyG5bvJAXDMLBr1y6eEEqmJlQFZb3DIbgQBJkYx1OSAfOUzIZFE0nBMAzk5eVx8CRzUwFbTDBHVjI1jqckA+YpmQ3f2omIiIiIiDxg0UREREREROQBF4IgKaiqiqioKK6iQ+ZmCPhl5gOG8HYk1JykpdVrG8dTkgHzlMyGRRNJwT54EpmZYgDW7fneDoPII46nJAPmKZkNy3eSgq7r2LZtG3Rd93YoRG4JTcHJ/u0hNK6eR+bF8ZRkwDwls2HRRFIQQqCwsBBC8LAnMjEF0COCANZMZGIcT0kGzFMyGxZNREREREREHrBoIiIiIiIi8oBFE0lBVVV07tyZq+iQuekC1vV5gM7DSci8OJ6SDJinZDZcPY+koKoqIiIivB0GkUeKAPz2FXs7DCKPOJ6SDJinZDYs30kKuq5j48aNXEWHTE1oCkqHRnH1PDI1jqckA+YpmQ1nmkgKQgiUlpZyFR0yNwUwgq1cPY9cSk8H0tNcb/N0fdrGxvGUZMA8JbPhTBMREREREZEHnGkiampVvkJOTXfelJ6aBiLyorM5xUNERNLiTBNJQdM0JCQkQNM0b4dC5J4uELD6EFfPI1PjeEoyYJ6S2XCmiaSgKApCQ0O9HQaRR4oAtNxSb4dB5BHHU5IB85TMhjNNJAWbzYa1a9fCZrN5OxQit4RFQcmYWAgLV4Ig8+J4SjJgnpLZSFU0zZs3DykpKQgODkZERAQuueQSbN++3dth0VnCZUdJBsIi1bBKzRTHU5IB85TMRKp39xUrVuCuu+7Cb7/9hiVLlqCiogIjRozAiRMnvB0aERERERH5KKnOafrhhx+cfl+4cCEiIiLwxx9/YNCgQV6KioiIiIiIfJlURVN1hYWFAIDWrVu73aesrAxlZWWO34uKigBUHitrP05WVVWoqgrDMGAYhmNfe7uu604XV3PXrmkaFEWpcfytfeWX6tPM7totFguEEE7tiqJA07QaMbpr97U+CSFw7rnneuyrafukKFAAaEJAqABUpcr+BoRQoSgGgNN9EpU7QlF0AKJGu9AU5wuo6gKKQM1zaWynblvHdsUmIBQAWpV2ASi6h/ZqfYIBKIaAUBXnuWxDQDHgPnYf6JMAELDiAIQuoMA3+lQjdpP2yXaqG6oQMBSlyl8TIFTXfTIgoALQFQVV1zvUROXrZ1OcY9ROjSN17aurPimKDiE0AMapv/tKut6AsbxKnKqo1qdT41zVcU8IgZ49e0II4Rgb+f7EPpmtT1Xz1Gaz+USffPF18oU+1fW8OWmLJsMwcN9992HAgAE499xz3e43b948PP744zXa169fjxYtWgAAwsPDER8fj6ysLOTl5Tn2iYqKQlRUFHbs2OEo0ACgc+fOiIiIwJYtW1BaenqlrISEBISGhmL9+vVOyZKYmAir1YqMjAynGJKTk1FeXo5NmzY52jRNQ0pKCgoLC5GZmeloDwwMRFJSEo4cOYLdu3c72kNCQtCjRw9kZ2fjwIEDjnZf7FObNm3QpUsX+foUF4eQkhL0yMlBRdcwVCSEOfZva81CXl482rbNQnDw6T7l50chPz8K7drtQFDQ6T7l5XVGcXEETg7uCCPY6mgPWH0IWm4pSkfGOJ1TE7hsP5RSG0rGxDn1KWhRFkSgBaVDox1tis1A0KI9MMIDcbJ/pKNdLS5H4LIDsEUHo7xP+OnnILcEAatr9smytxj+G/JQntgWtphgR7tfZj6s2/NRdl476BFBjnbr+jz47Sv2nT4pgHVdHlRf6hPM/zpltAHCi4sRn5eHrLZtkRd8uk8V2a77dMTIQ0RxMbZ07IhS6+k+JRw6hNDSUqyPiYGunu5T4v79sNoa1qeOwVtw4EASgoOPIDz89Li3Y0cDxvK40/F0zqvWp1PjWfVxTwgBRVH4/sQ+mbpP9jz1pT754uske5/qepqPIqqWZBK544478P3332PVqlWIiopyu5+rmabo6GgcPXoUrVq1AsAqXYY+6bqOdevWISUlxdGv+vRp9uyqszUaAAWKUtmnxx5zH/ucOfb9cWrW57SZM+vYpzlzHDNNy1fC6dv+lQOnn9FM0+CfHzflt/0AfG4Goz59EhYFJaNiEbQoC2qF8Ik+1Yjdh/o0+MIzm2lKX3XmfVo5cJrLmaZp085sLH/8cR2Dfp59+kGrvE4DB8ExwFUdy+3jad++fWE9VSzy/Yl9MlufysvLHXmqaZpP9MkXXydf6FNRURHatGmDwsJCR23gipQzTXfffTe+/fZbrFy50mPBBAD+/v7w9/ev0W6xWGCxOHff/qRX5+7Cau7aq9/vmbQriuKy3V2M9W2XsU/KqQ8wZ9InV18NCGE5Fav72Kvezr5/VXXqU5U7qayNqhdBp4uhmjG67pPi5uKpis11O+rRroh6tlfr0+l2AadjpOzt7mL3kT4p4tT9wnf6VKd2Cftk/4vTXA0QACxu2uvT1+qxn/6bVh1//wCgaQDS0uB6JADUtDSX454Qmst4FF3AIlBjgLOPV/YPK/Zxle9P7JPZ+mTPT3vBBMjfJ198nXyhT+6219i/TnuZhBAC99xzD7744gukp6cjrsohCURERERERE1BqqLprrvuwgcffICvvvoKwcHByMnJAVB5TGRgYKCXoyMiIiIiIl8k1XWaXnvtNRQWFiI1NRWRkZGOn48//tjboVET0zQNycnJbqdeiUzBJhC0KMv9IWlEJsDxlGTAPCWzkWqmSdI1K6iRlJeXc0aRTE8EWqAUV3g7DCKPOJ6SDJinZCZSzTRR86XrOjZt2lRjlRYiU7EolctOV18xjshEOJ6SDJinZDYsmoiIiIiIiDxg0UREREREROQBiyaSBk8GJRkoNhcXCCIyGY6nJAPmKZmJVAtBUPNlsViQkpLi7TCIPFJsAkGL9ng7DCKPOJ6SDJinZDYsmqhO0tLq197YhBAoLCxESEiI4wr2Zlb1eUlN91YUdLYJBTDCA6HmlULhYp+ml57ufltq6tmK4uyrbTz19nhPBMj3vk++j0UTSUHXdWRmZiI5ORkWi++kbWp6mttt6anut5FJaQpO9o/ktZrI1Bzj6eLFsLi4lIfbL3rS7P+mudmBqPH46vs+yYvnNBEREREREXnAoomIiIiIiMgDFk0kBUVREBgYyOOaydwEoBaXAzwyj0zMMZ56OxAiD/i+T2bDoomkoGkakpKSuPwomZqiCwQuOwBFZ9VE5uUYT12cz0RkFnzfJ7Nh0URSMAwDubm5MAxeA4fMSyhARadgCH4xSibmGE+9HQiRB3zfJ7Nh0URSMAwDu3fv5uBJ5qYpKO8TDmismsi8HOMpD3siE+P7PpkNiyYiIiIiIiIPWDQRERERERF5wKuFkRQURTHdVcF5fUeqQQBabglXz/MB6enejqDpOMZTbwdC5IEZ3/epeWPRRFLQNA09evTwdhhnVWp6mrdDoHpSdIGA1TneDoNMzO3ftZvmBnPx7Y4GoHmNpmfA07di/MbsrGiO7/tkbjw8j6RgGAYOHDjAE0LJ1IQKlHcPg+DISiZmADgQFsbV88jU+L5PZsO3dpICB0+SgqqgIiEMUHk4CZmXoSiVRRMPeyIT4/s+mQ2LJiIiIiIiIg9YNBEREREREXnAoomkoKoqwsPDoapMWTIxA7DsLQZPFiEzUwGEFxfzAwCZGt/3yWy4eh5JQVVVxMfHezsMIo8UQ8B/Q563wyDySBUC8XnMUzI3vu+T2bB8JykYhoFdu3bxhFAyNaEqKOsdDsGFIMjEDEXBrvBwLgRBpsb3fTIbFk0kBcMwkJeXx8GTzE0FbDHBHFnJ1AwAecHBPIqUTI3v+2Q2fGsnIiIiIiLygOc0kYMMFzmfMwcQwvU2GeInInIlPb2W7WlnIwrP7DG6ioXjLxH5OhZNdHZ4eketw7utqqqIioqCEE08OVotltT00/9PT3XeRlSDIeCXmQ8Ybip7ojOU2ohVkyoEovLzobr7BorobHLzGUAFEHXLLVw9j0yDRRNJwV40EZmZYgDW7fneDoPIIxVAVD7zlMxNBfi+T6bC8p2koOs6tm3bBkXRvR0KkVtCU3Cyf3sIjauSkXnpioJt7dtD5+p5ZGK6omDbtm3Qdb7vkzmwaCIpCCFQWFgIgIeTkIkpgB4RBPCzKJmYAFAYFMTRlExNACgsLITgYaRkEiyaiIiIiIiIPGDRRERERERE5AGLJpKCqqro3Llz06+eR9QQuoB1fR6g83ASMi9VCHTOy+PqeWRqqhDo3LkzV88j0+DqeSQFVVURERHh7TCIPFIE4Lev2NthEHmkAogoZp6SuakA3/fJVFi+kxR0XcfGjRu5eh6ZmtAUlA6N4up5ZGq6omBjVBRXzyNT0xUFGzdu5Op5ZBosmkgKQgiUlpaCq+eRqSmAEWzl6nlkagJAqdXK0ZRMTQAoLS3l6nlkGjw8jxrEzYW8a0hNr/Z7aiMHQkRETtLTXbcLC1DSCvj5Z0CxndWQGo2n956m2EZExKLJ7Jr7CG/vo6IAcXEY9PNiKLbKb53SU9PO6C5T06vd7szuxvN9EhGRbzrT997m8J5N5MN4eB5JQRMCCYcOcVUyMjddIGA185RMjnlKEtCEQEJCAjRN83YoRAA400SSUACElpZC4Xs8mZgiAC231NthEHnEPCUZKABCQ0O9HQaRA2eaSAo2RcHa2FgIC8+wJ/MSFgUlY5inZG7MU5KBTVGwdu1a2GySnnhHPodFE0lD5wXuSALCwjwl82Oekgy43DiZCUdNIiIiIiIiD1g0ERERERERecCiiaSgCYHE/fsBG1eCIBOzCQQuY56SyTFPSQKaEEhMTOTqeWQaLJpIGlaeDEoSUEqZp2R+zFOSgdVq9XYIRA4smkgKuqIgIy4O4GpPZGYWBSVjmKdkcsxTkoCuKMjIyOBiEGQavE6Tr3Jz5fH0dPc3Sa3D3aanur7fxlQ19NT0yn+FBShpBQSdyZ1UuR9XPD0nzveRVus+RES+ztVYmJ7qYX/7NjfvSw1VPR6PsTjtl+ZmLy/w9NyYaRtRfflQrnGmiYiIiIiIyAMWTURERERERB6waCI52ASCFmVxtScyN+YpyYB5ShLQhEBycjJXzyPTYNFE0hCBPAWPzI95SjJgnpIMysvLvR0CkQOLJpKDRUHp0Giu9kTmxjwlGTBPSQK6omDTpk1cPY9Mg0UTERERERGRByyaiIiIiIiIPGDRRNJQbIa3QyCqFfOUZMA8JRlwEQgyE54JSlJQbAJBi/Z4Owwij5inJAPmKcnAIgRSUlK8HQaRA4smSaWnAy4uyg5Augss14lQACM8EGpeKRSukksmxTwlGZztPE1PP/VvWs1tnt6vfPG9rLlz9Zqmpp/6N9W5XQAoLChASEgIFIWLlpD3sWgiOWgKTvaP5LVFyNyYpyQDL+Vpqsuqyf6vi22+xtf72MgVsK4oyMzMRHJyMiwWFx9XWXHTWcZzmoiIiIiIiDxg0UREREREROQBiyaSgwDU4vLKg5yJzIp5SjJgnpIEFACBgYE8n4lMg+c0kRQUXSBw2QFvh0HkEfOUZMA8JRloQiApKcnbYRA5cKaJpCAUoKJTMAS/cCITY56SDJinJAMDQG5uLgyD1xQjc2DRRHLQFJT3CQc0vsuTiTFPSQbMU5KAoSjYvXs3iyYyDRZNREREREREHrBoIiIiIiIi8oBFE8lBAFpuCVd7InNjnpIMmKckAQVASEgIV88j0+DqeV6WlubmKul14O526alnGs2ZP2bl47rf1lCKLhCwOsfjPlUvAJ6a3mShELlVlzwl8jYz5Wl6+qn/uHj/SK26XxO+v9SFI85qUlPPwoNXfXM7G6o8XvV+e3odPL7vnrpdaj3C0IRAj48/rsctqvD0nJ3t59OTM42zIf07w9t6vBlqeUwfwZkmkoJQgfLuYRDMWDIx5inJgHlKMjAAHAgLA5eBILPgkElyUBVUJIQBKqfpycSYpyQD5ilJwFCUyqKJh+eRSbBoIiIiIiIi8oBFExERERERkQcsmkgOBmDZWwwe3EymxjwlGTBPSQIqgPDiYn5QJdPg6nkkBcUQ8N+Q5+0wiDxinpIMmKckA1UIxOcxT8k8pCzgX3nlFcTGxiIgIADnn38+1qxZ4+2QqIkJVUFZ73AInrhMJsY8JRkwT0kGhqJgV3g4F4Ig05CuaPr4448xdepUzJw5E+vWrUNSUhJGjhyJ3Nxcb4dGTUkFbDHBEmYsNSvMU5IB85QkYADICw7mUaRkGtINmc8//zwmT56MSZMm4ZxzzsHrr7+OoKAgvPPOO94OjYiIiIiIfJBU5zSVl5fjjz/+wCOPPOJoU1UVw4YNw+rVq13epqysDGVlZY7fCwsLAQDHjh2DzWZz3IeqqjAMA4Zx+jsNe7uu6xBC1NquaRoURXHcb9V2ANB1vUZ7WRlwHGVO7YoNEAoArUqjABTdQ7sK5xLYABTDQ7sGoOqMtw4owkN79Uyxd7FKe1nZsSrBOff1hK3MKfZjZZUPowEwCgqcnveyMscWx3MjoKD0ZCmE7WTl10+q/fFwqoPqqcesfD2Oo6xR+uSpXdbXiX1quj4JKCgtK4Whn4Rq840+1YidfZK+T448RRkUm5CiT2VlRag6xleq3LH6e2hdX6fT7yEWAALHjp1+31LKyk69C1X+HHfTp2OnHtreFfv+djXfnTy327vu/CnC3Tur+3YLAFFY6PS5Q1EUaJpW+Vmnyuei0++4NWM/YXN+nSqfM9fRH9fLGvT3ZH8u7X0qVxQcLy1FflkZNCFOvUrOfXUXu6c+qUVFXvm856rdUlbmvk/VPhu5e/1q5N6xY577VFbmPveKitz2qazMffYdq/I3WK/XKT//dJ9cvB5n63UqKioCAKfbuqKI2vYwkezsbHTs2BG//vor+vfv72h/8MEHsWLFCvz+++81bpOWlobHH3/8bIZJREREREQS2b9/P6Kiotxul2qm6Uw88sgjmDp1quN3wzBw7NgxtGnTBgpPLpRGUVERoqOjsX//frRq1crb4RC5xDwlGTBPSQbMUzpbhBAoLi5Ghw4dPO4nVdHUtm1baJqGw4cPO7UfPnwY7du3d3kbf39/+Pv7O7WFhoY2VYjUxFq1asXBk0yPeUoyYJ6SDJindDaEhITUuo9UC0FYrVb069cPS5cudbQZhoGlS5c6Ha5HRERERETUWKSaaQKAqVOnYuLEiUhOTsZ5552H+fPn48SJE5g0aZK3QyMiIiIiIh8kXdF01VVXIS8vDzNmzEBOTg569+6NH374Ae3atfN2aNSE/P39MXPmzBqHWhKZCfOUZMA8JRkwT8lspFo9j4iIiIiI6GyT6pwmIiIiIiKis41FExERERERkQcsmoiIiIiIiDxg0UREREREROQBiyYyjVdeeQWxsbEICAjA+eefjzVr1rjd988//8Tll1+O2NhYKIqC+fPnn71AqVmrT56+9dZbGDhwIMLCwhAWFoZhw4Z53J+osdQnT//3v/8hOTkZoaGhaNGiBXr37o333nvvLEZLzVV98rSqjz76CIqi4JJLLmnaAImqYNFEpvDxxx9j6tSpmDlzJtatW4ekpCSMHDkSubm5LvcvKSlB586d8eSTT6J9+/ZnOVpqruqbp+np6bjmmmuwfPlyrF69GtHR0RgxYgQOHjx4liOn5qS+edq6dWs89thjWL16NTZt2oRJkyZh0qRJWLx48VmOnJqT+uap3Z49e/Cvf/0LAwcOPEuRElXikuNkCueffz5SUlLw8ssvAwAMw0B0dDTuuecePPzwwx5vGxsbi/vuuw/33XffWYiUmrOG5CkA6LqOsLAwvPzyy7jhhhuaOlxqphqapwDQt29fjBkzBrNmzWrKUKkZO5M81XUdgwYNwk033YSff/4ZBQUF+PLLL89i1NSccaaJvK68vBx//PEHhg0b5mhTVRXDhg3D6tWrvRgZ0WmNkaclJSWoqKhA69atmypMauYamqdCCCxduhTbt2/HoEGDmjJUasbONE+feOIJRERE4Oabbz4bYRI5sXg7AKIjR45A13W0a9fOqb1du3bIzMz0UlREzhojTx966CF06NDB6YMCUWM60zwtLCxEx44dUVZWBk3T8Oqrr2L48OFNHS41U2eSp6tWrcLbb7+NDRs2nIUIiWpi0UREdBY8+eST+Oijj5Ceno6AgABvh0PkJDg4GBs2bMDx48exdOlSTJ06FZ07d0Zqaqq3QyNCcXExJkyYgLfeegtt27b1djjUTLFoIq9r27YtNE3D4cOHndoPHz7MRR7INBqSp88++yyefPJJ/PTTT0hMTGzKMKmZO9M8VVUVXbp0AQD07t0b27Ztw7x581g0UZOob57u2rULe/bswdixYx1thmEAACwWC7Zv3474+PimDZqaPZ7TRF5ntVrRr18/LF261NFmGAaWLl2K/v37ezEyotPONE+ffvppzJo1Cz/88AOSk5PPRqjUjDXWeGoYBsrKypoiRKJ652lCQgI2b96MDRs2OH7+8Y9/YMiQIdiwYQOio6PPZvjUTHGmiUxh6tSpmDhxIpKTk3Heeedh/vz5OHHiBCZNmgQAuOGGG9CxY0fMmzcPQOVJpFu3bnX8/+DBg9iwYQNatmzp+LaUqLHVN0+feuopzJgxAx988AFiY2ORk5MDAGjZsiVatmzptX6Qb6tvns6bNw/JycmIj49HWVkZvvvuO7z33nv/3979x1RV/38Af15A4O6CXrG7GIXELhgyTYg5t0Ku/HCIKJIlYKBciuyHVlT0AyHhXlTWQAjdovmPMOiPbGC4DBQF0mpNaqGsMlkBJkwRCVRIC+7788d393y9Xu/xXtNQeD62s3ne533e53UOZ27PnXPeFxUVFRN5GjTJOXKfuru7Y968eRb7q9VqALBqJ7pbGJronpCcnIwLFy5gy5YtOHfuHEJCQtDY2Ch9JHrmzBk4Of3/g9G+vj6EhoZK6yUlJSgpKYFOp0Nra+t/XT5NEY7epxUVFfj777/xzDPPWIyTn5+PgoKC/7J0mkIcvU9HRkbwyiuv4OzZs1AqlQgKCkJNTQ2Sk5Mn6hRoCnD0PiWaaPydJiIiIiIiIhmM8ERERERERDIYmoiIiIiIiGQwNBEREREREclgaCIiIiIiIpLB0ERERERERCSDoYmIiIiIiEgGQxMREREREZEMhiYiIiIiIiIZDE1ERFNIQUEBFArFRJcxKen1enh4eNzxcc1/s4GBgTs+NhER2YehiYjoPlVZWQmFQiEt7u7u8PHxQWxsLHbu3InLly/fkeP09fWhoKAA7e3td2S8ifbtt9+ioKAAQ0NDE12Khe3bt+Pzzz+/4+Pq9XooFAo89thjEEJYbVcoFNi0aZO03t3dLd1TtbW1Vv0Z4ohoKmJoIiK6zxmNRlRXV6OiogKvvvoqACArKwvz58/HyZMnLfrm5eXhr7/+cmj8vr4+GAyGSRWaDAbDlAlNZh0dHairq3NoH6PReNOgRUQ01TA0ERHd5+Li4pCWloaMjAzk5OTg4MGDOHz4MPr7+5GQkGARklxcXODu7j6B1dJEUCqVmDNnjkMhKCQkBCdPnsS+ffvucnVERPc+hiYiokkoKioK77//Pnp6elBTUyO13+ybpqamJoSHh0OtVsPDwwOPPvooNm/eDABobW3FwoULAQAZGRnSa1uVlZUAgGPHjmHNmjWYPXs23Nzc4OvrizfeeMPqaZb5e5/e3l4kJibCw8MDGo0G2dnZGB8ft+hrMplQXl6O+fPnw93dHRqNBsuWLcP3339v0a+mpgZhYWFQKpXw8vJCSkoK/vjjD9nrUlBQgLfffhsA4O/vL51Pd3c3AGBsbAyFhYXQarVwc3PDI488gs2bN+PatWt2XHVr7e3t0Gg0WLJkCa5cuWKzn0KhwMjICKqqqqSa9Hq9RZ+hoSHo9Xqo1WrMmDEDGRkZGB0dtasOJycn5OXlORSCUlJSHA5aRESTFUMTEdEktW7dOgDAoUOHbPb56aefsGLFCly7dg1GoxE7duxAQkICvvnmGwDA3LlzYTQaAQAbNmxAdXU1qqurERERAQD47LPPMDo6ipdffhm7du1CbGwsdu3ahfXr11sda3x8HLGxsZg1axZKSkqg0+mwY8cO7N6926Lf888/j6ysLPj6+uKDDz7Ae++9B3d3d3z33XdSn23btmH9+vUIDAxEaWkpsrKycOTIEURERMi+drd69WqsXbsWAFBWViadj0ajAQBkZmZiy5YtePzxx1FWVgadToeioiKkpKTc6nJbaWtrQ1RUFEJDQ9HQ0CA7SUR1dTXc3NywePFiqaYXX3zRok9SUhIuX76MoqIiJCUlobKyEgaDwe56nn32WQQGBtodgpydnZGXl4cTJ07waRMRkSAiovvSnj17BADR1tZms8+MGTNEaGiotJ6fny+u/6+/rKxMABAXLlywOUZbW5sAIPbs2WO1bXR01KqtqKhIKBQK0dPTI7Wlp6cLAMJoNFr0DQ0NFWFhYdJ6c3OzACBee+01q3FNJpMQQoju7m7h7Owstm3bZrG9o6NDuLi4WLXfqLi4WAAQXV1dFu3t7e0CgMjMzLRoz87OFgBEc3Oz7Ljp6elCpVIJIYT4+uuvxfTp00V8fLy4evWq7H5mKpVKpKenW7Wb/2bPPfecRftTTz0lZs2adctxr6+rqqpKABB1dXXSdgBi48aN0npXV5cAIIqLi8XY2JgIDAwUCxYskK6/uR65e4aIaLLhkyYioknMw8NDdhY9tVoNAKivr4fJZHJ4fKVSKf17ZGQEAwMDeOKJJyCEwI8//mjV/6WXXrJYX7x4MX7//Xdpvba2FgqFAvn5+Vb7ml8rrKurg8lkQlJSEgYGBqTF29sbgYGBaGlpcfg8AODLL78EALz55psW7W+99RYA4MCBA3aN09LSgtjYWERHR6Ourg5ubm63Vc+NbnbtLl68iEuXLtk9Rmpq6m0/bbqbk1QQEd3rGJqIiCaxK1euwNPT0+b25ORkPPnkk8jMzMSDDz6IlJQU7N271+4AdebMGej1enh5eUnfKel0OgDA8PCwRV/z90nXmzlzJv78809p/bfffoOPjw+8vLxsHrOzsxNCCAQGBkKj0Vgsv/zyC/r7++2q/UY9PT1wcnJCQECARbu3tzfUajV6enpuOcbVq1cRHx+P0NBQ7N27F66urhbbh4eHce7cOWkZHBy0u77Zs2dbrM+cORMALK7frZhDUHt7u90hKDU1FQEBAfy2iYimNJeJLoCIiO6Os2fPYnh42CoEXE+pVOLo0aNoaWnBgQMH0NjYiE8//RRRUVE4dOgQnJ2dbe47Pj6OpUuXYnBwEO+++y6CgoKgUqnQ29sLvV5vFbzkxnKEyWSCQqFAQ0PDTcf8tz8w+29+/NfNzQ3Lly9HfX09GhsbsWLFCovtr7/+OqqqqqR1nU6H1tZWu8a2df0cDTKpqakoLCyE0WhEYmKiXcfNy8uDXq9HfX29Q8ciIposGJqIiCap6upqAEBsbKxsPycnJ0RHRyM6OhqlpaXYvn07cnNz0dLSgpiYGJshoqOjA6dPn0ZVVZXFxA9NTU23XbNWq8XBgwcxODho82mTVquFEAL+/v6YM2eOw8ewdT5+fn4wmUzo7OzE3Llzpfbz589jaGgIfn5+do39ySefYNWqVVizZg0aGhqwZMkSafs777yDtLQ0ad38tEiurjvtdkJQWloatm7dCoPBgISEhLtcIRHRvYev5xERTULNzc0oLCyEv78/UlNTbfa72ethISEhACBNs61SqQDAalY685OP6590CCFQXl5+23U//fTTEELcdFY483FWr14NZ2dnGAwGq6csQghcvHhR9hi2zmf58uUAgA8//NCivbS0FAAQHx9v1zm4urqirq4OCxcuxMqVK3H8+HFpW3BwMGJiYqQlLCzMoq7/6gd309LSEBAQYPfse9e/1rd///67XB0R0b2HT5qIiO5zDQ0NOHXqFMbGxnD+/Hk0NzejqakJfn5+2L9/v+yP2RqNRhw9ehTx8fHw8/NDf38/PvroIzz88MMIDw8H8H9PdtRqNT7++GN4enpCpVJh0aJFCAoKglarRXZ2Nnp7ezF9+nTU1tY69I3NjSIjI7Fu3Trs3LkTnZ2dWLZsGUwmE44dO4bIyEhs2rQJWq0WW7duRU5ODrq7u5GYmAhPT090dXVh37592LBhA7Kzs20ewxxUcnNzkZKSgmnTpmHlypVYsGAB0tPTsXv3bgwNDUGn0+H48eOoqqpCYmIiIiMj7T4PpVKJL774AlFRUYiLi8NXX32FefPmye4TFhaGw4cPo7S0FD4+PvD398eiRYvsPqYjnJ2dkZubi4yMDLv3Mb/W197efldqIiK6p03MpH1ERPRvmaccNy+urq7C29tbLF26VJSXl4tLly5Z7XPjlONHjhwRq1atEj4+PsLV1VX4+PiItWvXitOnT1vsV19fL4KDg4WLi4vF9OM///yziImJER4eHuKBBx4QL7zwgjhx4oTVFOXXT3stV48QQoyNjYni4mIRFBQkXF1dhUajEXFxceKHH36w6FdbWyvCw8OFSqUSKpVKBAUFiY0bN4pff/31lteusLBQPPTQQ8LJycli+vF//vlHGAwG4e/vL6ZNmyZ8fX1FTk6OXdOG3+wcBwYGRHBwsPD29hadnZ2y+586dUpEREQIpVIpAEjTj9ua4tv8979x6nR76jKfq1arlZ1y/EbX33OccpyIphKFEJwKh4iIiIiIyBZ+00RERERERCSDoYmIiIiIiEgGQxMREREREZEMhiYiIiIiIiIZDE1EREREREQyGJqIiIiIiIhkMDQRERERERHJYGgiIiIiIiKSwdBEREREREQkg6GJiIiIiIhIBkMTERERERGRDIYmIiIiIiIiGf8D4qzpqG6xveMAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot ID and OOD distributions \n",
    "# -----------------------------------\n",
    "from src.analysis.dknn import plot_score_distributions\n",
    "plot_score_distributions(\n",
    "    dknn_scores_id=dknn_scores_id,\n",
    "    dknn_scores_ood=dknn_scores_ood,\n",
    "    bins=100,\n",
    "    title=\"DKNN Distance Distributions (L2 Normalized)\",\n",
    "    save_path = PLOT_DIR + f\"densities_layer{LAYER}_token{TOKENS}_k{k}.png\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":'("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brouillon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e09cb13a04ee4c888ededada3b9cdc8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from src.model_loader.llama_loader import load_llama\n",
    "\n",
    "# Load model and tokenizer \n",
    "model, tokenizer = load_llama(MODEL_NAME)\n",
    "model.eval() \n",
    "\n",
    "def get_layer_output(\n",
    "    model,\n",
    "    inputs,\n",
    "    layer_idx,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Run a forward pass and extract the hidden states from a specific transformer layer\n",
    "    (more memory-efficient than using output_hidden_states=True).\n",
    "    Transformer layer = self-attention + FFN + normalization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : PreTrainedModel\n",
    "        A Hugging Face causal language model (e.g., LLaMA, GPT-2).\n",
    "    inputs : dict\n",
    "        Tokenized inputs returned by a tokenizer with return_tensors=\"pt\".\n",
    "    layer_idx : int\n",
    "        Index of the transformer block to capture:\n",
    "        - Use 0 to N-1 for internal layers.\n",
    "        - Use -1 to retrieve the final transformer block (not logits).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        Hidden states from the selected transformer layer.\n",
    "        Shape: (batch_size, seq_len, hidden_size)\n",
    "    \"\"\"\n",
    "    # If layer_idx = -1, interpret as \"last transformer block\"\n",
    "    if layer_idx == -1:\n",
    "        layer_idx = len(model.model.layers) - 1  # last layer index\n",
    "\n",
    "    captured_hidden = {} # store the activation retrieved by the hook\n",
    "\n",
    "    def hook_fn(module, input, output):\n",
    "        \"\"\"Function called automatically by PyTorch just after \n",
    "        the layer has produced its output during the forward pass.\"\"\"\n",
    "        # output is a tuple (hidden_states,) -> keep [0]\n",
    "        captured_hidden[\"layer_output\"] = output[0]\n",
    "        #captured_hidden[\"layer_output\"] = model.model.norm(output[0])  # post RMSNorm!\n",
    "\n",
    "\n",
    "    # Register hook on the transformer block\n",
    "    # When Pytorch pass through this layer during forward pass, it also execute hook_fn.\n",
    "    handle = model.model.layers[layer_idx].register_forward_hook(hook_fn)\n",
    "\n",
    "    # Pass inputs through the model\n",
    "    # When the target layer is reached, the hook executes and saves its output in captured_hidden.\n",
    "    with torch.no_grad():\n",
    "        _ = model(**inputs, return_dict=True)\n",
    "\n",
    "    # Remove the hook to avoid polluting future passages\n",
    "    handle.remove()\n",
    "\n",
    "    if \"layer_output\" not in captured_hidden:\n",
    "        raise RuntimeError(f\"Layer {layer_idx} did not produce an output.\")\n",
    "\n",
    "    return captured_hidden[\"layer_output\"]  # shape: (batch_size, seq_len, hidden_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in the prompt: 6\n",
      "Inputs tokens: tensor([   1,  450, 7483,  310, 3444,  338], device='cuda:0')\n",
      "generated_ids: tensor([[    1,   450,  7483,   310,  3444,   338,  3681, 29889,    13,  2177,\n",
      "           275,   338,   278, 10150,  4272,   297,  3444,   322,   338,  2998,\n",
      "           363,   967,   380, 27389, 11258, 29892,  1616, 19133, 29879, 29892,\n",
      "           322, 15839,  2982, 22848, 29889,  3834,   310,   278,  1556, 13834,\n",
      "         19650,  1953,   297,  3681,  3160,   278,   382,  2593,   295, 23615,\n",
      "         29892,   278,  4562, 12675,  6838, 29892]], device='cuda:0')\n",
      "Full generated text (full_text):\n",
      " <s> The capital of France is Paris.\n",
      "Paris is the largest city in France and is known for its stunning architecture, art museums, and historical landmarks. Some of the most famous attractions in Paris include the Eiffel Tower, the Louvre Museum,\n",
      "\n",
      "\n",
      "\n",
      "gen_hidden.shape torch.Size([50, 4096])\n",
      "full_inputs: {'input_ids': tensor([[    1,   450,  7483,   310,  3444,   338,  3681, 29889,    13,  2177,\n",
      "           275,   338,   278, 10150,  4272,   297,  3444,   322,   338,  2998,\n",
      "           363,   967,   380, 27389, 11258, 29892,  1616, 19133, 29879, 29892,\n",
      "           322, 15839,  2982, 22848, 29889,  3834,   310,   278,  1556, 13834,\n",
      "         19650,  1953,   297,  3681,  3160,   278,   382,  2593,   295, 23615,\n",
      "         29892,   278,  4562, 12675,  6838, 29892]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "full_hidden.shape torch.Size([56, 4096])\n",
      "full_hidden_gen.shape torch.Size([50, 4096])\n",
      "Norm of difference for each generated token: tensor([160.3750, 152.1250, 173.7500, 150.6250, 158.3750, 171.3750, 145.6250,\n",
      "        141.6250, 152.8750, 140.0000, 161.1250, 139.5000, 149.7500, 146.8750,\n",
      "        151.1250, 149.3750, 150.2500, 150.1250, 147.0000, 154.1250, 144.3750,\n",
      "        132.0000, 145.0000, 148.7500, 152.1250, 144.0000, 140.3750, 135.2500,\n",
      "        155.3750, 149.2500, 156.8750, 147.5000, 150.7500, 150.0000, 138.5000,\n",
      "        143.3750, 148.2500, 132.2500, 151.8750, 154.0000, 153.3750, 138.5000,\n",
      "        148.7500, 139.7500, 149.3750, 150.2500, 139.3750, 140.5000, 134.8750,\n",
      "        145.0000], device='cuda:0', dtype=torch.float16)\n",
      "Cosine similarity per step: tensor([-0.0376,  0.0676,  0.0370, -0.0172, -0.0291, -0.0211, -0.0349, -0.0354,\n",
      "        -0.0339, -0.0361, -0.0533, -0.0089, -0.0266, -0.0177, -0.0426, -0.0381,\n",
      "        -0.0286, -0.0272, -0.0323, -0.0389, -0.0198,  0.0134, -0.0050, -0.0475,\n",
      "        -0.0464, -0.0183, -0.0076, -0.0190, -0.0057, -0.0035, -0.0237, -0.0080,\n",
      "        -0.0204, -0.0032, -0.0006, -0.0099, -0.0138,  0.0016, -0.0303, -0.0258,\n",
      "        -0.0075, -0.0133,  0.0036,  0.0013, -0.0051, -0.0183, -0.0157, -0.0153,\n",
      "         0.0018, -0.0055], device='cuda:0', dtype=torch.float16)\n",
      "All close? False\n"
     ]
    }
   ],
   "source": [
    "from src.utils.general import seed_all\n",
    "seed_all(44)\n",
    "\n",
    "prompt = \"The capital of France is\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "print(\"Number of tokens in the prompt:\", len(inputs[\"input_ids\"][0]))\n",
    "print(\"Inputs tokens:\", inputs[\"input_ids\"][0])\n",
    "\n",
    "# Define witch layer to retrieve \n",
    "layer_idx = -1  # Last layer\n",
    "\n",
    "# METHOD 1: Retrieve token embeddings with model.generate \n",
    "# =========================================\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=50,\n",
    "            output_hidden_states=True,\n",
    "            return_dict_in_generate=True,\n",
    "            do_sample=True,\n",
    "            temperature=0.6,\n",
    "            top_p=0.9,\n",
    "            top_k=50,\n",
    "            use_cache=False, \n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.eos_token_id \n",
    "        )\n",
    "\"\"\"\n",
    "Rappel sur la forme de output_hidden_states: \n",
    "-----\n",
    "Quand on appelle model.generate(..., output_hidden_states=True), \n",
    "la variable outputs.hidden_states est une liste (ou un tuple) où:\n",
    "\n",
    "Le premier élément (indice 0) contient les embeddings (hidden states) \n",
    "de tous les tokens du prompt (avant génération).\n",
    "Sa forme est (batch_size, prompt_len, hidden_size).\n",
    "\n",
    "Chaque élément suivant (indice 1, 2, 3, ...) contient les embeddings \n",
    "du token généré à cette étape uniquement.\n",
    "Sa forme est (batch_size, 1, hidden_size) car c'est un seul token.\n",
    "\n",
    "Donc, si on a un prompt de 6 tokens et que l'on génère 3 tokens, on aura:\n",
    "# L = numéro de la couche (ex: -1 pour la dernière)\n",
    "outputs.hidden_states[0][L]  -> embeddings des 6 tokens du prompt (shape: [1, 6, hidden_size]) \n",
    "outputs.hidden_states[1][L]  -> embeddings du 1er token généré (shape: [1, 1, hidden_size])\n",
    "outputs.hidden_states[2][L]  -> embeddings du 2e token généré (shape: [1, 1, hidden_size])\n",
    "outputs.hidden_states[3][L]  -> embeddings du 3e token généré (shape: [1, 1, hidden_size])\n",
    "\n",
    "len(outputs.hidden_states[0]) = Number of layers\n",
    "\"\"\"\n",
    "\n",
    "# Complete generated sequence (prompt + response)\n",
    "generated_ids = outputs.sequences  # Shape - (1, prompt_len + gen_len) = (1, 6+50) = (1, 56)\n",
    "print(\"generated_ids:\", generated_ids)\n",
    "full_text = tokenizer.decode(generated_ids[0])\n",
    "print(\"Full generated text (full_text):\\n\", full_text)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "# Extract hidden states from model.generate \n",
    "gen_hidden = []\n",
    "for step in range(len(outputs.hidden_states)): # Shape - len(outputs.hidden_states) = 50\n",
    "    # Shape - outputs.hidden_states[step][layer_idx] = (1, 1, hidden_size)\n",
    "    gen_hidden.append(outputs.hidden_states[step][layer_idx][0, 0, :]) \n",
    "gen_hidden = torch.stack(gen_hidden, dim=0)  # (gen_len, hidden_size)\n",
    "print(\"gen_hidden.shape\", gen_hidden.shape)\n",
    "\n",
    "# METHOD 2: Retrieve embeddings with complete forward + hook \n",
    "# => This is the method I am using so far in my code in batch_extract_answer_token_activations() from inference_utils.py\n",
    "# =========================================\n",
    "# 1. We take full_text[4:] to remove the '<s> ' appended by the tokenizer \n",
    "# so that embeddings from METHOD 1 and 2 can be aligned and thus compared \n",
    "# 2. We retokenize the full generated text so that we can have access to the attention mask and pass\n",
    "# it to get_layer_output like I did in batch_extract_answer_token_activations()\n",
    "full_inputs = tokenizer(full_text[4:], return_tensors=\"pt\").to(model.device)\n",
    "print(\"full_inputs:\", full_inputs)\n",
    "full_hidden = get_layer_output(model, full_inputs, layer_idx)[0]  # (seq_len, hidden_size) = (50, 4096) # 0 to select batch=1\n",
    "print(\"full_hidden.shape\", full_hidden.shape)\n",
    "\n",
    "\n",
    "# Alignment and comparison\n",
    "# =========================================\n",
    "prompt_len = inputs[\"input_ids\"].shape[1]  \n",
    "full_hidden_gen = full_hidden[prompt_len:prompt_len+gen_hidden.shape[0], :]  # (gen_len, hidden_size) = (56, 4096)\n",
    "print(\"full_hidden_gen.shape\", full_hidden_gen.shape)\n",
    "\n",
    "\n",
    "# Force the same device\n",
    "device = model.device if hasattr(model, \"device\") else next(model.parameters()).device\n",
    "gen_hidden = gen_hidden.to(device)\n",
    "full_hidden_gen = full_hidden_gen.to(device)\n",
    "# Compute difference\n",
    "diff = torch.norm(full_hidden_gen - gen_hidden, dim=1)\n",
    "print(\"Norm of difference for each generated token:\", diff)\n",
    "# Compute cosine_sim to see if vectors point in the same direction\n",
    "cosine_sim = torch.nn.functional.cosine_similarity(full_hidden_gen, gen_hidden, dim=1)\n",
    "print(\"Cosine similarity per step:\", cosine_sim)\n",
    "print(\"All close?\", torch.allclose(full_hidden_gen, gen_hidden, atol=1e-5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pourquoi c'est différent entre model.generate et forward complet + hook ?**\n",
    "\n",
    "\n",
    "On voit que la norme 2 est différente et la cosine similarity est proche de 0 et négative ce qui indique que les vecteurs sont quasiment orthogonaux et de sens opposés.\n",
    "Un utilisateur a déjà repporté ce probleme:\n",
    "- en Nov 2024 pour GPT2 : https://discuss.huggingface.co/t/gpt2-hidden-states-get-by-output-hidden-states-is-different-from-those-by-register-forward-hook/125835\n",
    "\n",
    "- Dans la définition de hook_fn, faut-il que je mette: \n",
    "captured_hidden[\"layer_output\"] = output[0] ou captured_hidden[\"layer_output\"] = model.model.norm(output[0])  # post RMSNorm!\n",
    "    - Dans le code Llama, chaque bloc (LlamaDecoderLayer) applique : RMSNorm => SelfAttention (avec causal mask) => résiduel => RMSNorm => MLP => résiduel.\n",
    "    - Moi dans mon forward + hook je fais: Blocs LlamaDecoderLayer: RMSNorm => SelfAttention (avec causal mask) => résiduel => RMSNorm => MLP => résiduel. Puis je refais RMSNorm final (model.model.norm) et je capture ce tenseur. \n",
    "    De toutes façons que je mette output[0] ou model.model.norm(output[0]), model.generate et forward diffèrent. \n",
    "\n",
    "Ce qui est identique entre model.generate et le forward complet + hook : \n",
    "- Que ce soit pour model.generate ou pour le forward complet, le masque d'attention causal est appliqué dans les deux cas. En effet, comme pour model.generate, lors d’un forward pass complet, chaque token ne peut \"voir\" que les tokens précédents grâce au masque causal. Ainsi, l’attention utilise systématiquement le masque causal : chaque token ne peut donc porter attention qu’aux tokens précédents ou à lui-même donc il ne devrait pas y avoir de différence à ce niveau entre forward et model.generate \n",
    "- Que ce soit avec model.generate ou lors d’un forward complet, la normalisation (RMSNorm/LayerNorm) est toujours appliquée séparément à chaque token (RMSNorm ne dépend pas de la longueur de séquence ni du contexte global)\n",
    "\n",
    "Mes hypothèses sur ce qui peut différer :\n",
    "- Une différence pourrait venir du cache (past_key_values) car lors de model.generate, les clés/valeurs d'attention sont réutilisées alors que c'est pas le cas pour le forward complet. Je te joins plus tard mon petit test \n",
    "sur le cache où l'on observe des résultats très différents avec ou sans utlisation du cache. \n",
    "Cependant, je mets ici use_cache = False, j'ai toujours cette différence entre model.generate() et le forward + hook. \n",
    "\n",
    "- RMSNorm  est utilisé dans LLaMA 2 à la place du LayerNorm classique. Cette normalisation s’applique indépendamment sur chaque vecteur de token en sortant de chaque bloc Transformer, en normalisant par la norme quadratique des composantes du vecteur. En soi, RMSNorm ne dépend pas de la longueur de séquence ni du contexte global : que l’on traite un token isolé ou une séquence complète, le calcul du RMSNorm d’une étape donnée devrait être le même pour ce token. Cependant, la moindre variation numérique en entrée du RMSNorm sera amplifiée (puisqu’on divise par une moyenne quadratique) et propagée aux couches suivantes. Ainsi, si les vecteurs avant normalisation diffèrent ne serait-ce que légèrement, les sorties RMSNorm différeront encore plus. \n",
    "\n",
    "- Les embeddings de position (RoPE) sont incrémentés étape par étape lors de la génération, alors qu’ils sont calculés d’un coup lors du forward complet. Ceci pourrait créer une différence. \n",
    "\n",
    "\n",
    "- Quelques sources supplémentaires : \n",
    "    - \"During the generation step, you're not doing 1 forward pass, but say 256 passes for 256 new tokens. Each token must be generated, hence its 256x slower. [...] They find the closest token and do a different forward pass each time, instead of using the caches previous hidden states in a regular forward pass.\" (https://www.reddit.com/r/LocalLLaMA/comments/1aplei9/why_is_modelgenerate_so_much_for_inference_so/)\n",
    "\n",
    "    - \"The two methods do something completely different.\n",
    "        - Calling the model (which means the forward method) uses the labels for teacher forcing. This means inputs to the decoder are the labels shifted by one. With teacher forcing, the decoder always gets the ground-truth token in the next step, no matter what the prediction was. Teacher forcing is used from model training, all steps are fully differentiable.\n",
    "        - When you call the generate method, the model is used in the autoregressive fashion. Any token it generates is put as the input in the next step. However, selecting the token is a \"hard\" decision, and the gradient cannot be propagated through this decision. The generate method cannot be used for training. The output is coherent because the decoder reacts to what was previously generated.\n",
    "        - With teacher forcing, the model might want to prefer generating a token and continue consistently with the generated token. However, it cannot continue consistently, because it is forced to continue as if it generated the token that actually is in the labels argument. This why you observe the incoherent output (which was nevertheless never intended to be output but only to be used for training).\" \n",
    "        (https://stackoverflow.com/questions/67328345/how-to-use-forward-method-instead-of-model-generate-for-t5-model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lila.roig/.env/ood_env/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen_hidden_cache:  tensor([[ 2.8545e-01, -7.6011e-02,  1.3866e-02,  ...,  1.6535e-02,\n",
      "          1.7483e-03,  3.5943e-02],\n",
      "        [ 1.3763e+00, -2.0311e+00, -5.9080e-01,  ..., -7.7493e-01,\n",
      "          1.4942e+00,  1.2476e+00],\n",
      "        [ 1.3237e+00,  1.1104e+00,  2.2428e+00,  ..., -1.1177e+00,\n",
      "          4.9294e-01,  1.8590e+00],\n",
      "        ...,\n",
      "        [ 5.5895e-01, -6.7917e-01, -1.8643e+00,  ...,  1.8280e+00,\n",
      "          7.3041e-01,  4.5444e-01],\n",
      "        [ 7.5555e-01, -2.9025e-01, -2.2392e+00,  ...,  1.1273e+00,\n",
      "         -9.9765e-01, -1.2817e+00],\n",
      "        [ 5.6314e-01, -2.3225e+00, -1.5054e-01,  ...,  1.4187e-02,\n",
      "          2.8200e-01,  1.4845e+00]], device='cuda:0')\n",
      "gen_hidden_Nocache tensor([[ 0.2855, -0.0760,  0.0139,  ...,  0.0165,  0.0017,  0.0359],\n",
      "        [ 0.2855, -0.0760,  0.0139,  ...,  0.0165,  0.0017,  0.0359],\n",
      "        [ 0.2855, -0.0760,  0.0139,  ...,  0.0165,  0.0017,  0.0359],\n",
      "        ...,\n",
      "        [ 0.2855, -0.0760,  0.0139,  ...,  0.0165,  0.0017,  0.0359],\n",
      "        [ 0.2855, -0.0760,  0.0139,  ...,  0.0165,  0.0017,  0.0359],\n",
      "        [ 0.2855, -0.0760,  0.0139,  ...,  0.0165,  0.0017,  0.0359]],\n",
      "       device='cuda:0')\n",
      "Norm of difference for each generated token: tensor([  0.0000, 164.8543, 148.5035, 161.4170, 158.7584, 159.5243, 157.9769,\n",
      "        157.6191, 156.4160, 155.5492, 158.0371, 158.3670, 156.4990, 159.7969,\n",
      "        155.6030, 155.8346, 155.0412, 158.5352, 156.7593, 155.6289, 155.8183,\n",
      "        160.9505, 157.1931, 155.6251, 153.7855, 160.7579, 158.1563, 155.1758,\n",
      "        158.2697, 157.0105, 155.6895, 153.9490, 159.0450, 160.0259, 155.8337,\n",
      "        156.0888, 156.3061, 156.1120, 154.9264, 155.2348, 155.6712, 148.4924,\n",
      "        162.6936, 156.8201, 161.1221, 158.8624, 147.9801, 153.8398, 154.9956,\n",
      "        160.3123], device='cuda:0')\n",
      "All close? False\n",
      "Cosine similarity per step: tensor([ 1.0000e+00, -5.1497e-02,  7.0175e-02, -6.3307e-03, -1.5780e-02,\n",
      "        -3.6827e-02, -3.8159e-02, -3.1605e-02, -2.4535e-02, -7.1374e-03,\n",
      "        -1.0292e-02, -2.5879e-02, -2.1952e-02, -2.0821e-02,  6.2284e-03,\n",
      "        -1.0474e-02, -5.0904e-03, -3.5924e-02, -2.1264e-02, -1.0963e-02,\n",
      "        -1.0886e-02, -3.8837e-02, -2.7708e-02, -9.5714e-03,  1.4069e-02,\n",
      "        -2.1646e-02, -3.9937e-02, -3.4081e-03, -4.1243e-02, -2.7213e-02,\n",
      "        -9.5106e-03,  4.5724e-03, -2.2682e-02, -2.3041e-02, -9.9338e-03,\n",
      "         3.6108e-03, -1.3240e-02, -4.6743e-04, -1.0220e-03, -1.7517e-03,\n",
      "        -4.8812e-03,  1.5076e-02, -4.6635e-02, -7.3744e-03, -3.9168e-02,\n",
      "        -3.4796e-02,  2.6364e-03,  1.0131e-02, -9.4938e-03, -1.8569e-02],\n",
      "       device='cuda:0')\n",
      "Same tokens generated? True\n"
     ]
    }
   ],
   "source": [
    "from src.utils.general import seed_all\n",
    "seed_all(44)\n",
    "model = model.float() # Pass the model to float32\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs_cache = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=50,\n",
    "            output_hidden_states=True,\n",
    "            return_dict_in_generate=True,\n",
    "            do_sample=False,\n",
    "            temperature=1,\n",
    "            #top_p=0.9,\n",
    "            #top_k=50,\n",
    "            use_cache=True, \n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.eos_token_id \n",
    "        )\n",
    "    \n",
    "# Extract hidden states from model.generate \n",
    "gen_hidden_cache = []\n",
    "for step in range(len(outputs_cache.hidden_states)): # Shape - len(outputs.hidden_states) = 50\n",
    "    gen_hidden_cache.append(outputs_cache.hidden_states[step][layer_idx][0, 0, :]) \n",
    "gen_hidden_cache = torch.stack(gen_hidden_cache, dim=0)  # (gen_len, hidden_size)\n",
    "print(\"gen_hidden_cache: \", gen_hidden_cache)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs_nocache = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=50,\n",
    "            output_hidden_states=True,\n",
    "            return_dict_in_generate=True,\n",
    "            do_sample=False,\n",
    "            temperature=1,\n",
    "            #top_p=0.9,\n",
    "            #top_k=50,\n",
    "            use_cache=False, \n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.eos_token_id \n",
    "        )\n",
    "    \n",
    "# Extract hidden states from model.generate \n",
    "gen_hidden_Nocache = []\n",
    "for step in range(len(outputs_nocache.hidden_states)): \n",
    "    gen_hidden_Nocache.append(outputs_nocache.hidden_states[step][layer_idx][0, 0, :]) \n",
    "gen_hidden_Nocache = torch.stack(gen_hidden_Nocache, dim=0)  # (gen_len, hidden_size)\n",
    "print(\"gen_hidden_Nocache\", gen_hidden_Nocache)\n",
    "\n",
    "# Dettect if cache or not makes a difference \n",
    "diff = torch.norm(gen_hidden_cache - gen_hidden_Nocache, dim=1)\n",
    "print(\"Norm of difference for each generated token:\", diff)\n",
    "print(\"All close?\", torch.allclose(gen_hidden_cache, gen_hidden_Nocache, atol=1e-5))\n",
    "\n",
    "# Compute cosine_sim to see if vectors point in the same direction\n",
    "cosine_sim = torch.nn.functional.cosine_similarity(gen_hidden_cache, gen_hidden_Nocache, dim=1)\n",
    "print(\"Cosine similarity per step:\", cosine_sim)\n",
    "\n",
    "# Do the 2 methods generate the same tokens? \n",
    "print(\"Same tokens generated?\", torch.equal(outputs_cache.sequences, outputs_nocache.sequences))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici on voit une différence notable entre use_cache=True et use_cache=False entre les deux runs de model.generate(), meme si je passe le modèle en float 32. \n",
    "En fait, quand use_cache=False chaque étape de génération recalcule toute la séquence précédente à chaque pas. \n",
    "Cela signifie que des petites erreurs apparaissent très tôt, et elles se propagent et s’accumulent exponentiellement sur 50 étapes de génération. D'ailleurs on voit que pour le premier step la norme de la différence est 0 et ensuite elle s'accumule très vite. Même quand on supprime toute stochasticité avec le seed, en mettant do_sample=False (greedy decoding) temperature=1, et en supprimant top_p=0.9 et top_k=50 on a une différence. \n",
    "Bien sur si on met pour les 2 ruens de model.generate() use_cache=True ou use_cache=False on a pas de différence. \n",
    "Mais peut-être que la distance L2 n'est pas la distance la plus pertinente car elle ne fait que mesurer la norme des vecteurs. Donc je regarde avec la cosine similarity si les vecteurs pointent dans le meme sens. Pour rappel: cos_sim= +1 => les deux vecteurs pointent exactement dans la même direction, cos_sim = 0 => vecteurs orthogonaux, aucune similarité directionnelle et cos_sim = -1 vecteirs se sens opposés. Et on voit que l'on a des cosine faibles et négatives, indiquant des vecteurs opposés dans l’espace latent et absence de similarité directionnelle.\n",
    "Cependant, on constate que les tokens générés sont identiques. \n",
    "\n",
    "Ce probleme de différence a déjà été rapporté ici \n",
    "- En 2023: https://github.com/huggingface/transformers/issues/25420 : \"When using KV caches (and, in some models, left-padding), we are changing the input shape to some matrix multiplication operations. For instance, in Llama, when you apply the linear projection to obtain the QKV for the attention layer, the input shape will be different depending on whether you're using left-padding and/or KV caches. Therefore, the output of these operations may be different, and these tiny differences build up across layers and across generated tokens, especially at lower resolutions.\" \n",
    "- en mars 2025 pour \"Llama-3.1-8B-Instruct\" : https://discuss.huggingface.co/t/model-generate-use-cache-true-generates-different-results-than-use-cache-false/144001 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mettre un hook sur le generate\n",
    "# Regarder d'autres codes pour comprendre pourquoi \n",
    "# pour obtenir les hidden states du prompt + génération, les conseillers recommandent exactement cela : faire un forward sur le prompt initial puis utiliser generate pour les sorties, puis combiner\n",
    " # Comment modifier mon code pour avoir input hidden states + generated: https://discuss.huggingface.co/t/the-hidden-states-when-i-use-model-generate/73169/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "200cfd74f13b4392a0135d751b7cd5d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in the prompt: 6\n",
      "Inputs tokens: tensor([   1,  450, 7483,  310, 3444,  338], device='cuda:0')\n",
      "Length of outputs.hidden_states: 50\n",
      "Checking the shape of the hidden states:\n",
      "Length of outputs.hidden_states[0]: 33\n",
      "Shape of outputs.hidden_states[0][layer_idx]: torch.Size([1, 6, 4096])\n",
      "Shape of outputs.hidden_states[1][layer_idx]: torch.Size([1, 7, 4096])\n",
      "Shape of outputs.hidden_states[2][layer_idx]: torch.Size([1, 8, 4096])\n",
      "Shape of outputs.hidden_states[-1][layer_idx]: torch.Size([1, 55, 4096])\n",
      "generated_ids: tensor([[    1,   450,  7483,   310,  3444,   338,  3681, 29889,    13,  2177,\n",
      "           275,   338,   278, 10150,  4272,   297,  3444,   322,   338,  2998,\n",
      "           363,   967,   380, 27389, 11258, 29892,  1616, 19133, 29879, 29892,\n",
      "           322, 15839,  2982, 22848, 29889,  3834,   310,   278,  1556, 13834,\n",
      "         19650,  1953,   297,  3681,  3160,   278,   382,  2593,   295, 23615,\n",
      "         29892,   278,  4562, 12675,  6838, 29892]], device='cuda:0')\n",
      "Full generated text (full_text):\n",
      " <s> The capital of France is Paris.\n",
      "Paris is the largest city in France and is known for its stunning architecture, art museums, and historical landmarks. Some of the most famous attractions in Paris include the Eiffel Tower, the Louvre Museum,\n",
      "\n",
      "\n",
      "\n",
      "gen_hidden.shape torch.Size([50, 4096])\n",
      "full_inputs: {'input_ids': tensor([[    1,   450,  7483,   310,  3444,   338,  3681, 29889,    13,  2177,\n",
      "           275,   338,   278, 10150,  4272,   297,  3444,   322,   338,  2998,\n",
      "           363,   967,   380, 27389, 11258, 29892,  1616, 19133, 29879, 29892,\n",
      "           322, 15839,  2982, 22848, 29889,  3834,   310,   278,  1556, 13834,\n",
      "         19650,  1953,   297,  3681,  3160,   278,   382,  2593,   295, 23615,\n",
      "         29892,   278,  4562, 12675,  6838, 29892]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "full_hidden.shape torch.Size([56, 4096])\n",
      "full_hidden_gen.shape torch.Size([50, 4096])\n",
      "Norm of difference for each generated token: tensor([0.1899, 0.9048, 0.1869, 0.2438, 0.2047, 0.2001, 0.2717, 0.2803, 0.2529,\n",
      "        0.3936, 0.2279, 0.3145, 0.2477, 0.2529, 0.2419, 0.2294, 0.2323, 0.2268,\n",
      "        0.2245, 0.2246, 0.2522, 0.2729, 0.2332, 0.2430, 0.2346, 0.2559, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], device='cuda:0', dtype=torch.float16)\n",
      "Cosine similarity per step: tensor([0.9995, 1.0000, 1.0000, 1.0000, 0.9995, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9995, 1.0000, 1.0000, 0.9995,\n",
      "        1.0000, 0.9995, 0.9995, 1.0000, 1.0000, 0.9995, 1.0000, 1.0000, 1.0000,\n",
      "        0.9995, 1.0000, 1.0010, 1.0000, 1.0010, 0.9995, 0.9995, 0.9995, 1.0010,\n",
      "        1.0000, 1.0000, 1.0000, 0.9995, 0.9995, 0.9995, 0.9995, 0.9995, 0.9995,\n",
      "        1.0000, 1.0000, 1.0010, 0.9995], device='cuda:0', dtype=torch.float16)\n",
      "All close? False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from src.model_loader.llama_loader import load_llama\n",
    "from src.utils.general import seed_all\n",
    "\n",
    "# Clear memory to avoid \"CUDA out of memory\"\n",
    "# -----------------------------------\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "\n",
    "seed_all(44)\n",
    "\n",
    "MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# Load model and tokenizer\n",
    "model, tokenizer = load_llama(MODEL_NAME)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "def get_layer_output(\n",
    "    model,\n",
    "    inputs,\n",
    "    layer_idx,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Run a forward pass and extract the hidden states from a specific transformer layer\n",
    "    (more memory-efficient than using output_hidden_states=True).\n",
    "    Transformer layer = self-attention + FFN + normalization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : PreTrainedModel\n",
    "        A Hugging Face causal language model (e.g., LLaMA, GPT-2).\n",
    "    inputs : dict\n",
    "        Tokenized inputs returned by a tokenizer with return_tensors=\"pt\".\n",
    "    layer_idx : int\n",
    "        Index of the transformer block to capture:\n",
    "        - Use 0 to N-1 for internal layers.\n",
    "        - Use -1 to retrieve the final transformer block (not logits).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        Hidden states from the selected transformer layer.\n",
    "        Shape: (batch_size, seq_len, hidden_size)\n",
    "    \"\"\"\n",
    "\n",
    "    captured_hidden = {}  # store the activation retrieved by the hook\n",
    "\n",
    "    def hook_fn(module, input, output):\n",
    "        \"\"\"Function called automatically by PyTorch just after\n",
    "        the layer has produced its output during the forward pass.\"\"\"\n",
    "        # output is a tuple (hidden_states,) -> keep [0]\n",
    "        if layer_idx == -1:\n",
    "            captured_hidden[\"layer_output\"] = model.model.norm(output[0])  # post RMSNorm!\n",
    "        else:\n",
    "            captured_hidden[\"layer_output\"] = output[0]\n",
    "\n",
    "    # Register hook on the transformer block\n",
    "    # When Pytorch pass through this layer during forward pass, it also execute hook_fn.\n",
    "    handle = model.model.layers[layer_idx].register_forward_hook(hook_fn)\n",
    "\n",
    "    # Pass inputs through the model\n",
    "    # When the target layer is reached, the hook executes and saves its output in captured_hidden.\n",
    "    with torch.no_grad():\n",
    "        _ = model(**inputs, return_dict=True)\n",
    "\n",
    "    # Remove the hook to avoid polluting future passages\n",
    "    handle.remove()\n",
    "\n",
    "    if \"layer_output\" not in captured_hidden:\n",
    "        raise RuntimeError(f\"Layer {layer_idx} did not produce an output.\")\n",
    "\n",
    "    return captured_hidden[\"layer_output\"]  # shape: (batch_size, seq_len, hidden_size)\n",
    "\n",
    "\n",
    "prompt = \"The capital of France is\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "print(\"Number of tokens in the prompt:\", len(inputs[\"input_ids\"][0]))\n",
    "print(\"Inputs tokens:\", inputs[\"input_ids\"][0])\n",
    "\n",
    "# Define witch layer to retrieve\n",
    "layer_idx = -1  # Last layer\n",
    "\n",
    "# METHOD 1: Retrieve token embeddings with model.generate\n",
    "# =========================================\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=50,\n",
    "        output_hidden_states=True,\n",
    "        return_dict_in_generate=True,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "        top_k=50,\n",
    "        num_beams = 1,\n",
    "        use_cache=False,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Length of outputs.hidden_states:\", len(outputs.hidden_states))\n",
    "\n",
    "\n",
    "print(\"Checking the shape of the hidden states:\")\n",
    "print(\"Length of outputs.hidden_states[0]:\", len(outputs.hidden_states[0]))\n",
    "print(\n",
    "    \"Shape of outputs.hidden_states[0][layer_idx]:\",\n",
    "    outputs.hidden_states[0][layer_idx].shape,\n",
    ")\n",
    "print(\n",
    "    \"Shape of outputs.hidden_states[1][layer_idx]:\",\n",
    "    outputs.hidden_states[1][layer_idx].shape,\n",
    ")\n",
    "print(\n",
    "    \"Shape of outputs.hidden_states[2][layer_idx]:\",\n",
    "    outputs.hidden_states[2][layer_idx].shape,\n",
    ")\n",
    "print(\n",
    "    \"Shape of outputs.hidden_states[-1][layer_idx]:\",\n",
    "    outputs.hidden_states[-1][layer_idx].shape,\n",
    ")\n",
    "\n",
    "# Complete generated sequence (prompt + response)\n",
    "generated_ids = (\n",
    "    outputs.sequences\n",
    ")  # Shape - (1, prompt_len + gen_len) = (1, 6+50) = (1, 56)\n",
    "print(\"generated_ids:\", generated_ids)\n",
    "full_text = tokenizer.decode(generated_ids[0])\n",
    "print(\"Full generated text (full_text):\\n\", full_text)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "# Extract hidden states from model.generate\n",
    "gen_hidden = []\n",
    "for step in range(\n",
    "    len(outputs.hidden_states)\n",
    "):  # Shape - len(outputs.hidden_states) = 50\n",
    "    # Shape - outputs.hidden_states[step][layer_idx] = (1, 1, hidden_size)\n",
    "    gen_hidden.append(outputs.hidden_states[step][layer_idx][0, -1, :])\n",
    "gen_hidden = torch.stack(gen_hidden, dim=0)  # (gen_len, hidden_size)\n",
    "print(\"gen_hidden.shape\", gen_hidden.shape)\n",
    "\n",
    "# METHOD 2: Retrieve embeddings with complete forward + hook\n",
    "# => This is the method I am using so far in my code in batch_extract_answer_token_activations() from inference_utils.py\n",
    "# =========================================\n",
    "# 1. We take full_text[4:] to remove the '<s> ' appended by the tokenizer\n",
    "# so that embeddings from METHOD 1 and 2 can be aligned and thus compared\n",
    "# 2. We retokenize the full generated text so that we can have access to the attention mask and pass\n",
    "# it to get_layer_output like I did in batch_extract_answer_token_activations()\n",
    "full_inputs = tokenizer(full_text[4:], return_tensors=\"pt\").to(model.device)\n",
    "print(\"full_inputs:\", full_inputs)\n",
    "full_hidden = get_layer_output(model, full_inputs, layer_idx)[\n",
    "    0\n",
    "]  # (seq_len, hidden_size) = (50, 4096) # 0 to select batch=1\n",
    "print(\"full_hidden.shape\", full_hidden.shape)\n",
    "\n",
    "\n",
    "# Alignment and comparison\n",
    "# =========================================\n",
    "prompt_len = inputs[\"input_ids\"].shape[1]\n",
    "full_hidden_gen = full_hidden[\n",
    "    prompt_len : prompt_len + gen_hidden.shape[0], :\n",
    "]  # (gen_len, hidden_size) = (56, 4096)\n",
    "print(\"full_hidden_gen.shape\", full_hidden_gen.shape)\n",
    "\n",
    "\n",
    "# Force the same device\n",
    "device = model.device if hasattr(model, \"device\") else next(model.parameters()).device\n",
    "gen_hidden = gen_hidden.to(device)\n",
    "full_hidden_gen = full_hidden_gen.to(device)\n",
    "# Compute difference\n",
    "diff = torch.norm(full_hidden_gen[:-1] - gen_hidden[1:], dim=1)\n",
    "print(\"Norm of difference for each generated token:\", diff)\n",
    "# Compute cosine_sim to see if vectors point in the same direction\n",
    "cosine_sim = torch.nn.functional.cosine_similarity(\n",
    "    full_hidden_gen[:-1], gen_hidden[1:], dim=1\n",
    ")\n",
    "print(\"Cosine similarity per step:\", cosine_sim)\n",
    "print(\"All close?\", torch.allclose(full_hidden_gen, gen_hidden, atol=1e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answers_with_hook(\n",
    "    model,\n",
    "    inputs,\n",
    "    tokenizer,\n",
    "    layer_idx,\n",
    "    max_new_tokens=50\n",
    "):\n",
    "    activations = []\n",
    "\n",
    "    def hook_fn(module, input, output):\n",
    "        # Stocke tous les embeddings du layer pour chaque forward \n",
    "        if layer_idx == -1:\n",
    "            activations.append( model.model.norm(output[0]).detach().cpu()) # post RMSNorm!\n",
    "        else:\n",
    "            activations.append(output[0].detach().cpu())\n",
    "\n",
    "\n",
    "    handle = model.model.layers[layer_idx].register_forward_hook(hook_fn)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.6,\n",
    "            top_p=0.9,\n",
    "            top_k=50,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    handle.remove()\n",
    "\n",
    "    # activations est une liste de tenseurs (batch_size, seq_len, hidden_size)\n",
    "    # Il faudra les organiser selon la logique de génération (voir plus haut)\n",
    "    return output_ids, activations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in the prompt: 15\n",
      "Inputs tokens: tensor([   2,    2,    2,    2,    2,    2,    2,    2,    2,    1,  450, 7483,\n",
      "         310, 3444,  338], device='cuda:0')\n",
      "Length of outputs.hidden_states: 50\n",
      "Checking the shape of the hidden states:\n",
      "Length of outputs.hidden_states[0]: 33\n",
      "Shape of outputs.hidden_states[0][layer_idx]: torch.Size([3, 15, 4096])\n",
      "Shape of outputs.hidden_states[1][layer_idx]: torch.Size([3, 1, 4096])\n",
      "Shape of outputs.hidden_states[2][layer_idx]: torch.Size([3, 1, 4096])\n",
      "Shape of outputs.hidden_states[-1][layer_idx]: torch.Size([3, 1, 4096])\n",
      "=======================================\n",
      "Length of activations: 50\n",
      "Shape of activations[0]: torch.Size([3, 15, 4096])\n",
      "Checking the shape of the hidden states:\n",
      "Length of activations[0]: 3\n",
      "Shape of activations[0]: torch.Size([3, 15, 4096])\n",
      "Shape of activations[1]: torch.Size([3, 1, 4096])\n",
      "Shape of activations[2]: torch.Size([3, 1, 4096])\n",
      "Shape of activations[-1]: torch.Size([3, 1, 4096])\n",
      "generated_ids: tensor([[    2,     2,     2,     2,     2,     2,     2,     2,     2,     1,\n",
      "           450,  7483,   310,  3444,   338,  3681, 29889,   450,  4272,   338,\n",
      "          5982,   297,   278, 14622,  6555,   760,   310,  3444,   322,   338,\n",
      "          2998,   363,   967,   380, 27389, 11258, 29892,  1616, 19133, 29879,\n",
      "         29892, 13460, 29892,   322,  6017,  7716, 25005, 29889,  3681,   338,\n",
      "          3271,   304,  1784, 13834,  2982, 22848,  1316,   408,   278,   382,\n",
      "          2593,   295, 23615, 29892, 24337],\n",
      "        [    1,  1724,   338,   278,  1024,   310,   278,  5176, 10470,  1058,\n",
      "           471,  1602,   481, 22731, 29973,    13,    13,  1576,  5176, 10470,\n",
      "          1058,   471,  1602,   481, 22731,   338,  9932, 21089,   262,  2353,\n",
      "         29889,  2296,   471,   278,  1833, 26624,   310,  3444,  1434,   278,\n",
      "          5176, 14595,   322,   471,  8283,   491,  1410,   453,   327,   457,\n",
      "           373,  5533, 29871, 29896, 29953, 29892, 29871, 29896, 29955, 29929,\n",
      "         29941, 29889,     2,     2,     2],\n",
      "        [    2,     2,     2,     2,     2,     2,     1, 24948,   592,  2020,\n",
      "           278, 14744,   338,  7254, 29973,    13,    13,  1576, 14744,  5692,\n",
      "          7254,  1363,   310,   263, 27791,   265,  2000,  9596,   280,  1141,\n",
      "         14801,   292, 29892,   607, 10008,   746,  6575,  4366,  9850, 29879,\n",
      "          1549,   278, 11563, 29915, 29879, 25005, 29889,  1932,  6575,  4366,\n",
      "         24395,   278, 25005, 29892,   372,  2094,  1309,  2153, 21577, 13206,\n",
      "         21337,   310,   330,  2129,  1316]], device='cuda:0')\n",
      "Full generated text (full_text):\n",
      " </s></s></s></s></s></s></s></s></s><s> The capital of France is Paris. The city is located in the northern central part of France and is known for its stunning architecture, art museums, fashion, and romantic atmosphere. Paris is home to many famous landmarks such as the Eiffel Tower, Notre\n",
      "\n",
      "\n",
      "\n",
      "gen_hidden.shape torch.Size([50, 3, 4096])\n",
      "hook_hidden.shape torch.Size([50, 3, 4096])\n",
      "Norm of difference for each generated token: tensor([[2.6328, 1.6592, 2.6094,  ..., 2.2617, 1.6074, 0.8042],\n",
      "        [1.1582, 0.4661, 3.4512,  ..., 1.5791, 2.8027, 2.5020],\n",
      "        [2.0898, 3.1055, 1.0596,  ..., 1.1533, 1.5576, 1.1406],\n",
      "        ...,\n",
      "        [3.7852, 1.8701, 0.7524,  ..., 3.0020, 2.6621, 1.6807],\n",
      "        [0.8530, 1.5684, 2.1875,  ..., 2.5410, 2.7773, 1.7041],\n",
      "        [1.6230, 1.1543, 0.6416,  ..., 2.8379, 5.1445, 2.0254]],\n",
      "       dtype=torch.float16)\n",
      "Cosine similarity per step: tensor([[ 0.9995, -0.1024,  0.7681,  ...,  0.9370,  0.8589,  0.8384],\n",
      "        [ 0.9565,  0.6328,  0.9404,  ...,  0.2700,  0.2469,  0.3105],\n",
      "        [ 0.0523,  0.7090,  0.3022,  ...,  0.3374,  0.2157,  0.1187],\n",
      "        ...,\n",
      "        [ 0.9824,  0.9619,  0.1360,  ...,  0.9463,  0.9590,  0.9468],\n",
      "        [ 0.7471,  0.9707,  0.8755,  ...,  0.9814,  0.9893,  0.9453],\n",
      "        [ 0.8247,  0.9644,  0.9839,  ...,  0.9590,  0.9604,  0.6689]],\n",
      "       dtype=torch.float16)\n",
      "All close? False\n",
      "Inference time: Time elapsed: 0000000 min 0000001 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CODE OK \n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "\n",
    "k_beams=1\n",
    "\n",
    "import time\n",
    "def print_time_elapsed(start, end, label=\"\"):\n",
    "    elapsed = end - start\n",
    "    mins, secs = divmod(elapsed, 60)\n",
    "    print(f\"{label}Time elapsed: {int(mins):07d} min {int(secs):07d} sec\\n\")\n",
    "\n",
    "\n",
    "OUTPUT_HIDDEN_STATES = True\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "\n",
    "prompt = [\n",
    "    \"The capital of France is\",\n",
    "    \"What is the name of the French Queen who was decapitated?\",\n",
    "    \"Tell me why the sky is blue?\"\n",
    "]\n",
    "\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "print(\"Number of tokens in the prompt:\", len(inputs[\"input_ids\"][0]))\n",
    "print(\"Inputs tokens:\", inputs[\"input_ids\"][0])\n",
    "\n",
    "# Define witch layer to retrieve\n",
    "layer_idx = -1  # Last layer\n",
    "\n",
    "\n",
    "activations = []\n",
    "\n",
    "def hook_fn(module, input, output):\n",
    "    # Stocke tous les embeddings du layer pour chaque forward\n",
    "    if layer_idx == -1:\n",
    "        activations.append( model.model.norm(output[0]).detach().cpu()) # post RMSNorm!\n",
    "    else:\n",
    "        activations.append(output[0].detach().cpu())\n",
    "        \n",
    "\n",
    "handle = model.model.layers[layer_idx].register_forward_hook(hook_fn)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=50,\n",
    "        output_hidden_states=OUTPUT_HIDDEN_STATES,\n",
    "        return_dict_in_generate=OUTPUT_HIDDEN_STATES,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "        top_k=50,\n",
    "        num_beams = k_beams,\n",
    "        use_cache=True,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "handle.remove()\n",
    "\n",
    "# activations est une liste de tenseurs (batch_size, seq_len, hidden_size)\n",
    "# Il faudra les organiser selon la logique de génération (voir plus haut)\n",
    "\n",
    "if OUTPUT_HIDDEN_STATES:\n",
    "    print(\"Length of outputs.hidden_states:\", len(outputs.hidden_states))\n",
    "\n",
    "    print(\"Checking the shape of the hidden states:\")\n",
    "    print(\"Length of outputs.hidden_states[0]:\", len(outputs.hidden_states[0]))\n",
    "    print(\n",
    "        \"Shape of outputs.hidden_states[0][layer_idx]:\",\n",
    "        outputs.hidden_states[0][layer_idx].shape,\n",
    "    )\n",
    "    print(\n",
    "        \"Shape of outputs.hidden_states[1][layer_idx]:\",\n",
    "        outputs.hidden_states[1][layer_idx].shape,\n",
    "    )\n",
    "    print(\n",
    "        \"Shape of outputs.hidden_states[2][layer_idx]:\",\n",
    "        outputs.hidden_states[2][layer_idx].shape,\n",
    "    )\n",
    "    print(\n",
    "        \"Shape of outputs.hidden_states[-1][layer_idx]:\",\n",
    "        outputs.hidden_states[-1][layer_idx].shape,\n",
    "    )\n",
    "\n",
    "print(\"=======================================\")\n",
    "print(\"Length of activations:\", len(activations))\n",
    "print(\"Shape of activations[0]:\", activations[0].shape)  # [3, 15, 4096]\n",
    "\n",
    "print(\"Checking the shape of the hidden states:\")\n",
    "print(\"Length of activations[0]:\", len(activations[0]))\n",
    "print(\n",
    "    \"Shape of activations[0]:\",\n",
    "    activations[0].shape,\n",
    ")\n",
    "print(\n",
    "    \"Shape of activations[1]:\",\n",
    "    activations[1].shape,\n",
    ")\n",
    "print(\n",
    "    \"Shape of activations[2]:\",\n",
    "    activations[2].shape,\n",
    ")\n",
    "print(\n",
    "    \"Shape of activations[-1]:\",\n",
    "    activations[-1].shape,\n",
    ")\n",
    "\n",
    "if OUTPUT_HIDDEN_STATES:\n",
    "    # Complete generated sequence (prompt + response)\n",
    "    generated_ids = (\n",
    "        outputs.sequences\n",
    "    )  # Shape - (1, prompt_len + gen_len) = (1, 6+50) = (1, 56)\n",
    "    print(\"generated_ids:\", generated_ids)\n",
    "    full_text = tokenizer.decode(generated_ids[0])\n",
    "    print(\"Full generated text (full_text):\\n\", full_text)\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "# METHOD 1: Retrieve token embeddings with model.generate\n",
    "# =========================================\n",
    "# Extract hidden states from model.generate\n",
    "if OUTPUT_HIDDEN_STATES:\n",
    "    gen_hidden = []\n",
    "    for step in range(\n",
    "        len(outputs.hidden_states)\n",
    "    ):  # Shape - len(outputs.hidden_states) = 50\n",
    "        # Shape - outputs.hidden_states[step][layer_idx] = (1, 1, hidden_size)\n",
    "        gen_hidden.append(outputs.hidden_states[step][layer_idx][:, -1, :])\n",
    "    gen_hidden = torch.stack(gen_hidden, dim=0)  # (gen_len, hidden_size)\n",
    "    print(\"gen_hidden.shape\", gen_hidden.shape)\n",
    "\n",
    "\n",
    "# METHOD 2: Retrieve token embeddings with model.generate + hook\n",
    "# =========================================\n",
    "# Pour chaque forward, on veut le dernier token généré\n",
    "hook_hidden = []\n",
    "for step in range(len(activations)):\n",
    "    # activations[step] : (batch, seq_len, hidden_size)\n",
    "    # On prend le dernier token de la séquence générée à cette étape\n",
    "    hook_hidden.append(activations[step][:, -1, :])\n",
    "hook_hidden = torch.stack(hook_hidden, dim=0)  # (gen_len, hidden_size)\n",
    "print(\"hook_hidden.shape\", hook_hidden.shape)\n",
    "\n",
    "\n",
    "# Alignment and comparison\n",
    "# =========================================\n",
    "# S'assurer que les tenseurs sont sur le même device\n",
    "if OUTPUT_HIDDEN_STATES:\n",
    "    gen_hidden = gen_hidden.to(hook_hidden.device)\n",
    "\n",
    "    # Calculer la norme de la différence pour chaque token généré\n",
    "    diff = torch.norm(gen_hidden - hook_hidden, dim=1)\n",
    "    print(\"Norm of difference for each generated token:\", diff)\n",
    "\n",
    "    # Calculer la similarité cosinus\n",
    "    cosine_sim = torch.nn.functional.cosine_similarity(gen_hidden, hook_hidden, dim=1)\n",
    "    print(\"Cosine similarity per step:\", cosine_sim)\n",
    "\n",
    "    # Vérifier si les résultats sont identiques (à une tolérance numérique près)\n",
    "    print(\"All close?\", torch.allclose(gen_hidden, hook_hidden, atol=1e-5))\n",
    "\n",
    "\n",
    "t1 = time.time()\n",
    "print_time_elapsed(t0, t1, label=\"Inference time: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec le code ci-dessus, on voit que utiliser `model.generate()` donne les memes résultats que utiliser un `hook` durant le generate. Ceci est sutout vrai pour le dernier layer et plus on va dans les premiers layers et plus il y a une différence entre `model.generate()` et le `hook`.  Que je mette `use_cache=True` ou `use_cache=False` on a bien `model.generate()` qui donne les mêmes sorties que le `hook`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in the prompt: 17\n",
      "Inputs tokens: tensor([   2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    1,\n",
      "         450, 7483,  310, 3444,  338], device='cuda:0')\n",
      "=======================================\n",
      "Length of outputs.hidden_states: 50\n",
      "Shape of outputs.hidden_states[0][layer_idx]: torch.Size([8, 17, 4096])\n",
      "Shape of outputs.hidden_states[1][layer_idx]: torch.Size([8, 18, 4096])\n",
      "Shape of outputs.hidden_states[-1][layer_idx]: torch.Size([8, 66, 4096])\n",
      "Length of activations: 100\n",
      "Shape of activations[0]: torch.Size([8, 17, 4096])\n",
      "Shape of activations[1]: torch.Size([8, 17, 4096])\n",
      "Shape of activations[-1]: torch.Size([8, 66, 4096])\n",
      "=======================================\n",
      "generated_ids: tensor([[    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     1,   450,  7483,   310,  3444,   338,  3681, 29889,   739,\n",
      "           338,  5982,   297,   278, 11683,   280, 29899,   311, 29899, 16066,\n",
      "          5120,   322,   338,  2998,   363,   967,   380, 27389, 11258, 29892,\n",
      "          1616, 19133, 29879, 29892, 13460, 29892,   322,  2723,   275,   457,\n",
      "         29889,   450,  4272,   338,  3271,   304,  1784, 13834,  2982, 22848,\n",
      "          1316,   408,   278,   382,  2593,   295, 23615],\n",
      "        [    2,     2,     1,  1724,   338,   278,  1024,   310,   278,  5176,\n",
      "         10470,  1058,   471,  1602,   481, 22731, 29973,    13,    13,  1576,\n",
      "          5176, 10470,  1058,   471,  1602,   481, 22731,   471,  9932, 21089,\n",
      "           262,  2353, 29889,  2296,   471,   278,  1833, 26624,   310,  3444,\n",
      "          1434,   278,  5176, 14595,   322,   471,  8283,   491,  1410,   453,\n",
      "           327,   457,   373,  5533, 29871, 29896, 29953, 29892, 29871, 29896,\n",
      "         29955, 29929, 29941, 29889,     2,     2,     2],\n",
      "        [    2,     2,     2,     2,     2,     2,     2,     2,     1, 24948,\n",
      "           592,  2020,   278, 14744,   338,  7254, 29973,    13,    13,  1576,\n",
      "         14744,  5692,  7254,  1363,   310,   263, 27791,   265,  2000,  9596,\n",
      "           280,  1141, 14801,   292, 29889,  1932,  6575,  4366, 24395, 11563,\n",
      "         29915, 29879, 25005, 29892,   372,  2094,  1309,  2153, 21577, 13206,\n",
      "         21337,   310,   330,  2129,  1316,   408, 21767,   307,  1885,   322,\n",
      "           288, 28596, 29889,  4525, 13206, 21337, 14801],\n",
      "        [    1,  1724,   338,   278,  2927,   310,   278,  5076,   310, 26761,\n",
      "         12537, 29973,   313, 12011,   697,  1734, 29897,    13,    13, 29933,\n",
      "          4708,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Full prompt + generated text (full_text):\n",
      " </s></s></s></s></s></s></s></s></s></s></s><s> The capital of France is Paris. It is located in the Île-de-France region and is known for its stunning architecture, art museums, fashion, and cuisine. The city is home to many famous landmarks such as the Eiffel Tower\n",
      "Full prompt + generated text (full_text):\n",
      " </s></s><s> What is the name of the French Queen who was decapitated?\n",
      "\n",
      "The French Queen who was decapitated was Marie Antoinette. She was the last queen of France before the French Revolution and was executed by guillotine on October 16, 1793.</s></s></s>\n",
      "Full prompt + generated text (full_text):\n",
      " </s></s></s></s></s></s></s></s><s> Tell me why the sky is blue?\n",
      "\n",
      "The sky appears blue because of a phenomenon called Rayleigh scattering. When sunlight enters Earth's atmosphere, it encounters tiny molecules of gases such as nitrogen and oxygen. These molecules scatter\n",
      "Full prompt + generated text (full_text):\n",
      " <s> What is the color of the eyes of Elisabeth Taylor? (answer one word)\n",
      "\n",
      "Brown</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>\n",
      "=======================================\n",
      "\n",
      "\n",
      "\n",
      "aligned_hidden_states.shape: torch.Size([4, 50, 4096])\n",
      "Inference time: Time elapsed: 0000000 min 0000002 sec\n",
      "\n",
      "generation_mask: tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Semble ok pour le beam search et attention mask de la génération\n",
    "# maintenant il faut l'adapter pour activations car pour le moment c'est fait que pour \n",
    "# output.hidden_states\n",
    "# ==========================\n",
    "import torch\n",
    "import time\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "\n",
    "k_beams = 2\n",
    "\n",
    "def print_time_elapsed(start, end, label=\"\"):\n",
    "    elapsed = end - start\n",
    "    mins, secs = divmod(elapsed, 60)\n",
    "    print(f\"{label}Time elapsed: {int(mins):07d} min {int(secs):07d} sec\\n\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "prompts = [\n",
    "    \"The capital of France is\",\n",
    "    \"What is the name of the French Queen who was decapitated?\",\n",
    "    \"Tell me why the sky is blue?\",\n",
    "    \"What is the color of the eyes of Elisabeth Taylor? (answer one word)\"\n",
    "]\n",
    "\n",
    "inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "batch_size = inputs[\"input_ids\"].shape[0]\n",
    "print(\"Number of tokens in the prompt:\", len(inputs[\"input_ids\"][0]))\n",
    "print(\"Inputs tokens:\", inputs[\"input_ids\"][0])\n",
    "\n",
    "layer_idx = -1  # Last layer\n",
    "\n",
    "activations = []\n",
    "\n",
    "def hook_fn(module, input, output):\n",
    "    if layer_idx == -1:\n",
    "        activations.append(model.model.norm(output[0]).detach().cpu())\n",
    "    else:\n",
    "        activations.append(output[0].detach().cpu())\n",
    "\n",
    "handle = model.model.layers[layer_idx].register_forward_hook(hook_fn)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=50,\n",
    "        output_hidden_states=True,\n",
    "        return_dict_in_generate=True,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "        top_k=50,\n",
    "        num_beams=k_beams,\n",
    "        use_cache=False,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "handle.remove()\n",
    "\n",
    "print(\"=======================================\")\n",
    "\n",
    "print(\"Length of outputs.hidden_states:\", len(outputs.hidden_states))\n",
    "print(\"Shape of outputs.hidden_states[0][layer_idx]:\", outputs.hidden_states[0][layer_idx].shape)\n",
    "print(\"Shape of outputs.hidden_states[1][layer_idx]:\", outputs.hidden_states[1][layer_idx].shape)\n",
    "print(\"Shape of outputs.hidden_states[-1][layer_idx]:\", outputs.hidden_states[-1][layer_idx].shape)\n",
    "\n",
    "print(\"Length of activations:\", len(activations))\n",
    "print(\"Shape of activations[0]:\", activations[0].shape)\n",
    "print(\"Shape of activations[1]:\", activations[1].shape)\n",
    "print(\"Shape of activations[-1]:\", activations[-1].shape)\n",
    "\n",
    "print(\"=======================================\")\n",
    "generated_ids = outputs.sequences\n",
    "print(\"generated_ids:\", generated_ids)\n",
    "print(\"\\n\\n\")\n",
    "for s in generated_ids:\n",
    "    full_text = tokenizer.decode(s)\n",
    "    print(\"Full prompt + generated text (full_text):\\n\", full_text)\n",
    "print(\"=======================================\")\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "# Extraction vectorisée des hidden states alignés (méthode 2)\n",
    "gen_len = outputs.beam_indices.shape[1] # = len(activations) = len(outputs.hidden_states)\n",
    "hidden_size = outputs.hidden_states[0][layer_idx].shape[-1]\n",
    "# hidden_states corresponding to the selected beam \n",
    "aligned_hidden_states = torch.zeros((batch_size, gen_len, hidden_size), dtype=outputs.hidden_states[0][layer_idx].dtype)\n",
    "\n",
    "for step in range(gen_len):\n",
    "    # hidden_states[step][layer_idx]: (batch_size * k_beams, seq_len, hidden_size)\n",
    "    h = outputs.hidden_states[step][layer_idx]  # (batch_size * k_beams, seq_len, hidden_size)\n",
    "    beam_indices = outputs.beam_indices[:, step].to(h.device)  # (batch_size,)\n",
    "    valid = beam_indices >= 0\n",
    "    if valid.any():\n",
    "        aligned_hidden_states = aligned_hidden_states.to(h.device) \n",
    "        aligned_hidden_states[valid, step, :] = h[beam_indices[valid], -1, :]  # (batch_size_valid, hidden_size)\n",
    "        # -1: last token generated at this step for each beam selected.\n",
    "\n",
    "print(\"aligned_hidden_states.shape:\", aligned_hidden_states.shape)\n",
    "# aligned_hidden_states contains for each prompt in the batch and \n",
    "# for each generation step, the hidden state vector associated with the last token \n",
    "# generated for the sequence selected by the beam search.\n",
    "\n",
    "# aligned_hidden_states[i] contient les embeddings alignés pour le prompt i\n",
    "\n",
    "t1 = time.time()\n",
    "print_time_elapsed(t0, t1, label=\"Inference time: \")\n",
    "\n",
    "\n",
    "# Construction of the generation_mask\n",
    "# Assumes that during generation all sequences are padded with the EOS token, and not with random tokens. \n",
    "# =====================================\n",
    "prompt_len = inputs[\"input_ids\"].shape[1] # Assumes all prompts are padded to the same length\n",
    "gen_only_ids = generated_ids[:, prompt_len:] # Shape: (batch_size, gen_len)\n",
    "batch_size, gen_len = gen_only_ids.shape\n",
    "\n",
    "# Initialize the generation mask to identify which tokens are padded during generation\n",
    "# Positions will be marked as True up to and including the first eos_token_id\n",
    "generation_mask = torch.zeros((batch_size, gen_len), dtype=torch.bool, device=gen_only_ids.device)\n",
    "\n",
    "eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Create a boolean mask of where eos_token_id appears in the generated tokens\n",
    "eos_mask = (gen_only_ids == eos_token_id) # Shape: (batch_size, gen_len) \n",
    "\n",
    "# Initialize a tensor with max length for each sequence = gen_len\n",
    "# (meaning: if there is no eos, we consider the whole sequence as valid)\n",
    "eos_positions = torch.full((batch_size,), gen_len, dtype=torch.long, device=gen_only_ids.device)\n",
    "\n",
    "# Find which sequences actually contain at least one eos_token_id\n",
    "any_eos = eos_mask.any(dim=1) # Shape: (batch_size,) -> True where at least one eos is found\n",
    "\n",
    "# For sequences that contain an eos_token_id, find the index of its first occurrence\n",
    "# Note: .float().argmax(dim=1) works because True -> 1.0, False -> 0.0\n",
    "# So argmax will return the first position where eos_token_id == True\n",
    "eos_positions[any_eos] = eos_mask[any_eos].float().argmax(dim=1)\n",
    "\n",
    "# Create a tensor with position indices: [0, 1, 2, ..., gen_len - 1], repeated for each batch item\n",
    "position_ids = torch.arange(gen_len, device=gen_only_ids.device).unsqueeze(0).expand(batch_size, -1) # Shape: (batch_size, gen_len)\n",
    "\n",
    "# Build the final generation mask by marking positions as True\n",
    "# for all positions up to and including the first eos_token_id\n",
    "generation_mask = position_ids <= eos_positions.unsqueeze(1)\n",
    "\n",
    "print(\"generation_mask:\", generation_mask)\n",
    "\n",
    "# IL faudra gérer le padding ensuite \n",
    "# et notamment le padding de la génération \n",
    "# Sauf que mes fonctions extract_average_token_activations, extract_last_token_activations et extract_max_token_activations\n",
    "# supposent qu'un seul type de padding et souvent left. Or, moi si je cree un attention mask de la génération\n",
    "# il y aura de right padding car la génération est right padded. \n",
    "# donc il faudra a mon avis modifier les fonctions extract_..._token_activations pour gérer ces cas. \n",
    "# ou alors je padde la réponse générée à gauche mais c'est moins propre\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float16"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.hidden_states[0][layer_idx].dtype\n",
    "activations[0].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in the prompt: 17\n",
      "Inputs tokens: tensor([   2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    1,\n",
      "         450, 7483,  310, 3444,  338], device='cuda:0')\n",
      "=======================================\n",
      "Length of activations: 50\n",
      "Shape of activations[0]: torch.Size([8, 17, 4096])\n",
      "Shape of activations[1]: torch.Size([8, 18, 4096])\n",
      "Shape of activations[-1]: torch.Size([8, 66, 4096])\n",
      "=======================================\n",
      "generated_ids: tensor([[    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     1,   450,  7483,   310,  3444,   338,  3681, 29889,   739,\n",
      "           338,  5982,   297,   278, 11683,   280, 29899,   311, 29899, 16066,\n",
      "          5120,   322,   338,  2998,   363,   967, 16375, 29892, 15839, 29892,\n",
      "           322,  1616,  4695,   902, 16639, 29889,   450,  4272,   338,  3271,\n",
      "           304,  1784, 13834,  2982, 22848,  1316,   408,   278,   382,  2593,\n",
      "           295, 23615, 29892,   278,  4562, 12675,  6838],\n",
      "        [    2,     2,     1,  1724,   338,   278,  1024,   310,   278,  5176,\n",
      "         10470,  1058,   471,  1602,   481, 22731, 29973,    13,    13,  1576,\n",
      "          5176, 10470,  1058,   471,  1602,   481, 22731,   471,  9932, 21089,\n",
      "           262,  2353, 29889,  2296,   471,   278,  1833, 26624,   310,  3444,\n",
      "          1434,   278,  5176, 14595,   322,   471,  8283,   491,  1410,   453,\n",
      "           327,   457,   373,  5533, 29871, 29896, 29953, 29892, 29871, 29896,\n",
      "         29955, 29929, 29941, 29889,     2,     2,     2],\n",
      "        [    2,     2,     2,     2,     2,     2,     2,     2,     1, 24948,\n",
      "           592,  2020,   278, 14744,   338,  7254, 29973,    13,    13,  1576,\n",
      "         14744,  5692,  7254,  1363,   310,   263, 27791,   265,  2000,  9596,\n",
      "           280,  1141, 14801,   292, 29889,  1932,  6575,  4366, 24395, 11563,\n",
      "         29915, 29879, 25005, 29892,   372,  2094,  1309,  2153, 21577, 13206,\n",
      "         21337,   310,   330,  2129,  1316,   408, 21767,   307,  1885,   322,\n",
      "           288, 28596, 29889,  4525, 13206, 21337, 14801],\n",
      "        [    1,  1724,   338,   278,  2927,   310,   278,  5076,   310, 26761,\n",
      "         12537, 29973,   313, 12011,   697,  1734, 29897,    13,    13, 29933,\n",
      "          4708,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Full prompt + generated text (full_text):\n",
      " </s></s></s></s></s></s></s></s></s></s></s><s> The capital of France is Paris. It is located in the Île-de-France region and is known for its cultural, historical, and artistic heritage. The city is home to many famous landmarks such as the Eiffel Tower, the Louvre Museum\n",
      "Full prompt + generated text (full_text):\n",
      " </s></s><s> What is the name of the French Queen who was decapitated?\n",
      "\n",
      "The French Queen who was decapitated was Marie Antoinette. She was the last queen of France before the French Revolution and was executed by guillotine on October 16, 1793.</s></s></s>\n",
      "Full prompt + generated text (full_text):\n",
      " </s></s></s></s></s></s></s></s><s> Tell me why the sky is blue?\n",
      "\n",
      "The sky appears blue because of a phenomenon called Rayleigh scattering. When sunlight enters Earth's atmosphere, it encounters tiny molecules of gases such as nitrogen and oxygen. These molecules scatter\n",
      "Full prompt + generated text (full_text):\n",
      " <s> What is the color of the eyes of Elisabeth Taylor? (answer one word)\n",
      "\n",
      "Brown</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>\n",
      "=======================================\n",
      "\n",
      "\n",
      "\n",
      "aligned_hidden_states.shape: torch.Size([4, 50, 4096])\n",
      "Inference time: Time elapsed: 0000000 min 0000002 sec\n",
      "\n",
      "generation_mask: tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "\n",
    "k_beams = 2\n",
    "\n",
    "\n",
    "prompts = [\n",
    "    \"The capital of France is\",\n",
    "    \"What is the name of the French Queen who was decapitated?\",\n",
    "    \"Tell me why the sky is blue?\",\n",
    "    \"What is the color of the eyes of Elisabeth Taylor? (answer one word)\"\n",
    "]\n",
    "\n",
    "inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "batch_size = inputs[\"input_ids\"].shape[0]\n",
    "print(\"Number of tokens in the prompt:\", len(inputs[\"input_ids\"][0]))\n",
    "print(\"Inputs tokens:\", inputs[\"input_ids\"][0])\n",
    "\n",
    "layer_idx = -1  # Last layer\n",
    "\n",
    "activations = []\n",
    "# The hook plays the role of `output_hidden_states=True` and `return_dict_in_generate=True`\n",
    "# in `model.generate()` but it is faster and more memory efficient because we retrieve exactly\n",
    "# the actiavtions wanted. \n",
    "def hook_fn(module, input, output):\n",
    "    \"\"\"Function called automatically by PyTorch just after\n",
    "        the layer has produced its output during the forward pass.\"\"\"\n",
    "    # output is a tuple (hidden_states,) → keep [0]\n",
    "    if layer_idx == -1:\n",
    "        activations.append(model.model.norm(output[0]).detach().cpu()) # post RMSNorm!\n",
    "    else:\n",
    "        activations.append(output[0].detach().cpu())\n",
    "\n",
    "handle = model.model.layers[layer_idx].register_forward_hook(hook_fn)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=50,\n",
    "        output_hidden_states=False, # hook will do the job of ` output_hidden_states=True` in a more efficient way\n",
    "        return_dict_in_generate=True,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "        top_k=50,\n",
    "        num_beams=k_beams,\n",
    "        use_cache=False,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "handle.remove()\n",
    "\n",
    "print(\"=======================================\")\n",
    "\n",
    "print(\"Length of activations:\", len(activations))\n",
    "print(\"Shape of activations[0]:\", activations[0].shape)\n",
    "print(\"Shape of activations[1]:\", activations[1].shape)\n",
    "print(\"Shape of activations[-1]:\", activations[-1].shape)\n",
    "\n",
    "print(\"=======================================\")\n",
    "generated_ids = outputs.sequences\n",
    "print(\"generated_ids:\", generated_ids)\n",
    "print(\"\\n\\n\")\n",
    "for s in generated_ids:\n",
    "    full_text = tokenizer.decode(s)\n",
    "    print(\"Full prompt + generated text (full_text):\\n\", full_text)\n",
    "print(\"=======================================\")\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "# Extraction vectorisée des hidden states alignés (méthode 2)\n",
    "gen_len = outputs.beam_indices.shape[1] # = len(activations)\n",
    "hidden_size = activations[0].shape[-1]\n",
    "# hidden_states corresponding to the selected beam \n",
    "aligned_hidden_states = torch.zeros((batch_size, gen_len, hidden_size), dtype=activations[0].dtype)\n",
    "\n",
    "for step in range(gen_len):\n",
    "    h = activations[step] # Shape (batch_size * k_beams, seq_len, hidden_size)\n",
    "    beam_indices = outputs.beam_indices[:, step].to(h.device)  # Shape (batch_size,)\n",
    "    valid = beam_indices >= 0\n",
    "    if valid.any():\n",
    "        aligned_hidden_states = aligned_hidden_states.to(h.device) \n",
    "        aligned_hidden_states[valid, step, :] = h[beam_indices[valid], -1, :]  # Shape (batch_size_valid, hidden_size)\n",
    "        # -1: last token generated at this step for each beam selected.\n",
    "\n",
    "print(\"aligned_hidden_states.shape:\", aligned_hidden_states.shape) \n",
    "# aligned_hidden_states contains for each prompt in the batch and \n",
    "# for each generation step, the hidden state vector associated with the last token \n",
    "# generated for the sequence selected by the beam search.\n",
    "\n",
    "# Construction of the generation_mask\n",
    "# Assumes that during generation all sequences are padded with the EOS token, and not with random tokens. \n",
    "# =====================================\n",
    "prompt_len = inputs[\"input_ids\"].shape[1] # Assumes all prompts are padded to the same length\n",
    "gen_only_ids = generated_ids[:, prompt_len:] # Shape: (batch_size, gen_len)\n",
    "\n",
    "# Initialize the generation mask to identify which tokens are padded during generation\n",
    "# Positions will be marked as True up to and including the first eos_token_id\n",
    "generation_mask = torch.zeros((batch_size, gen_len), dtype=torch.bool, device=gen_only_ids.device)\n",
    "\n",
    "eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Create a boolean mask of where eos_token_id appears in the generated tokens\n",
    "eos_mask = (gen_only_ids == eos_token_id) # Shape: (batch_size, gen_len) \n",
    "\n",
    "# Initialize a tensor with max length for each sequence = gen_len\n",
    "# (meaning: if there is no eos, we consider the whole sequence as valid)\n",
    "eos_positions = torch.full((batch_size,), gen_len, dtype=torch.long, device=gen_only_ids.device)\n",
    "\n",
    "# Find which sequences actually contain at least one eos_token_id\n",
    "any_eos = eos_mask.any(dim=1) # Shape: (batch_size,) -> True where at least one eos is found\n",
    "\n",
    "# For sequences that contain an eos_token_id, find the index of its first occurrence\n",
    "# Note: .float().argmax(dim=1) works because True -> 1.0, False -> 0.0\n",
    "# So argmax will return the first position where eos_token_id == True\n",
    "eos_positions[any_eos] = eos_mask[any_eos].float().argmax(dim=1)\n",
    "\n",
    "# Create a tensor with position indices: [0, 1, 2, ..., gen_len - 1], repeated for each batch item\n",
    "position_ids = torch.arange(gen_len, device=gen_only_ids.device).unsqueeze(0).expand(batch_size, -1) # Shape: (batch_size, gen_len)\n",
    "\n",
    "# Build the final generation mask by marking positions as True\n",
    "# for all positions up to and including the first eos_token_id\n",
    "generation_mask = position_ids <= eos_positions.unsqueeze(1)\n",
    "\n",
    "print(\"generation_mask:\", generation_mask)\n",
    "\n",
    "# IL faudra gérer le padding ensuite \n",
    "# et notamment le padding de la génération \n",
    "# Sauf que mes fonctions extract_average_token_activations, extract_last_token_activations et extract_max_token_activations\n",
    "# supposent qu'un seul type de padding et souvent left. Or, moi si je cree un attention mask de la génération\n",
    "# il y aura de right padding car la génération est right padded. \n",
    "# donc il faudra a mon avis modifier les fonctions extract_..._token_activations pour gérer ces cas. \n",
    "# ou alors je padde la réponse générée à gauche mais c'est moins propre\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedModel\n",
    "from typing import List\n",
    "import torch\n",
    "from torch.utils.hooks import RemovableHandle\n",
    "\n",
    "\n",
    "\n",
    "def hook_generation_activations(\n",
    "    model: PreTrainedModel,\n",
    "    activations_list: List[torch.Tensor],\n",
    "    layer_idx: int = -1\n",
    ") -> RemovableHandle:\n",
    "    \"\"\"\n",
    "    Attaches a forward hook to a specific transformer layer to capture hidden states\n",
    "    during autoregressive text generation (i.e., at each decoding step).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : PreTrainedModel\n",
    "        The Hugging Face causal language model (e.g., GPT, LLaMA).\n",
    "    activations_list : List[torch.Tensor]\n",
    "        A list that will be filled with hidden states for each generation step. \n",
    "        Each tensor has shape (batch_size * num_beams, seq_len, hidden_size).\n",
    "    layer_idx : int\n",
    "        Index of the transformer block to hook. Defaults to -1 (the last layer).\n",
    "        Use a positive integer if you want to hook an intermediate layer instead.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    RemovableHandle : A handle object\n",
    "        Call `handle.remove()` after generation to remove the hook.\n",
    "    \n",
    "    \"\"\"\n",
    "    call_counter = torch.tensor(0)\n",
    "\n",
    "    def hook_fn(module, input, output):\n",
    "        \"\"\"Function called automatically by PyTorch just after\n",
    "            the layer has produced its output during the forward pass.\"\"\"\n",
    "        # output is a tuple (hidden_states,) → keep [0]\n",
    "\n",
    "        nonlocal call_counter\n",
    "        call_counter += 1  # count how many times the hook is triggered\n",
    "        \n",
    "        if layer_idx == -1:\n",
    "            # Capture the final normalized output \n",
    "            activations.append(model.model.norm(output[0]).detach().cpu())  # post RMSNorm!\n",
    "        else:\n",
    "            # Capture raw hidden states before layer normalization\n",
    "            activations.append(output[0].detach().cpu()) ###### ????? ######\n",
    "    \n",
    "    # Register hook on the transformer block\n",
    "    # When Pytorch pass through this layer during forward pass, it also execute hook_fn.\n",
    "    handle = model.model.layers[layer_idx].register_forward_hook(hook_fn)\n",
    "    \n",
    "    return handle, call_counter\n",
    "\n",
    "## FAUT IL FAIRE QQCH POUR calculer le layer comme avec get_layer_output ? \n",
    "\n",
    "\n",
    "\n",
    "from transformers import PreTrainedModel, PreTrainedTokenizer, BatchEncoding\n",
    "from typing import Union, Dict\n",
    "import torch\n",
    "\n",
    "\n",
    "def generate(\n",
    "    model: PreTrainedModel,\n",
    "    inputs: BatchEncoding,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    max_new_tokens: int = 50,\n",
    "    k_beams: int = 1,\n",
    "    **generate_kwargs\n",
    ") -> Union[torch.Tensor, Dict[str, torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Generate sequences from the model with optional beam search.\n",
    "    Supports advanced options via **generate_kwargs (e.g., output_attentions).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : PreTrainedModel\n",
    "        The language model to use for generation.\n",
    "    inputs : BatchEncoding\n",
    "        Tokenized input prompts.\n",
    "    tokenizer : PreTrainedTokenizer\n",
    "        Tokenizer providing eos and pad token IDs.\n",
    "    max_new_tokens : int, optional\n",
    "        Maximum number of new tokens to generate.\n",
    "    k_beams : int, optional\n",
    "        Number of beams to use. If 1, uses sampling. If >1, beam search is enabled.\n",
    "    **generate_kwargs : dict\n",
    "        Additional keyword arguments passed to `model.generate()`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Union[torch.Tensor, Dict[str, torch.Tensor]]\n",
    "        - If k_beams == 1:\n",
    "            Returns a tensor of generated token IDs: shape (batch_size, prompt_len + gen_len)\n",
    "        - If k_beams > 1:\n",
    "            Returns a dictionary with keys:\n",
    "                - \"sequences\": the generated token IDs\n",
    "                - \"beam_indices\": the beam path for each token\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.6,\n",
    "            top_p=0.9,\n",
    "            top_k=50,\n",
    "            num_beams=k_beams,\n",
    "            use_cache=True, \n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.eos_token_id, # Ensures clean padding (right padding using eos token)\n",
    "            output_hidden_states=False,      # We rely on the hook to extract hidden states instead (more memory efficient)\n",
    "            return_dict_in_generate=True,    # Needed for access to beam_indices when num_beams > 1\n",
    "            **generate_kwargs                # For future flexibility (e.g., output_attentions, output_scores)\n",
    "        )\n",
    "        return outputs \n",
    "    \n",
    "\n",
    "def build_generation_attention_mask(\n",
    "    prompt_and_gen_ids: torch.Tensor,\n",
    "    prompt_len: int,\n",
    "    eos_token_id: int\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Build an attention mask for the generated part of sequences, marking all tokens up to and \n",
    "    including the first EOS token as valid (True), and the rest as padding (False).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    prompt_and_gen_ids : torch.Tensor\n",
    "        Tensor of shape (batch_size, prompt_len + gen_len) containing full sequences.\n",
    "    prompt_len : int\n",
    "        Number of tokens in the prompt (to isolate generated tokens).\n",
    "    eos_token_id : int\n",
    "        ID of the EOS token used for padding and stopping generation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        Boolean tensor of shape (batch_size, gen_len), where True marks valid generated tokens.\n",
    "    \"\"\"\n",
    "    batch_size, _ = prompt_and_gen_ids.shape\n",
    "\n",
    "    # Extract only the generated tokens IDs (excluding the prompt part)\n",
    "    gen_only_ids = prompt_and_gen_ids[:, prompt_len:]  # Shape: (batch_size, gen_len)\n",
    "    gen_len = gen_only_ids.shape[1]\n",
    "\n",
    "    # Create a boolean mask with True values where tokens equal to eos_token_id\n",
    "    eos_mask = (gen_only_ids == eos_token_id) # Shape: (batch_size, gen_len)\n",
    "    \n",
    "    # Default eos position = gen_len (means: no eos -> whole sequence is valid)\n",
    "    eos_positions = torch.full((batch_size,), gen_len, dtype=torch.long, device=gen_only_ids.device)\n",
    "\n",
    "    # Find first eos position for sequences that have one\n",
    "    any_eos = eos_mask.any(dim=1)  # Find which sequences actually contain at least one eos_token_id - Shape: (batch_size,)\n",
    "    eos_positions[any_eos] = eos_mask[any_eos].float().argmax(dim=1) # argmax returns the 1st position where eos_token_id == True\n",
    "\n",
    "    # Generate a position index tensor (e.g., [0, 1, ..., gen_len-1]), repeated for each batch item\n",
    "    position_ids = torch.arange(gen_len, device=gen_only_ids.device).unsqueeze(0).expand(batch_size, -1) # Shape: (batch_size, gen_len)\n",
    "\n",
    "    # Final generation attetion mask: True for all positions <= first eos (included)\n",
    "    generation_attention_mask = position_ids <= eos_positions.unsqueeze(1) # Shape (batch_size, gen_len)\n",
    "\n",
    "    return generation_attention_mask\n",
    "\n",
    "\n",
    "\n",
    "from typing import List\n",
    "import torch\n",
    "\n",
    "def align_generation_hidden_states(\n",
    "    activations: List[torch.Tensor],\n",
    "    beam_indices: torch.Tensor,\n",
    "    k_beams: int\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    If k_beams > 1, aligns (extracts) the hidden states from `activations` that \n",
    "    correspond to the generated sequence selected by the beam search algorithm. \n",
    "    If k_beams == 1, returns stacked outputs for greedy/top-k decoding.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    activations : List[torch.Tensor]\n",
    "        List of activation tensors per generation step. Each tensor has shape\n",
    "        (batch_size * k_beams, seq_len, hidden_size) if k_beams > 1,\n",
    "        or (batch_size, seq_len, hidden_size) if k_beams == 1.\n",
    "\n",
    "    beam_indices : torch.Tensor\n",
    "        Tensor of shape (batch_size, gen_len) indicating which beam was selected at each step.\n",
    "\n",
    "    k_beams : int\n",
    "        Number of beams used during generation (1 = greedy/top-k, >1 = beam search).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        Aligned hidden states of shape (batch_size, gen_len, hidden_size).\n",
    "    \"\"\"\n",
    "    gen_len = len(activations) # gen_len = max_new_tokens from `model.generate()`\n",
    "    hidden_size = activations[0].shape[-1]\n",
    "    batch_size = beam_indices.shape[0] if k_beams > 1 else activations[0].shape[0]\n",
    "\n",
    "    if k_beams > 1:\n",
    "        # Allocate tensor for aligned hidden states for selected beams\n",
    "        aligned_hidden_states = torch.zeros((batch_size, gen_len, hidden_size), dtype=activations[0].dtype)\n",
    "        \n",
    "        # Align hidden states for the selected beams\n",
    "        for step in range(gen_len):\n",
    "            h = activations[step]  # Shape: (batch_size * k_beams, seq_len, hidden_size)\n",
    "            indices = beam_indices[:, step].to(h.device)  # Shape: (batch_size,)\n",
    "            valid = indices >= 0\n",
    "            if valid.any():\n",
    "                # For each batch item, select the last generated hidden state at this step, for the selected beam sequence \n",
    "                aligned_hidden_states[valid, step, :] = h[indices[valid], -1, :] # Shape (batch_size_valid, hidden_size)\n",
    "    else:\n",
    "        # No beam alignment needed, output comes directly from top-k sampling\n",
    "        # For each batch item, take the last generated hidden state at this step\n",
    "        aligned_hidden_states = torch.stack(\n",
    "            [h[:, -1, :] for h in activations], dim=1\n",
    "        ).to(activations[0].device)  # Shape: (batch_size, gen_len, hidden_size)\n",
    "\n",
    "    return aligned_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hook was called 50 times.\n",
      "=======================================\n",
      "Length of activations: 50\n",
      "Shape of activations[0]: torch.Size([8, 17, 4096])\n",
      "Shape of activations[1]: torch.Size([8, 1, 4096])\n",
      "Shape of activations[-1]: torch.Size([8, 1, 4096])\n",
      "=======================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# ===============================\n",
    "# INITIAL SETUP\n",
    "# ===============================\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "\n",
    "k_beams = 2  # Number of beams for beam search\n",
    "layer_idx = -1  # -1 corresponds to the final transformer block (pre-head)\n",
    "\n",
    "prompts = [\n",
    "    \"The capital of France is\",\n",
    "    \"What is the name of the French Queen who was decapitated?\",\n",
    "    \"Tell me why the sky is blue?\",\n",
    "    \"What is the color of the eyes of Elisabeth Taylor? (answer one word)\"\n",
    "]\n",
    "\n",
    "# Tokenize inputs and move to the model's device\n",
    "inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "batch_size = inputs[\"input_ids\"].shape[0]\n",
    "prompt_len = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "# ===============================\n",
    "# TEXT GENERATION + ACTIVATION CAPTURE\n",
    "# ===============================\n",
    "\n",
    "# This hook collects the hidden states at each decoding step\n",
    "activations = [] # activations[k] of Shape: (batch_size * k_beams, seq_len, hidden_size)\n",
    "handle, call_counter = hook_generation_activations(model, activations, layer_idx=layer_idx)\n",
    "\n",
    "# Generate text from prompts using beam search or sampling\n",
    "outputs = generate(model,inputs,tokenizer,max_new_tokens=50,k_beams=k_beams)\n",
    "\n",
    "# Remove the hook to avoid memory leaks or duplicate logging\n",
    "handle.remove() \n",
    "\n",
    "print(f\"Hook was called {call_counter.item()} times.\")\n",
    "print(\"=======================================\")\n",
    "print(\"Length of activations:\", len(activations))\n",
    "print(\"Shape of activations[0]:\", activations[0].shape)\n",
    "print(\"Shape of activations[1]:\", activations[1].shape)\n",
    "print(\"Shape of activations[-1]:\", activations[-1].shape)\n",
    "print(\"=======================================\")\n",
    "\n",
    "# ===============================\n",
    "# ALIGN GENERATED HIDDEN STATES\n",
    "# ===============================\n",
    "# Extract the hidden states that correspond to the generated sequence\n",
    "# selected by the beam search (or top-k sampling if k_beams = 1)\n",
    "aligned_hidden_states = align_generation_hidden_states(\n",
    "    activations=activations,\n",
    "    beam_indices=outputs.beam_indices,\n",
    "    k_beams=k_beams\n",
    ")\n",
    "\n",
    "# ===============================\n",
    "# BUILD GENERATION ATTENTION MASK\n",
    "# ===============================\n",
    "# This mask marks which generated tokens are valid (i.e., not padding).\n",
    "# Positions are marked True up to and including the first eos_token_id\n",
    "generation_attention_mask = build_generation_attention_mask(\n",
    "    prompt_and_gen_ids=outputs.sequences,\n",
    "    prompt_len=prompt_len,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Optional notes:\n",
    "# Your downstream functions like extract_average_token_activations(), etc.\n",
    "# should now consume `aligned_hidden_states` and `generation_mask`.\n",
    "# These masks are based on right-padding with eos_token_id (not left-padding),\n",
    "# so make sure your logic accounts for this.\n",
    "\n",
    "# If needed, you can reverse-pad the activations, but it's often cleaner\n",
    "# to keep right-padding and adjust the reduction functions accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde du code \n",
    "'''\n",
    "import torch\n",
    "\n",
    "# ===============================\n",
    "# 1. INITIAL SETUP\n",
    "# ===============================\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "\n",
    "k_beams = 2                  # Number of beams for beam search\n",
    "layer_idx = -1              # -1 refers to the final transformer block (before the head)\n",
    "\n",
    "prompts = [\n",
    "    \"The capital of France is\",\n",
    "    \"What is the name of the French Queen who was decapitated?\",\n",
    "    \"Tell me why the sky is blue?\",\n",
    "    \"What is the color of the eyes of Elisabeth Taylor? (answer one word)\"\n",
    "]\n",
    "\n",
    "# Tokenize inputs and move to the model's device\n",
    "inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "batch_size = inputs[\"input_ids\"].shape[0]\n",
    "prompt_len = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "# ===============================\n",
    "# 2. TEXT GENERATION + ACTIVATION CAPTURE\n",
    "# ===============================\n",
    "\n",
    "# This hook collects the hidden states at each decoding step\n",
    "activations = []  # List of shape: (batch_size * k_beams, seq_len, hidden_size)\n",
    "handle, call_counter = hook_generation_activations(model, activations, layer_idx=layer_idx)\n",
    "\n",
    "# Generate text from prompts using beam search or sampling\n",
    "outputs = generate(model, inputs, tokenizer, max_new_tokens=50, k_beams=k_beams)\n",
    "\n",
    "# Remove the hook after generation to prevent memory leaks or duplicate logging\n",
    "handle.remove()\n",
    "\n",
    "print(f\"Hook was called {call_counter.item()} times.\")\n",
    "print(\"=======================================\")\n",
    "print(\"Length of activations:\", len(activations))\n",
    "print(\"Shape of activations[0]:\", activations[0].shape)\n",
    "print(\"Shape of activations[1]:\", activations[1].shape)\n",
    "print(\"Shape of activations[-1]:\", activations[-1].shape)\n",
    "print(\"=======================================\")\n",
    "\n",
    "# ===============================\n",
    "# 3. ALIGN GENERATED HIDDEN STATES\n",
    "# ===============================\n",
    "\n",
    "# Extract the hidden states that correspond to the generated sequence\n",
    "# selected by the beam search (or top-k sampling if k_beams = 1)\n",
    "\n",
    "gen_len = len(activations)                     # Should equal max_new_tokens\n",
    "hidden_size = activations[0].shape[-1]\n",
    "\n",
    "if k_beams > 1:\n",
    "    # Allocate tensor for aligned hidden states for selected beams\n",
    "    aligned_hidden_states = torch.zeros((batch_size, gen_len, hidden_size), dtype=activations[0].dtype)\n",
    "\n",
    "    for step in range(gen_len):\n",
    "        h = activations[step]  # Shape: (batch_size * k_beams, seq_len, hidden_size)\n",
    "        beam_indices = outputs.beam_indices[:, step].to(h.device)  # Shape: (batch_size,)\n",
    "        valid = beam_indices >= 0\n",
    "        if valid.any():\n",
    "            # Select the hidden state of the last generated token at this step\n",
    "            aligned_hidden_states[valid, step, :] = h[beam_indices[valid], -1, :]\n",
    "else:\n",
    "    # No beam alignment needed for sampling (k_beams == 1)\n",
    "    # Stack the final token activation at each step for each sequence\n",
    "    aligned_hidden_states = torch.stack(\n",
    "        [h[:, -1, :] for h in activations], dim=1\n",
    "    ).to(activations[0].device)  # Shape: (batch_size, gen_len, hidden_size)\n",
    "\n",
    "# ===============================\n",
    "# 4. BUILD GENERATION ATTENTION MASK\n",
    "# ===============================\n",
    "\n",
    "# This mask marks which generated tokens are valid (i.e., not padding)\n",
    "# Positions are marked True up to and including the first eos_token_id\n",
    "\n",
    "# Extract generated token IDs (excluding prompt part)\n",
    "prompt_and_gen_ids = outputs.sequences                      # Shape: (batch_size, prompt_len + gen_len)\n",
    "gen_only_ids = prompt_and_gen_ids[:, prompt_len:]           # Shape: (batch_size, gen_len)\n",
    "\n",
    "# Create boolean mask where tokens are equal to eos_token_id\n",
    "eos_token_id = tokenizer.eos_token_id\n",
    "eos_mask = (gen_only_ids == eos_token_id)                   # Shape: (batch_size, gen_len)\n",
    "\n",
    "# Default: if no eos found, all positions are valid (eos at end)\n",
    "eos_positions = torch.full((batch_size,), gen_len, dtype=torch.long, device=gen_only_ids.device)\n",
    "\n",
    "# Identify sequences with at least one eos token\n",
    "any_eos = eos_mask.any(dim=1)                               # Shape: (batch_size,)\n",
    "eos_positions[any_eos] = eos_mask[any_eos].float().argmax(dim=1)  # First eos position for each sequence\n",
    "\n",
    "# Create position indices: [0, 1, ..., gen_len-1] for each sequence\n",
    "position_ids = torch.arange(gen_len, device=gen_only_ids.device).unsqueeze(0).expand(batch_size, -1)\n",
    "\n",
    "# Final attention mask: True for all positions <= first eos (inclusive)\n",
    "generation_attention_mask = position_ids <= eos_positions.unsqueeze(1)  # Shape: (batch_size, gen_len)\n",
    "\n",
    "# ===============================\n",
    "# NOTES FOR DOWNSTREAM PROCESSING\n",
    "# ===============================\n",
    "\n",
    "# Your downstream functions (e.g., extract_average_token_activations) \n",
    "# should now consume `aligned_hidden_states` and `generation_attention_mask`.\n",
    "# \n",
    "# Note: these masks assume right-padding with eos_token_id.\n",
    "# If needed, you can reverse-pad the activations,\n",
    "# but it's often cleaner to adapt reduction logic accordingly.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ood_env",
   "language": "python",
   "name": "ood_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
