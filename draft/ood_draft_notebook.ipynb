{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OOD detection applied to Hallucination Detection\n",
    "\n",
    " The goal is to predict if an INPUT prompt  is going to produce an hallucination or not (using OOD detection methods). For now, we don’t look at the output generated by the model, we may consider this in a second time. Retrieve ID samples:  To do this, take a general (easy) QA dataset containing questions along with their true hallucination-free answers. Feed the questions to the model. Let the model generate responses and check if the a given generated response is the same as the real hallucination-free answer. All the correct generated responses will be considered ID. More precisely, the ID dataset will consist of the embeddings of the last token of the last layer of the input (or maybe middle layer) of the correct generated responses.  Test a new sample to see if this is going to be OOD=hallucination: Take another dataset containing questions susceptible to trigger hallucinations along with the true hallucination-free answers (or no answer if the model cannot know the answer by any way and all response that the model might produce will necessarily be hallucinated). Feed a question to the model and let it generate a response. Check by comparing to the hallucination-free answer is that generated response is hallucinated or not. At the same time, apply an OOD detection method on the input question (at the last token last layer) and see if there is a correspondence between a high OOD score and a generated hallucination. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "# -----------------------------------\n",
    "import torch\n",
    "import sys\n",
    "import time \n",
    "import os \n",
    "# Add the path to the src directory\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 44\n",
    "BATCH_SIZE = 16\n",
    "MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "OUTPUT_DIR = \"../results/raw/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.13 (main, Jun  4 2025, 08:57:30) [GCC 13.3.0]\n",
      "Cuda version: 12.6\n",
      "Number of available de GPU : 2\n",
      "GPU 1 : NVIDIA L40S\n",
      "GPU 2 : NVIDIA L40S\n"
     ]
    }
   ],
   "source": [
    "# Visualize setup \n",
    "# -----------------------------------\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Cuda version: {torch.version.cuda}\")\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Number of available de GPU : {num_gpus}\")\n",
    "for i in range(num_gpus):\n",
    "    print(f\"GPU {i + 1} : {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed everything\n",
    "# -----------------------------------\n",
    "from src.utils.general import seed_all\n",
    "seed_all(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f057ed1b274d49a19c5a28f758887f96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model\n",
    "# -----------------------------------\n",
    "from src.model_loader.llama_loader import load_llama\n",
    "\n",
    "model, tokenizer = load_llama(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ID dataset\n",
    "\n",
    "For the ID general dataset, we are going to use the SQUAD 1.1 dataset: \n",
    "\n",
    "***SQuAD 1.1:** Comprises over 100,000 question-answer pairs derived from more than 500 Wikipedia articles. Each question is paired with a specific segment of text (a span) from the corresponding article that serves as the answer.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Dataset Information =====\n",
      "Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers', 'original_index', 'is_impossible'],\n",
      "    num_rows: 10000\n",
      "})\n",
      "Mean ground-truth answer length: 3.19, Max length: 30\n",
      "Mean context + question length: 129.68, Max length: 497\n"
     ]
    }
   ],
   "source": [
    "# Load ID dataset\n",
    "# -----------------------------------\n",
    "from src.data_reader.squad_loader import load_id_fit_dataset\n",
    "\n",
    "id_fit_dataset = load_id_fit_dataset()\n",
    "id_fit_dataset = id_fit_dataset.shuffle(SEED) \n",
    "id_fit_dataset = id_fit_dataset.slice(idx_start=0, idx_end=10000)\n",
    "id_fit_dataset.print_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= Analyze one generation  =========\n",
      "----- Prompt construction: 0.000 sec\n",
      "----- Tokenization: 0.003 sec\n",
      "----- Token extraction: 0.390 sec\n",
      "----- Generation: 0.606 sec\n",
      "----- Decoding: 0.000 sec\n",
      "----- Similarity scoring: 0.043 sec\n",
      "\n",
      "=== Prompt ===\n",
      "<s>[INST]\n",
      "\n",
      "Just give the answer, without a complete sentence.           \n",
      "\n",
      "Context:\n",
      "Under a front page headline \"The Truth\", the paper printed allegations provided to them that some fans picked the pockets of crushed victims, that others urinated on members of the emergency services as they tried to help and that some even assaulted a police constable \"whilst he was administering the kiss of life to a patient.\" Despite the headline, written by Kelvin MacKenzie, the story was based on allegations either by unnamed and unattributable sources, or hearsay accounts of what named individuals had said – a fact made clear to MacKenzie by Harry Arnold, the reporter who wrote the story.\n",
      "\n",
      "Question:\n",
      "What was the story based on?\n",
      "\n",
      "Answer:\n",
      "[/INST]\n",
      "\n",
      "=== Shapes ===\n",
      "Shape - number of tokens: torch.Size([1, 184])\n",
      "Shape - selected_layer: torch.Size([1, 184, 4096])\n",
      "\n",
      "=== Generated Answer ===\n",
      "Allegations from unnamed and unattributable sources or hearsay accounts.\n",
      "\n",
      "=== Ground-truth Answer ===\n",
      "allegations either by unnamed and unattributable sources, or hearsay accounts of what named individuals had said\n",
      "\n",
      "=== Similarity Scores ===\n",
      "ROUGE-L F1: 0.6400\n",
      "Sentence-BERT Cosine Similarity: 0.8867\n",
      "Is generated answer correct: True\n"
     ]
    }
   ],
   "source": [
    "# Visualize one generation with the ID dataset\n",
    "# -----------------------------------\n",
    "from src.inference.inference_utils import analyze_single_generation, build_prompt, get_layer_output, extract_last_token_activations\n",
    "\n",
    "_ = analyze_single_generation(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer, \n",
    "    dataset=id_fit_dataset,\n",
    "    sample_idx=0,\n",
    "    build_prompt_fn=build_prompt,\n",
    "    get_layer_output_fn=get_layer_output,\n",
    "    layer_idx=-1,\n",
    "    extract_token_activations_fn=extract_last_token_activations,\n",
    "    offset=-5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve ID embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Retrieve ID embeddings and save results \n",
    "# -----------------------------------\n",
    "from src.inference.inference_utils import batch_extract_token_activations_with_generation, build_prompt, get_layer_output, extract_last_token_activations\n",
    "from src.utils.general import print_time_elapsed\n",
    "\n",
    "# Runs batched inference on a dataset using a decoder-only language model.\n",
    "# For each batch, generates answers, computes semantic similarity scores, extracts token-level activations,\n",
    "# and appends the results to a pickle file.\n",
    "print(\"\\nStart retrieving ID embeddings...\")\n",
    "t0 = time.time()\n",
    "\n",
    "batch_extract_token_activations_with_generation(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=id_fit_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    idx_start_sample=0,\n",
    "    max_samples=len(id_fit_dataset),\n",
    "    output_path = OUTPUT_DIR + \"id_fit_results.pkl\",\n",
    "    build_prompt_fn=build_prompt,\n",
    "    get_layer_output_fn=get_layer_output,\n",
    "    layer_idx=-1,  \n",
    "    extract_token_activations_fn=extract_last_token_activations,\n",
    "    offset=-5\n",
    ")\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"...end!\")\n",
    "print_time_elapsed(t0, t1, label=\"ID embeddings: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free memory \n",
    "del id_fit_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load OOD/Hallucinations test datasets \n",
    "\n",
    "To evaluate the Hallucination detection in question answering using OOD detection methods, we will use datasets in the SQuAD style:\n",
    "\n",
    "***SQuAD 2.0:** SQuAD 2.0 extends the original **SQuAD 1.1** dataset by adding around 50,000 unanswerable questions. These questions are carefully designed to look similar to answerable ones, making it more challenging for models to determine when there isn’t enough information in the context to provide an answer.*\n",
    "\n",
    "**Test Dataset Composition** \\\n",
    "Our test set will include two types of samples:\n",
    "- ***Impossible samples***: Questions that cannot be answered based on the provided context (i.e., the answer is not present in the text). These are taken from the training split of SQuAD 2.0, selecting only the unanswerable questions.\n",
    "- ***Possible samples***: Questions where the answer is explicitly present in the context. These are drawn from the validation split of SQuAD 1.1. This ensures there is no overlap with the in-distribution (ID) data from the SQuAD 1.1 training split.\n",
    "\n",
    "**Note on Evaluation Scope**\\\n",
    "Currently, our evaluation focuses on whether the model can answer questions using only the information provided in the input context. We do not test the model’s internal knowledge or ability to answer questions without supporting context. However, this setup closely matches the OOD scenario: if the information is not in the text, the model should recognize and indicate this.\n",
    "\n",
    "**Additional Dataset: TriviaQA**\\\n",
    "To further test generalization, we will also use the TriviaQA dataset. Like SQuAD, TriviaQA provides question-answering prompts with supporting context. The model must either extract the correct answer from the context or correctly identify when the answer is not present.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test datasets\n",
    "# -----------------------------------\n",
    "from src.data_reader.squad_loader import load_id_test_dataset, load_od_test_dataset\n",
    "\n",
    "# Load possible test dataset \n",
    "id_test_dataset = load_id_test_dataset()\n",
    "id_test_dataset = id_test_dataset.shuffle(SEED) \n",
    "id_test_dataset = id_test_dataset.slice(idx_start=0, idx_end=1000)\n",
    "id_test_dataset.print_info()\n",
    "\n",
    "# Load impossible test dataset \n",
    "od_test_dataset = load_od_test_dataset()\n",
    "od_test_dataset = od_test_dataset.shuffle(SEED) \n",
    "od_test_dataset = od_test_dataset.slice(idx_start=0, idx_end=1000)\n",
    "od_test_dataset.print_info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= Analyze one generation  =========\n",
      "----- Prompt construction: 0.000 sec\n",
      "----- Tokenization: 0.021 sec\n",
      "----- Token extraction: 0.044 sec\n",
      "----- Generation: 0.076 sec\n",
      "----- Decoding: 0.000 sec\n",
      "----- Similarity scoring: 0.004 sec\n",
      "\n",
      "=== Prompt ===\n",
      "<s>[INST]\n",
      "\n",
      "Just give the answer, without a complete sentence. Reply with 'Impossible to answer' if answer not in context.            \n",
      "\n",
      "Context:\n",
      "In the Catholic Church, canon law is the system of laws and legal principles made and enforced by the Church's hierarchical authorities to regulate its external organization and government and to order and direct the activities of Catholics toward the mission of the Church.\n",
      "\n",
      "Question:\n",
      "What mission this Canon law directly activities of all Christians towards?\n",
      "\n",
      "Answer:\n",
      "[/INST]\n",
      "\n",
      "=== Shapes ===\n",
      "Shape - number of tokens: torch.Size([1, 119])\n",
      "Shape - selected_layer: torch.Size([1, 119, 4096])\n",
      "\n",
      "=== Generated Answer ===\n",
      "Mission\n",
      "\n",
      "=== Ground-truth Answer ===\n",
      "\n",
      "\n",
      "=== Similarity Scores ===\n",
      "ROUGE-L F1: 0.0000\n",
      "Sentence-BERT Cosine Similarity: 0.3657\n",
      "Is generated answer correct: False\n"
     ]
    }
   ],
   "source": [
    "# Visualize one generation with the test impossible dataset\n",
    "# -----------------------------------\n",
    "from src.inference.inference_utils import analyze_single_generation, build_impossible_prompt, get_layer_output, extract_last_token_activations\n",
    "\n",
    "_ = analyze_single_generation(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer, \n",
    "    dataset=od_test_dataset,\n",
    "    sample_idx=3,\n",
    "    build_prompt_fn=build_impossible_prompt,\n",
    "    get_layer_output_fn=get_layer_output,\n",
    "    layer_idx=-1,\n",
    "    extract_token_activations_fn=extract_last_token_activations,\n",
    "    offset=-5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve test embeddings \n",
    "\n",
    "Test embeddings which may be OOD/Hallucinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start retrieving test impossible embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:50<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...end!\n",
      "Test impossible embeddings: Time elapsed: 00 min 50 sec\n",
      "\n",
      "\n",
      "Start retrieving test possible embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:51<00:00,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...end!\n",
      "Test possible embeddings: Time elapsed: 00 min 51 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Retrieve test embeddings and save results \n",
    "# -----------------------------------\n",
    "from src.inference.inference_utils import batch_extract_token_activations, build_prompt, get_layer_output, extract_last_token_activations\n",
    "from src.utils.general import print_time_elapsed\n",
    "\n",
    "# Runs batched inference on a dataset using a decoder-only language model.\n",
    "# For each batch, gextracts token-level activations, and appends the results to a pickle file.\n",
    "print(\"\\nStart retrieving test impossible embeddings...\")\n",
    "t2 = time.time()\n",
    "batch_extract_token_activations(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=od_test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    idx_start_sample=0,\n",
    "    max_samples=len(od_test_dataset),\n",
    "    save_to_pkl = True,\n",
    "    output_path = OUTPUT_DIR + \"od_test_results.pkl\",\n",
    "    build_prompt_fn=build_prompt,\n",
    "    get_layer_output_fn=get_layer_output,\n",
    "    layer_idx=-1,  \n",
    "    extract_token_activations_fn=extract_last_token_activations,\n",
    "    offset=-5\n",
    ")\n",
    "t3 = time.time()\n",
    "print(\"...end!\")\n",
    "print_time_elapsed(t2, t3, label=\"Test impossible embeddings: \")\n",
    "\n",
    "\n",
    "print(\"\\nStart retrieving test possible embeddings...\")\n",
    "t4 = time.time()\n",
    "batch_extract_token_activations(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=id_test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    idx_start_sample=0,\n",
    "    max_samples=len(id_test_dataset),\n",
    "    save_to_pkl = True,\n",
    "    output_path = OUTPUT_DIR + \"id_test_results.pkl\",\n",
    "    build_prompt_fn=build_prompt,\n",
    "    get_layer_output_fn=get_layer_output,\n",
    "    layer_idx=-1,  \n",
    "    extract_token_activations_fn=extract_last_token_activations,\n",
    "    offset=-5\n",
    ")\n",
    "t5 = time.time()\n",
    "print(\"...end!\")\n",
    "print_time_elapsed(t4, t5, label=\"Test possible embeddings: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free memory \n",
    "del od_test_dataset \n",
    "del id_test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load extracted embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10000 samples from: ../results/raw/id_fit_results.pkl\n",
      "Size before filtering incorrect samples: 10000.\n",
      "Size after filtering: 8865. Filtered 1135 samples.\n"
     ]
    }
   ],
   "source": [
    "# Load ID embeddings \n",
    "# -----------------------------------\n",
    "from src.data_reader.pickle_io import load_pickle_batches\n",
    "from src.utils.general import filter_correct_entries\n",
    "\n",
    "# Load extracted embeddings \n",
    "id_fit_embeddings = load_pickle_batches(OUTPUT_DIR + \"id_fit_results.pkl\")\n",
    "# Only keep rows with simiar generated and ground-truth answers\n",
    "id_fit_embeddings = filter_correct_entries(id_fit_embeddings) \n",
    "# Concatenate all embeddings \n",
    "id_fit_embeddings = torch.cat(id_fit_embeddings['activations'], dim=0) # shape: [N, D]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1000 samples from: ../results/raw/id_test_results.pkl\n",
      "Loaded 1000 samples from: ../results/raw/od_test_results.pkl\n"
     ]
    }
   ],
   "source": [
    "# Load test embeddings \n",
    "# -----------------------------------\n",
    "from src.data_reader.pickle_io import load_pickle_batches\n",
    "\n",
    "# Load extracted possible and impossible embeddings \n",
    "od_test_embeddings = load_pickle_batches(OUTPUT_DIR + \"id_test_results.pkl\")\n",
    "id_test_embeddings = load_pickle_batches(OUTPUT_DIR + \"od_test_results.pkl\")\n",
    "# Concatenate possible and impossible all embeddings \n",
    "od_test_embeddings = torch.cat(od_test_embeddings['activations'], dim=0) # shape: [N, D]\n",
    "id_test_embeddings = torch.cat(id_test_embeddings['activations'], dim=0) # shape: [N, D]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform DKNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructs the FAISS index from ID data.\n",
    "# -----------------------------------\n",
    "from src.analysis.dknn import fit_to_dataset\n",
    "index = fit_to_dataset(id_fit_embeddings, use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute DKNN scores on test data\n",
    "# -----------------------------------\n",
    "from src.analysis.dknn import score_tensor\n",
    "k=50\n",
    "dknn_scores_id  = score_tensor(index, id_test_embeddings, nearest=k, batch_size=BATCH_SIZE)\n",
    "dknn_scores_ood = score_tensor(index, od_test_embeddings, nearest=k, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auROC: 0.5229\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC: 0.5229\n"
     ]
    }
   ],
   "source": [
    "# Compute and plot auroc \n",
    "# -----------------------------------\n",
    "from src.analysis.dknn import compute_auroc\n",
    "auroc, fpr, tpr, thresholds = compute_auroc(dknn_scores_id, dknn_scores_ood, plot=True)\n",
    "print(f\"AUROC: {auroc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":'("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brouillon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.inference.inference_utils import build_prompt, get_layer_output, compute_token_offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_average_token_activations(\n",
    "    selected_layer: torch.Tensor,\n",
    "    attention_mask: torch.Tensor,\n",
    "    device: torch.device,\n",
    "    start_offset : int = 0,\n",
    "    end_offset : int = 0\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Extract the mean activation vector over a token span for each sequence in a batch.\n",
    "    The span is defined by applying start_offset (from the first non-padding token)\n",
    "    and end_offset (from the last non-padding token).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    selected_layer : torch.Tensor\n",
    "        Output tensor from the selected model layer (batch_size x seq_len x hidden_size).\n",
    "    attention_mask : torch.Tensor\n",
    "        Attention mask (batch_size x seq_len).\n",
    "    device : torch.device\n",
    "        Device for computation.\n",
    "    start_offset : int\n",
    "        Offset from first non-padding token (to skip e.g. [INST]).\n",
    "    end_offset : int\n",
    "        Offset from last non-padding token (to skip e.g. [/INST]).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        Averaged embeddings (batch_size x hidden_size)\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, _ = selected_layer.shape\n",
    "\n",
    "    # Detect left padding if any sequence starts with padding\n",
    "    is_left_padding = (attention_mask[:, 0] == 0).any()\n",
    "\n",
    "    # Find the index of the first and the  last non-padding token for each sequence\n",
    "    if is_left_padding:\n",
    "        #--- For left padding, first non-padding token is at index: number of padding tokens\n",
    "        first_indices = attention_mask.argmax(dim=1)\n",
    "        #--- For left padding, last non-padding token is at the end: compute its index by flipping and offsetting from the end\n",
    "        last_indices = (attention_mask.size(1) - 1) - attention_mask.flip(dims=[1]).argmax(dim=1)\n",
    "    else:\n",
    "        #--- For right padding, first non-padding token is always at index 0\n",
    "        first_indices = torch.zeros(batch_size, dtype=torch.long, device=device)\n",
    "        #--- For right padding, last non-padding token is at: (number of non-padding tokens) - 1\n",
    "        last_indices = (attention_mask.sum(dim=1) - 1)\n",
    "\n",
    "    first_indices = first_indices.to(device)\n",
    "    last_indices = last_indices.to(device)\n",
    "\n",
    "    # Apply offsets (e.g., skip <s> [INST] or [\\INST])\n",
    "    target_first_indices = first_indices + start_offset #-1\n",
    "    target_last_indices = last_indices + end_offset #+1\n",
    "\n",
    "    # Clamp indices to valid range\n",
    "    target_first_indices = torch.clamp(target_first_indices, min=0, max=seq_len - 1)\n",
    "    target_last_indices = torch.clamp(target_last_indices, min=0, max=seq_len - 1)\n",
    "\n",
    "    # Compute mask for averaging\n",
    "    #--- Create a tensor of positions: shape (1, seq_len), then expand to (batch_size, seq_len)\n",
    "    positions = torch.arange(seq_len, device=device).unsqueeze(0).expand(batch_size, seq_len)\n",
    "    #--- Build a boolean mask: True where the position is within [target_first_indices, target_last_indices] for each sequence\n",
    "    mask = (positions >= target_first_indices.unsqueeze(1)) & (positions <= target_last_indices.unsqueeze(1))\n",
    "    #--- Convert the boolean mask to float and add a singleton dimension for broadcasting with selected_layer\n",
    "    mask = mask.float().unsqueeze(-1)  # (batch_size, seq_len, 1)\n",
    "\n",
    "    # Apply mask and compute mean\n",
    "    #--- Apply the mask to the activations: zero out tokens outside the target interval\n",
    "    masked = selected_layer * mask\n",
    "    #--- Count the number of selected tokens for each sequence (avoid division by zero with clamp)\n",
    "    counts = mask.sum(dim=1).clamp(min=1e-6)\n",
    "    #--- Compute the mean activation vector for each sequence over the selected interval\n",
    "    avg = masked.sum(dim=1) / counts # (batch_size, hidden_size)\n",
    "\n",
    "    # Optionally, return also the indices used\n",
    "    indices = torch.stack([target_first_indices, target_last_indices], dim=1)\n",
    "\n",
    "    return avg, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizer, PreTrainedModel, BatchEncoding\n",
    "import torch\n",
    "from datasets import  Dataset\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Any, Callable\n",
    "import time\n",
    "\n",
    "from src.evaluation.similarity_metrics import rouge_l_simScore, sentence_bert_simScore\n",
    "from src.inference.inference_utils import extract_batch, generate_answers\n",
    "\n",
    "def batch_extract_token_activations_with_generation(\n",
    "    model: PreTrainedModel,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    dataset: Dataset,\n",
    "    batch_size: int = 2,\n",
    "    idx_start_sample: int = 0,\n",
    "    max_samples: int = 2,\n",
    "    output_path: str = \"outputs/all_batch_results.pkl\",\n",
    "    build_prompt_fn: Callable[[str, str], str] = None,\n",
    "    get_layer_output_fn: Callable = None,\n",
    "    layer_idx: int = -1,  \n",
    "    extract_token_activations_fn: Callable = None,\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs batched inference on a dataset using a decoder-only language model.\n",
    "    For each batch, generates answers, computes semantic similarity scores, extracts token-level activations,\n",
    "    and appends the results to a pickle file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : PreTrainedModel\n",
    "        The causal language model to evaluate (e.g., LLaMA).\n",
    "    tokenizer : PreTrainedTokenizer\n",
    "        The corresponding tokenizer.\n",
    "    dataset : Dataset\n",
    "        The input dataset.\n",
    "    batch_size : int\n",
    "        Number of samples per batch.\n",
    "    idx_start_sample : int\n",
    "        Index of the first sample to process from the dataset.\n",
    "    max_samples : int\n",
    "        Total number of examples to process from the dataset, starting from idx_start_sample. \n",
    "    output_path : str\n",
    "        Path to the pickle file for saving intermediate results.\n",
    "    build_prompt_fn : Callable\n",
    "        Function to build a prompt from context and question.\n",
    "    get_layer_output_fn : Callable\n",
    "        Function to extract the output of a specific model layer.\n",
    "    layer_idx : int\n",
    "        Index of the transformer layer to extract activations from (default: -1 for last layer).\n",
    "    extract_token_activations_fn : Callable\n",
    "        Function to extract token activations from a model layer.\n",
    "    **kwargs :\n",
    "        Extra keyword arguments passed to extract_token_activations_fn.\n",
    "    \"\"\"\n",
    "    for i in tqdm(range(idx_start_sample, idx_start_sample + max_samples, batch_size)):\n",
    "        batch_answers = []               # Generated answers\n",
    "        batch_gt_answers = []            # Ground-truth answers\n",
    "        batch_is_correct = []            # 0/1 labels indicating correctness\n",
    "        batch_dataset_ids = []           # 'id' field from dataset\n",
    "        batch_dataset_original_idx = []  # Original indices from dataset\n",
    "        batch_rouge_scores = []          # Rouge-L scores\n",
    "        batch_sbert_scores = []          # Sentence-Bert scores\n",
    "\n",
    "        batch = extract_batch(dataset, i, batch_size)\n",
    "        prompts = [build_prompt_fn(s[\"context\"], s[\"question\"]) for s in batch]\n",
    "        print(f\"prompts: {prompts[0]},\\n{prompts[1]}\")\n",
    "        answers = [s[\"answers\"][\"text\"] for s in batch]\n",
    "        inputs = tokenizer(prompts, padding=True, truncation=True, return_tensors=\"pt\").to(model.device)\n",
    "        print(f\"inputs:\", inputs)\n",
    "        selected_layer = get_layer_output_fn(model, inputs, layer_idx)\n",
    "        selected_token_vecs, target_indices = extract_token_activations_fn(\n",
    "                selected_layer, \n",
    "                inputs[\"attention_mask\"], \n",
    "                device=selected_layer.device,\n",
    "                **kwargs) \n",
    "        print(\"selected_token_vecs\", selected_token_vecs)\n",
    "        \n",
    "        for k in range(len(prompts)):\n",
    "            print(\"=========DECODING==========\")\n",
    "            print(\"target_indices[k]\", target_indices[k])\n",
    "            print(f\"\\nDecoding START target_indices[k]:-----{tokenizer.decode(inputs['input_ids'][k][target_indices[k][0]:target_indices[k][1]+1])}-----Decoding end\\n\")\n",
    "            print(f\"SELECTED TOKENS ---{tokenizer.decode(inputs['input_ids'][k][target_indices[k][0]])} and {tokenizer.decode(inputs['input_ids'][k][target_indices[k][1]])}---\")\n",
    "        \n",
    "        output_ids = generate_answers(model, inputs, tokenizer)\n",
    "        \n",
    "        for j in range(len(prompts)):\n",
    "            # --- Decode token IDs into text ---\n",
    "            prompt_len = len(inputs[\"input_ids\"][j]) # Length of prompt j\n",
    "            generated_answer_ids = output_ids[j][prompt_len:] # Remove prompt prefix to isolate the generated answer\n",
    "            generated_answer = tokenizer.decode(generated_answer_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "            # --- Compute semantic similarity between model's answer and ground-truth ---    \n",
    "            rouge_l_score = rouge_l_simScore(generated_answer, answers[j])\n",
    "            if rouge_l_score >= 0.5:\n",
    "                is_correct = True\n",
    "                sbert_score = None\n",
    "            else:\n",
    "                sbert_score = sentence_bert_simScore(generated_answer, answers[j])\n",
    "                is_correct = (sbert_score >= 0.4)\n",
    "\n",
    "            # --- Store everything ---\n",
    "            batch_dataset_ids.append(batch[j]['id'])\n",
    "            batch_dataset_original_idx.append(batch[j]['original_index'])\n",
    "            batch_answers.append(generated_answer)\n",
    "            batch_gt_answers.append(answers[j])\n",
    "            batch_is_correct.append(int(is_correct))\n",
    "            batch_rouge_scores.append(rouge_l_score)\n",
    "            batch_sbert_scores.append(sbert_score)\n",
    "\n",
    "        # --- Save progress to pickle after each batch ---\n",
    "        batch_results = {\n",
    "            \"id\": batch_dataset_ids,\n",
    "            \"original_indices\": batch_dataset_original_idx,\n",
    "            \"gen_answers\": batch_answers,\n",
    "            \"ground_truths\": batch_gt_answers,\n",
    "            \"activations\": [selected_token_vecs[i].unsqueeze(0).cpu() for i in range(selected_token_vecs.size(0))],\n",
    "            \"is_correct\": batch_is_correct,\n",
    "            \"sbert_scores\": batch_sbert_scores,\n",
    "            \"rouge_scores\": batch_rouge_scores\n",
    "        }\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Input text =====\n",
      "[INST] <<SYS>>\n",
      "Just give the answer, without a complete sentence. Reply with 'Impossible to answer' if answer not in context.\n",
      "<<SYS>>\n",
      "\n",
      "Context:\n",
      "Under a front page headline \"The Truth\", the paper printed allegations provided to them that some fans picked the pockets of crushed victims, that others urinated on members of the emergency services as they tried to help and that some even assaulted a police constable \"whilst he was administering the kiss of life to a patient.\" Despite the headline, written by Kelvin MacKenzie, the story was based on allegations either by unnamed and unattributable sources, or hearsay accounts of what named individuals had said – a fact made clear to MacKenzie by Harry Arnold, the reporter who wrote the story.\n",
      "\n",
      "Question:\n",
      "What was the story based on?\n",
      "\n",
      "Answer:\n",
      "[/INST]\n",
      "\n",
      "===== Decoded text between `start_offset` and `end_offset` =====\n",
      "----START TEXT---<<SYS>>\n",
      "Just give the answer, without a complete sentence. Reply with 'Impossible to answer' if answer not in context.\n",
      "<<SYS>>\n",
      "\n",
      "Context:\n",
      "Under a front page headline \"The Truth\", the paper printed allegations provided to them that some fans picked the pockets of crushed victims, that others urinated on members of the emergency services as they tried to help and that some even assaulted a police constable \"whilst he was administering the kiss of life to a patient.\" Despite the headline, written by Kelvin MacKenzie, the story was based on allegations either by unnamed and unattributable sources, or hearsay accounts of what named individuals had said – a fact made clear to MacKenzie by Harry Arnold, the reporter who wrote the story.\n",
      "\n",
      "Question:\n",
      "What was the story based on?\n",
      "\n",
      "Answer:\n",
      "----END TEXT---\n",
      "\n",
      "start_offset: 4, end_offset:-4\n"
     ]
    }
   ],
   "source": [
    "text = build_prompt(id_fit_dataset[0][\"context\"], id_fit_dataset[0][\"question\"])\n",
    "start_offset, end_offset = compute_token_offsets(\n",
    "    text=text,\n",
    "    tokenizer=tokenizer,\n",
    "    start_phrase=\"<<SYS>>\",\n",
    "    end_phrase=\"Answer:\\n\",\n",
    "    debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompts: [INST] <<SYS>>\n",
      "Just give the answer, without a complete sentence. Reply with 'Impossible to answer' if answer not in context.\n",
      "<<SYS>>\n",
      "\n",
      "Context:\n",
      "Under a front page headline \"The Truth\", the paper printed allegations provided to them that some fans picked the pockets of crushed victims, that others urinated on members of the emergency services as they tried to help and that some even assaulted a police constable \"whilst he was administering the kiss of life to a patient.\" Despite the headline, written by Kelvin MacKenzie, the story was based on allegations either by unnamed and unattributable sources, or hearsay accounts of what named individuals had said – a fact made clear to MacKenzie by Harry Arnold, the reporter who wrote the story.\n",
      "\n",
      "Question:\n",
      "What was the story based on?\n",
      "\n",
      "Answer:\n",
      "[/INST],\n",
      "[INST] <<SYS>>\n",
      "Just give the answer, without a complete sentence. Reply with 'Impossible to answer' if answer not in context.\n",
      "<<SYS>>\n",
      "\n",
      "Context:\n",
      "The term Carnival is traditionally used in areas with a large Catholic presence. However, the Philippines, a predominantly Roman Catholic country, does not celebrate Carnival anymore since the dissolution of the Manila Carnival after 1939, the last carnival in the country. In historically Lutheran countries, the celebration is known as Fastelavn, and in areas with a high concentration of Anglicans and Methodists, pre-Lenten celebrations, along with penitential observances, occur on Shrove Tuesday. In Eastern Orthodox nations, Maslenitsa is celebrated during the last week before Great Lent. In German-speaking Europe and the Netherlands, the Carnival season traditionally opens on 11/11 (often at 11:11 a.m.). This dates back to celebrations before the Advent season or with harvest celebrations of St. Martin's Day.\n",
      "\n",
      "Question:\n",
      "When does Fastelavn occur?\n",
      "\n",
      "Answer:\n",
      "[/INST]\n",
      "inputs: {'input_ids': tensor([[    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     1,   518, 25580,\n",
      "         29962,  3532, 14816, 29903,  6778,    13, 14084,  2367,   278,  1234,\n",
      "         29892,  1728,   263,  4866, 10541, 29889, 10088,   368,   411,   525,\n",
      "          1888, 27338,   304,  1234, 29915,   565,  1234,   451,   297,  3030,\n",
      "         29889,    13,  9314, 14816, 29903,  6778,    13,    13,  2677, 29901,\n",
      "            13, 29177,   263,  4565,  1813,  2343,  1220,   376,  1576,  1605,\n",
      "          2806,   613,   278,  5650, 13350, 16831,   800,  4944,   304,   963,\n",
      "           393,   777, 24909, 18691,   278,   772,  9737,   310,  2181, 15392,\n",
      "          6879,  9893, 29892,   393,  4045,  5065,   262,   630,   373,  5144,\n",
      "           310,   278, 11176, 14703,  5786,   408,   896,  1898,   304,  1371,\n",
      "           322,   393,   777,  1584, 29159,   287,   263, 10974,  1040,   519,\n",
      "           376,  1332, 16613,   540,   471,  4113,  1531,   292,   278, 20057,\n",
      "           310,  2834,   304,   263, 16500,  1213, 19454,   278,  2343,  1220,\n",
      "         29892,  3971,   491, 27326,  3845,  4326, 29968,   264,  3914, 29892,\n",
      "           278,  5828,   471,  2729,   373, 16831,   800,  2845,   491,   443,\n",
      "         17514,   322,   443,  1131,  1091,  9246,  8974, 29892,   470,   540,\n",
      "          1503,   388, 15303,   310,   825,  4257, 15724,   750,  1497,   785,\n",
      "           263,  2114,  1754,  2821,   304,  4326, 29968,   264,  3914,   491,\n",
      "         10686, 20720, 29892,   278,  1634,  9555,  1058,  5456,   278,  5828,\n",
      "         29889,    13,    13, 16492, 29901,    13,  5618,   471,   278,  5828,\n",
      "          2729,   373, 29973,    13,    13, 22550, 29901,    13, 29961, 29914,\n",
      "         25580, 29962],\n",
      "        [    1,   518, 25580, 29962,  3532, 14816, 29903,  6778,    13, 14084,\n",
      "          2367,   278,  1234, 29892,  1728,   263,  4866, 10541, 29889, 10088,\n",
      "           368,   411,   525,  1888, 27338,   304,  1234, 29915,   565,  1234,\n",
      "           451,   297,  3030, 29889,    13,  9314, 14816, 29903,  6778,    13,\n",
      "            13,  2677, 29901,    13,  1576,  1840,  1704, 29876,  2561,   338,\n",
      "         11399,   635,  1304,   297, 10161,   411,   263,  2919, 11865, 10122,\n",
      "         29889,  2398, 29892,   278, 26260, 29892,   263,   758, 24130, 10835,\n",
      "          5917, 11865,  4234, 29892,   947,   451, 10894,   403,  1704, 29876,\n",
      "          2561, 15128,  1951,   278, 23556,   918,   310,   278,  2315,  4233,\n",
      "          1704, 29876,  2561,  1156, 29871, 29896, 29929, 29941, 29929, 29892,\n",
      "           278,  1833,  1559, 29876,  2561,   297,   278,  4234, 29889,   512,\n",
      "          3603,  1711, 24760,   273, 10916, 29892,   278, 10894,   362,   338,\n",
      "          2998,   408, 23786,   295,   485, 29876, 29892,   322,   297, 10161,\n",
      "           411,   263,  1880, 26702,   310,  3218,   506,   550,   322,  8108,\n",
      "          2879, 29892,   758, 29899, 29931, 19889, 10894,   800, 29892,  3412,\n",
      "           411,  6584,   277,  2556,  5820,  2925, 29892,  6403,   373,  1383,\n",
      "           307,   345,   323,  1041,  3250, 29889,   512, 16162, 23757, 17324,\n",
      "         19079, 29892,  8622,  2435,  1169, 29874,   338, 26301,  2645,   278,\n",
      "          1833,  4723,  1434,  7027,   365,   296, 29889,   512,  5332, 29899,\n",
      "          5965,  5086,  4092,   322,   278, 24553, 29892,   278,  1704, 29876,\n",
      "          2561,  4259, 11399,   635, 13246,   373, 29871, 29896, 29896, 29914,\n",
      "         29896, 29896,   313, 29877, 15535,   472, 29871, 29896, 29896, 29901,\n",
      "         29896, 29896,   263, 29889, 29885,  6250,   910, 10116,  1250,   304,\n",
      "         10894,   800,  1434,   278, 21255,  4259,   470,   411,  4023, 10147,\n",
      "         10894,   800,   310,   624, 29889,  6502, 29915, 29879,  8373, 29889,\n",
      "            13,    13, 16492, 29901,    13, 10401,   947, 23786,   295,   485,\n",
      "         29876,  6403, 29973,    13,    13, 22550, 29901,    13, 29961, 29914,\n",
      "         25580, 29962]], device='cuda:0'), 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "selected_token_vecs tensor([[ 1.3207, -1.1394,  1.0503,  ..., -0.6873,  0.8425, -0.9228],\n",
      "        [ 0.5228, -1.4229,  0.8777,  ..., -0.6418,  0.1306, -0.6244]],\n",
      "       device='cuda:1')\n",
      "=========DECODING==========\n",
      "target_indices[k] tensor([107, 267], device='cuda:1')\n",
      "\n",
      "Decoding START target_indices[k]:-----\n",
      "Context:\n",
      "Under a front page headline \"The Truth\", the paper printed allegations provided to them that some fans picked the pockets of crushed victims, that others urinated on members of the emergency services as they tried to help and that some even assaulted a police constable \"whilst he was administering the kiss of life to a patient.\" Despite the headline, written by Kelvin MacKenzie, the story was based on allegations either by unnamed and unattributable sources, or hearsay accounts of what named individuals had said – a fact made clear to MacKenzie by Harry Arnold, the reporter who wrote the story.\n",
      "\n",
      "Question:\n",
      "What was the story based on?\n",
      "\n",
      "Answer:\n",
      "-----Decoding end\n",
      "\n",
      "SELECTED TOKENS ---\n",
      " and \n",
      "---\n",
      "=========DECODING==========\n",
      "target_indices[k] tensor([ 40, 267], device='cuda:1')\n",
      "\n",
      "Decoding START target_indices[k]:-----\n",
      "Context:\n",
      "The term Carnival is traditionally used in areas with a large Catholic presence. However, the Philippines, a predominantly Roman Catholic country, does not celebrate Carnival anymore since the dissolution of the Manila Carnival after 1939, the last carnival in the country. In historically Lutheran countries, the celebration is known as Fastelavn, and in areas with a high concentration of Anglicans and Methodists, pre-Lenten celebrations, along with penitential observances, occur on Shrove Tuesday. In Eastern Orthodox nations, Maslenitsa is celebrated during the last week before Great Lent. In German-speaking Europe and the Netherlands, the Carnival season traditionally opens on 11/11 (often at 11:11 a.m.). This dates back to celebrations before the Advent season or with harvest celebrations of St. Martin's Day.\n",
      "\n",
      "Question:\n",
      "When does Fastelavn occur?\n",
      "\n",
      "Answer:\n",
      "-----Decoding end\n",
      "\n",
      "SELECTED TOKENS ---\n",
      " and \n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.51it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_extract_token_activations_with_generation(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=id_fit_dataset,\n",
    "    batch_size=2,\n",
    "    idx_start_sample= 0,\n",
    "    max_samples= 2,\n",
    "    output_path = \"outputs/all_batch_results.pkl\",\n",
    "    build_prompt_fn=build_impossible_prompt,\n",
    "    get_layer_output_fn=get_layer_output,\n",
    "    layer_idx = -1,  \n",
    "    extract_token_activations_fn=extract_average_token_activations,\n",
    "    start_offset = start_offset,\n",
    "    end_offset = end_offset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Input text =====\n",
      "[INST] <<SYS>>\n",
      "Just give the answer, without a complete sentence. Reply with 'Impossible to answer' if answer not in context.\n",
      "<<SYS>>\n",
      "\n",
      "Context:\n",
      "Under a front page headline \"The Truth\", the paper printed allegations provided to them that some fans picked the pockets of crushed victims, that others urinated on members of the emergency services as they tried to help and that some even assaulted a police constable \"whilst he was administering the kiss of life to a patient.\" Despite the headline, written by Kelvin MacKenzie, the story was based on allegations either by unnamed and unattributable sources, or hearsay accounts of what named individuals had said – a fact made clear to MacKenzie by Harry Arnold, the reporter who wrote the story.\n",
      "\n",
      "Question:\n",
      "What was the story based on?\n",
      "\n",
      "Answer:\n",
      "[/INST]\n",
      "\n",
      "===== Decoded text between `start_offset` and `end_offset` =====\n",
      "----START TEXT---\n",
      "Context:\n",
      "Under a front page headline \"The Truth\", the paper printed allegations provided to them that some fans picked the pockets of crushed victims, that others urinated on members of the emergency services as they tried to help and that some even assaulted a police constable \"whilst he was administering the kiss of life to a patient.\" Despite the headline, written by Kelvin MacKenzie, the story was based on allegations either by unnamed and unattributable sources, or hearsay accounts of what named individuals had said – a fact made clear to MacKenzie by Harry Arnold, the reporter who wrote the story.\n",
      "\n",
      "Question:\n",
      "What was the story based on?\n",
      "\n",
      "Answer:\n",
      "----END TEXT---\n",
      "\n",
      "start_offset: 40, end_offset:-4\n"
     ]
    }
   ],
   "source": [
    "from src.inference.inference_utils import (\n",
    "    build_prompt, \n",
    "    compute_token_offsets, \n",
    "    get_layer_output\n",
    ")\n",
    "text = build_prompt(id_fit_dataset[0][\"context\"], id_fit_dataset[0][\"question\"])\n",
    "start_offset, end_offset = compute_token_offsets(\n",
    "    text=text,\n",
    "    tokenizer=tokenizer,\n",
    "    start_phrase=\"\\nContext:\",\n",
    "    end_phrase=\"Answer:\\n\",\n",
    "    debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompts: [INST] <<SYS>>\n",
      "Just give the answer, without a complete sentence. Reply with 'Impossible to answer' if answer not in context.\n",
      "<<SYS>>\n",
      "\n",
      "Context:\n",
      "Under a front page headline \"The Truth\", the paper printed allegations provided to them that some fans picked the pockets of crushed victims, that others urinated on members of the emergency services as they tried to help and that some even assaulted a police constable \"whilst he was administering the kiss of life to a patient.\" Despite the headline, written by Kelvin MacKenzie, the story was based on allegations either by unnamed and unattributable sources, or hearsay accounts of what named individuals had said – a fact made clear to MacKenzie by Harry Arnold, the reporter who wrote the story.\n",
      "\n",
      "Question:\n",
      "What was the story based on?\n",
      "\n",
      "Answer:\n",
      "[/INST],\n",
      "[INST] <<SYS>>\n",
      "Just give the answer, without a complete sentence. Reply with 'Impossible to answer' if answer not in context.\n",
      "<<SYS>>\n",
      "\n",
      "Context:\n",
      "The term Carnival is traditionally used in areas with a large Catholic presence. However, the Philippines, a predominantly Roman Catholic country, does not celebrate Carnival anymore since the dissolution of the Manila Carnival after 1939, the last carnival in the country. In historically Lutheran countries, the celebration is known as Fastelavn, and in areas with a high concentration of Anglicans and Methodists, pre-Lenten celebrations, along with penitential observances, occur on Shrove Tuesday. In Eastern Orthodox nations, Maslenitsa is celebrated during the last week before Great Lent. In German-speaking Europe and the Netherlands, the Carnival season traditionally opens on 11/11 (often at 11:11 a.m.). This dates back to celebrations before the Advent season or with harvest celebrations of St. Martin's Day.\n",
      "\n",
      "Question:\n",
      "When does Fastelavn occur?\n",
      "\n",
      "Answer:\n",
      "[/INST]\n",
      "inputs: {'input_ids': tensor([[    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     1,   518, 25580,\n",
      "         29962,  3532, 14816, 29903,  6778,    13, 14084,  2367,   278,  1234,\n",
      "         29892,  1728,   263,  4866, 10541, 29889, 10088,   368,   411,   525,\n",
      "          1888, 27338,   304,  1234, 29915,   565,  1234,   451,   297,  3030,\n",
      "         29889,    13,  9314, 14816, 29903,  6778,    13,    13,  2677, 29901,\n",
      "            13, 29177,   263,  4565,  1813,  2343,  1220,   376,  1576,  1605,\n",
      "          2806,   613,   278,  5650, 13350, 16831,   800,  4944,   304,   963,\n",
      "           393,   777, 24909, 18691,   278,   772,  9737,   310,  2181, 15392,\n",
      "          6879,  9893, 29892,   393,  4045,  5065,   262,   630,   373,  5144,\n",
      "           310,   278, 11176, 14703,  5786,   408,   896,  1898,   304,  1371,\n",
      "           322,   393,   777,  1584, 29159,   287,   263, 10974,  1040,   519,\n",
      "           376,  1332, 16613,   540,   471,  4113,  1531,   292,   278, 20057,\n",
      "           310,  2834,   304,   263, 16500,  1213, 19454,   278,  2343,  1220,\n",
      "         29892,  3971,   491, 27326,  3845,  4326, 29968,   264,  3914, 29892,\n",
      "           278,  5828,   471,  2729,   373, 16831,   800,  2845,   491,   443,\n",
      "         17514,   322,   443,  1131,  1091,  9246,  8974, 29892,   470,   540,\n",
      "          1503,   388, 15303,   310,   825,  4257, 15724,   750,  1497,   785,\n",
      "           263,  2114,  1754,  2821,   304,  4326, 29968,   264,  3914,   491,\n",
      "         10686, 20720, 29892,   278,  1634,  9555,  1058,  5456,   278,  5828,\n",
      "         29889,    13,    13, 16492, 29901,    13,  5618,   471,   278,  5828,\n",
      "          2729,   373, 29973,    13,    13, 22550, 29901,    13, 29961, 29914,\n",
      "         25580, 29962],\n",
      "        [    1,   518, 25580, 29962,  3532, 14816, 29903,  6778,    13, 14084,\n",
      "          2367,   278,  1234, 29892,  1728,   263,  4866, 10541, 29889, 10088,\n",
      "           368,   411,   525,  1888, 27338,   304,  1234, 29915,   565,  1234,\n",
      "           451,   297,  3030, 29889,    13,  9314, 14816, 29903,  6778,    13,\n",
      "            13,  2677, 29901,    13,  1576,  1840,  1704, 29876,  2561,   338,\n",
      "         11399,   635,  1304,   297, 10161,   411,   263,  2919, 11865, 10122,\n",
      "         29889,  2398, 29892,   278, 26260, 29892,   263,   758, 24130, 10835,\n",
      "          5917, 11865,  4234, 29892,   947,   451, 10894,   403,  1704, 29876,\n",
      "          2561, 15128,  1951,   278, 23556,   918,   310,   278,  2315,  4233,\n",
      "          1704, 29876,  2561,  1156, 29871, 29896, 29929, 29941, 29929, 29892,\n",
      "           278,  1833,  1559, 29876,  2561,   297,   278,  4234, 29889,   512,\n",
      "          3603,  1711, 24760,   273, 10916, 29892,   278, 10894,   362,   338,\n",
      "          2998,   408, 23786,   295,   485, 29876, 29892,   322,   297, 10161,\n",
      "           411,   263,  1880, 26702,   310,  3218,   506,   550,   322,  8108,\n",
      "          2879, 29892,   758, 29899, 29931, 19889, 10894,   800, 29892,  3412,\n",
      "           411,  6584,   277,  2556,  5820,  2925, 29892,  6403,   373,  1383,\n",
      "           307,   345,   323,  1041,  3250, 29889,   512, 16162, 23757, 17324,\n",
      "         19079, 29892,  8622,  2435,  1169, 29874,   338, 26301,  2645,   278,\n",
      "          1833,  4723,  1434,  7027,   365,   296, 29889,   512,  5332, 29899,\n",
      "          5965,  5086,  4092,   322,   278, 24553, 29892,   278,  1704, 29876,\n",
      "          2561,  4259, 11399,   635, 13246,   373, 29871, 29896, 29896, 29914,\n",
      "         29896, 29896,   313, 29877, 15535,   472, 29871, 29896, 29896, 29901,\n",
      "         29896, 29896,   263, 29889, 29885,  6250,   910, 10116,  1250,   304,\n",
      "         10894,   800,  1434,   278, 21255,  4259,   470,   411,  4023, 10147,\n",
      "         10894,   800,   310,   624, 29889,  6502, 29915, 29879,  8373, 29889,\n",
      "            13,    13, 16492, 29901,    13, 10401,   947, 23786,   295,   485,\n",
      "         29876,  6403, 29973,    13,    13, 22550, 29901,    13, 29961, 29914,\n",
      "         25580, 29962]], device='cuda:0'), 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "selected_token_vecs tensor([[ 1.3207, -1.1394,  1.0503,  ..., -0.6873,  0.8425, -0.9228],\n",
      "        [ 0.5228, -1.4229,  0.8777,  ..., -0.6418,  0.1306, -0.6244]],\n",
      "       device='cuda:1')\n",
      "=========DECODING==========\n",
      "target_indices[k] tensor([107, 267], device='cuda:1')\n",
      "\n",
      "Decoding START target_indices[k]:-----\n",
      "Context:\n",
      "Under a front page headline \"The Truth\", the paper printed allegations provided to them that some fans picked the pockets of crushed victims, that others urinated on members of the emergency services as they tried to help and that some even assaulted a police constable \"whilst he was administering the kiss of life to a patient.\" Despite the headline, written by Kelvin MacKenzie, the story was based on allegations either by unnamed and unattributable sources, or hearsay accounts of what named individuals had said – a fact made clear to MacKenzie by Harry Arnold, the reporter who wrote the story.\n",
      "\n",
      "Question:\n",
      "What was the story based on?\n",
      "\n",
      "Answer:\n",
      "-----Decoding end\n",
      "\n",
      "SELECTED TOKENS ---\n",
      " and \n",
      "---\n",
      "=========DECODING==========\n",
      "target_indices[k] tensor([ 40, 267], device='cuda:1')\n",
      "\n",
      "Decoding START target_indices[k]:-----\n",
      "Context:\n",
      "The term Carnival is traditionally used in areas with a large Catholic presence. However, the Philippines, a predominantly Roman Catholic country, does not celebrate Carnival anymore since the dissolution of the Manila Carnival after 1939, the last carnival in the country. In historically Lutheran countries, the celebration is known as Fastelavn, and in areas with a high concentration of Anglicans and Methodists, pre-Lenten celebrations, along with penitential observances, occur on Shrove Tuesday. In Eastern Orthodox nations, Maslenitsa is celebrated during the last week before Great Lent. In German-speaking Europe and the Netherlands, the Carnival season traditionally opens on 11/11 (often at 11:11 a.m.). This dates back to celebrations before the Advent season or with harvest celebrations of St. Martin's Day.\n",
      "\n",
      "Question:\n",
      "When does Fastelavn occur?\n",
      "\n",
      "Answer:\n",
      "-----Decoding end\n",
      "\n",
      "SELECTED TOKENS ---\n",
      " and \n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.27it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_extract_token_activations_with_generation(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=id_fit_dataset,\n",
    "    batch_size=2,\n",
    "    idx_start_sample= 0,\n",
    "    max_samples= 2,\n",
    "    output_path = \"outputs/all_batch_results.pkl\",\n",
    "    build_prompt_fn=build_impossible_prompt,\n",
    "    get_layer_output_fn=get_layer_output,\n",
    "    layer_idx = -1,  \n",
    "    extract_token_activations_fn=extract_average_token_activations,\n",
    "    start_offset = start_offset,\n",
    "    end_offset = end_offset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizer, PreTrainedModel, BatchEncoding\n",
    "import torch\n",
    "from datasets import  Dataset\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Any, Callable, Tuple\n",
    "import time\n",
    "from src.inference.inference_utils import generate_answers, extract_batch\n",
    "\n",
    "def batch_extract_answer_token_activations(\n",
    "    model: PreTrainedModel,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    dataset: Dataset,\n",
    "    batch_size: int = 4,\n",
    "    idx_start_sample: int = 0,\n",
    "    max_samples: int = 1000,\n",
    "    output_path: str = \"outputs/all_batch_results.pkl\",\n",
    "    build_prompt_fn: Callable[[str, str], str] = None,\n",
    "    get_layer_output_fn: Callable = None,\n",
    "    layer_idx: int = -1,  \n",
    "    extract_token_activations_fn: Callable = None, \n",
    "    include_prompt: bool = True,\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs batched inference on a dataset using a decoder-only language model.\n",
    "    For each batch, generates answers, extracts token-level activations for the generated answer,\n",
    "    and appends the results to a pickle file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : PreTrainedModel\n",
    "        The causal language model to evaluate (e.g., LLaMA).\n",
    "    tokenizer : PreTrainedTokenizer\n",
    "        The corresponding tokenizer.\n",
    "    dataset : Dataset\n",
    "        The input dataset.\n",
    "    batch_size : int\n",
    "        Number of samples per batch.\n",
    "    idx_start_sample : int\n",
    "        Index of the first sample to process from the dataset.\n",
    "    max_samples : int\n",
    "        Total number of examples to process from the dataset, starting from idx_start_sample. \n",
    "    output_path : str\n",
    "        Path to the pickle file for saving intermediate results.\n",
    "    build_prompt_fn : Callable\n",
    "        Function to build a prompt from context and question.\n",
    "    get_layer_output_fn : Callable\n",
    "        Function to extract the output of a specific model layer. \n",
    "    layer_idx : int\n",
    "        Index of the transformer layer to extract activations from (default: -1 for last layer).\n",
    "    extract_token_activations_fn : Callable\n",
    "        Function to extract token activations from a model layer (default is average).\n",
    "    include_prompt : bool\n",
    "        Whether to include the prompt in the embedding extraction.\n",
    "        *Note:* Tokenization will always include the prompt.  \n",
    "    **kwargs :\n",
    "        Extra keyword arguments passed to extract_token_activations_fn, including start_offset.\n",
    "    \"\"\"\n",
    "    for i in tqdm(range(idx_start_sample, idx_start_sample + max_samples, batch_size)):\n",
    "        batch_answers = []               # Generated answers\n",
    "        batch_generated_embeddings = []  # Embeddings of generated answers\n",
    "        batch_dataset_ids = []           # 'id' field from dataset\n",
    "        batch_dataset_original_idx = []  # Original indices from dataset\n",
    "\n",
    "        # Extract a batch from the dataset\n",
    "        batch = extract_batch(dataset, i, batch_size)\n",
    "        prompts = [build_prompt_fn(s[\"context\"], s[\"question\"]) for s in batch]\n",
    "        \n",
    "        # Tokenize the prompt \n",
    "        inputs = tokenizer(prompts, padding=True, truncation=True, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        # Generate the answers for the batch\n",
    "        output_ids = generate_answers(model, inputs, tokenizer)\n",
    "        \n",
    "        for j in range(len(prompts)):\n",
    "            print(f\"***************** j={j} *****************\")\n",
    "            # --- Decode token IDs into text ---\n",
    "            prompt_len = len(inputs[\"input_ids\"][j])  # Length of the prompt for example j\n",
    "            generated_answer_ids = output_ids[j][prompt_len:]  # Remove prompt part\n",
    "            generated_answer = tokenizer.decode(generated_answer_ids, skip_special_tokens=True).strip()\n",
    "            print(\"prompt_len: \", prompt_len)\n",
    "            \n",
    "            # Create the full sequence: prompt + generated answer\n",
    "            full_input_sequence = tokenizer.decode(inputs[\"input_ids\"][j], skip_special_tokens=True) + generated_answer\n",
    "            print(\"full_input_sequence\", full_input_sequence)\n",
    "            \n",
    "            # --- Tokenize the full sequence (prompt + generated answer) for activation extraction ---\n",
    "            # We cannot directly use `output_ids` and need to retokenize since we need attention_mask for get_layer_output_fn\n",
    "            full_inputs = tokenizer(full_input_sequence, padding=True, truncation=True, return_tensors=\"pt\").to(model.device)\n",
    "            print(\"full_inputs\", full_inputs)\n",
    "\n",
    "            # --- Extract token embeddings for the full sequence ---\n",
    "            selected_layer = get_layer_output_fn(model, full_inputs, layer_idx)\n",
    "\n",
    "            # --- Start offset : if include_prompt is True, we start after the prompt ---\n",
    "            if include_prompt:\n",
    "                start_offset = prompt_len  # Start after the prompt if we include the prompt\n",
    "            else:\n",
    "                start_offset = kwargs.get(\"start_offset\", 0)  # Get start_offset from kwargs if no prompt included\n",
    "            \n",
    "            # Call the specified activation extraction function\n",
    "            selected_token_vecs, target_indices  = extract_token_activations_fn(\n",
    "                selected_layer, \n",
    "                full_inputs[\"attention_mask\"], \n",
    "                device=selected_layer.device,\n",
    "                start_offset=start_offset,  # Pass start_offset (calculated above)\n",
    "                **kwargs  # Pass other kwargs as needed (e.g., end_offset, etc.)\n",
    "            )\n",
    "\n",
    "            print(\"=========DECODING==========\")\n",
    "            print(\"target_indices\", target_indices)\n",
    "            print(f\"\\nDecoding START target_indices:-----{tokenizer.decode(full_inputs['input_ids'][0][target_indices[0][0]:target_indices[0][1]+1].tolist())}-----Decoding end\\n\")\n",
    "\n",
    "            print(\"=========END DECODING==========\")\n",
    "\n",
    "            # --- Store everything ---\n",
    "            batch_dataset_ids.append(batch[j]['id'])\n",
    "            batch_dataset_original_idx.append(batch[j]['original_index'])\n",
    "            batch_answers.append(generated_answer)\n",
    "            batch_generated_embeddings.append(selected_token_vecs.cpu())\n",
    "\n",
    "        # --- Save progress to pickle after each batch ---\n",
    "        '''\n",
    "        batch_results = {\n",
    "            \"id\": batch_dataset_ids,\n",
    "            \"original_indices\": batch_dataset_original_idx,\n",
    "            \"gen_answers\": batch_answers,\n",
    "            \"activations\": batch_generated_embeddings\n",
    "        }\n",
    "        '''\n",
    "        #append_to_pickle(output_path, batch_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************** j=0 *****************\n",
      "prompt_len:  341\n",
      "full_input_sequence [INST] <<SYS>>\n",
      "Just give the answer, without a complete sentence. Reply with 'Impossible to answer' if answer not in context.\n",
      "<<SYS>>\n",
      "\n",
      "Context:\n",
      "Madonna gave another provocative performance later that year at the 2003 MTV Video Music Awards, while singing \"Hollywood\" with Britney Spears, Christina Aguilera, and Missy Elliott. Madonna sparked controversy for kissing Spears and Aguilera suggestively during the performance. In October 2003, Madonna provided guest vocals on Spears' single \"Me Against the Music\". It was followed with the release of Remixed & Revisited. The EP contained remixed versions of songs from American Life and included \"Your Honesty\", a previously unreleased track from the Bedtime Stories recording sessions. Madonna also signed a contract with Callaway Arts & Entertainment to be the author of five children's books. The first of these books, titled The English Roses, was published in September 2003. The story was about four English schoolgirls and their envy and jealousy of each other. Kate Kellway from The Guardian commented, \"[Madonna] is an actress playing at what she can never be—a JK Rowling, an English rose.\" The book debuted at the top of The New York Times Best Seller list and became the fastest-selling children's picture book of all time.\n",
      "\n",
      "Question:\n",
      "What was the title of the first book Madonna penned?\n",
      "\n",
      "Answer:\n",
      "[/INST]The English Roses\n",
      "full_inputs {'input_ids': tensor([[    1,   518, 25580, 29962,  3532, 14816, 29903,  6778,    13, 14084,\n",
      "          2367,   278,  1234, 29892,  1728,   263,  4866, 10541, 29889, 10088,\n",
      "           368,   411,   525,  1888, 27338,   304,  1234, 29915,   565,  1234,\n",
      "           451,   297,  3030, 29889,    13,  9314, 14816, 29903,  6778,    13,\n",
      "            13,  2677, 29901,    13, 21878, 11586,  4846,  1790, 25725,  1230,\n",
      "          4180,  2678,   393,  1629,   472,   278, 29871, 29906, 29900, 29900,\n",
      "         29941, 28982, 13987,  6125,  9220, 29892,  1550, 23623,   376, 19984,\n",
      "         16239, 29908,   411,  3230,  3801,  5013,  1503, 29892,  2819,  1099,\n",
      "         24106,   309,  1572, 29892,   322,  4750, 29891, 26656,  1501, 29889,\n",
      "         26432, 16267,   287, 19341, 29891,   363, 20057,   292,  5013,  1503,\n",
      "           322, 24106,   309,  1572,  4368,  3598,  2645,   278,  4180, 29889,\n",
      "           512,  5533, 29871, 29906, 29900, 29900, 29941, 29892, 26432,  4944,\n",
      "         17838, 17985,   373,  5013,  1503, 29915,  2323,   376,  6816, 11454,\n",
      "           303,   278,  6125,  1642,   739,   471,  5643,   411,   278,  6507,\n",
      "           310,  5240, 11925,   669,   830,  1730,  1573, 29889,   450, 16502,\n",
      "         11122,  1083, 11925,  6910,   310, 12516,   515,  3082,  4634,   322,\n",
      "          5134,   376, 10858,  7906, 14596,   613,   263,  9251,   443,   276,\n",
      "          4611,  5702,   515,   278, 14195,  2230,   624,  3842, 16867, 21396,\n",
      "         29889, 26432,   884,  8794,   263,  8078,   411,  3037,   433,  1582,\n",
      "         11401,   669, 18189,   304,   367,   278,  4148,   310,  5320,  4344,\n",
      "         29915, 29879,  8277, 29889,   450,   937,   310,  1438,  8277, 29892,\n",
      "         25278,   450,  4223,  5678,   267, 29892,   471,  6369,   297,  3839,\n",
      "         29871, 29906, 29900, 29900, 29941, 29889,   450,  5828,   471,  1048,\n",
      "          3023,  4223,  3762, 29887,  9968,   322,  1009,  8829, 29891,   322,\n",
      "          1444, 20521, 29891,   310,  1269,   916, 29889, 23738,   476,   514,\n",
      "          1582,   515,   450, 29429, 19952, 29892, 14704, 21878, 11586, 29962,\n",
      "           338,   385, 20993,  8743,   472,   825,  1183,   508,  2360,   367,\n",
      "         30003, 29874,   435, 29968, 11438,  1847, 29892,   385,  4223, 11492,\n",
      "          1213,   450,  3143,  2553,  3860,   472,   278,  2246,   310,   450,\n",
      "          1570,  3088, 10277,  6407,   317,  4539,  1051,   322,  3897,   278,\n",
      "          5172,   342, 29899, 29879,  7807,  4344, 29915, 29879,  7623,  3143,\n",
      "           310,   599,   931, 29889,    13,    13, 16492, 29901,    13,  5618,\n",
      "           471,   278,  3611,   310,   278,   937,  3143, 26432,   282,  2108,\n",
      "           287, 29973,    13,    13, 22550, 29901,    13, 29961, 29914, 25580,\n",
      "         29962,  1576,  4223,  5678,   267]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "=========DECODING==========\n",
      "target_indices tensor([[341, 344]], device='cuda:0')\n",
      "\n",
      "Decoding START target_indices:-----The English Roses-----Decoding end\n",
      "\n",
      "=========END DECODING==========\n",
      "***************** j=1 *****************\n",
      "prompt_len:  341\n",
      "full_input_sequence [INST] <<SYS>>\n",
      "Just give the answer, without a complete sentence. Reply with 'Impossible to answer' if answer not in context.\n",
      "<<SYS>>\n",
      "\n",
      "Context:\n",
      "Following the success of Le Journal de Mickey (1934–44), dedicated comics magazines and full-colour comics albums became the primary outlet for comics in the mid-20th century. As in the US, at the time comics were seen as infantile and a threat to culture and literacy; commentators stated that \"none bear up to the slightest serious analysis\",[c] and that comics were \"the sabotage of all art and all literature\".[d]\n",
      "\n",
      "Question:\n",
      "In the United States in the middle of the 20th century comics were seen as a risk to culture and what?\n",
      "\n",
      "Answer:\n",
      "[/INST]Literacy\n",
      "full_inputs {'input_ids': tensor([[    1,   518, 25580, 29962,  3532, 14816, 29903,  6778,    13, 14084,\n",
      "          2367,   278,  1234, 29892,  1728,   263,  4866, 10541, 29889, 10088,\n",
      "           368,   411,   525,  1888, 27338,   304,  1234, 29915,   565,  1234,\n",
      "           451,   297,  3030, 29889,    13,  9314, 14816, 29903,  6778,    13,\n",
      "            13,  2677, 29901,    13, 29943,  2952,   292,   278,  2551,   310,\n",
      "           951,  8237,   316, 20279,  1989,   313, 29896, 29929, 29941, 29946,\n",
      "         29994, 29946, 29946,   511, 16955,   419,  1199,  2320,   834,  1475,\n",
      "           322,  2989, 29899,  1054,   473,   419,  1199, 20618,  3897,   278,\n",
      "          7601,   714,  1026,   363,   419,  1199,   297,   278,  7145, 29899,\n",
      "         29906, 29900,   386,  6462, 29889,  1094,   297,   278,  3148, 29892,\n",
      "           472,   278,   931,   419,  1199,   892,  3595,   408, 28056,   488,\n",
      "           322,   263, 28469,   304,  9257,   322,  4631,  4135, 29936,  3440,\n",
      "          4097,  8703,   393,   376,  9290, 11460,   701,   304,   278,  7248,\n",
      "           342, 10676,  7418,   613, 29961, 29883, 29962,   322,   393,   419,\n",
      "          1199,   892,   376,  1552, 15296,   327,   482,   310,   599,  1616,\n",
      "           322,   599, 12845,  1642, 29961, 29881, 29962,    13,    13, 16492,\n",
      "         29901,    13,   797,   278,  3303,  3900,   297,   278,  7256,   310,\n",
      "           278, 29871, 29906, 29900,   386,  6462,   419,  1199,   892,  3595,\n",
      "           408,   263, 12045,   304,  9257,   322,   825, 29973,    13,    13,\n",
      "         22550, 29901,    13, 29961, 29914, 25580, 29962, 24938,  4135]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========DECODING==========\n",
      "target_indices tensor([[198, 198]], device='cuda:0')\n",
      "\n",
      "Decoding START target_indices:-----acy-----Decoding end\n",
      "\n",
      "=========END DECODING==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_extract_answer_token_activations(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=id_fit_dataset,\n",
    "    batch_size=2,\n",
    "    idx_start_sample= 2,\n",
    "    max_samples= 2,\n",
    "    output_path = \"outputs/all_batch_results.pkl\",\n",
    "    build_prompt_fn=build_prompt,\n",
    "    get_layer_output_fn=get_layer_output,\n",
    "    layer_idx = -1,  \n",
    "    extract_token_activations_fn=extract_average_token_activations,\n",
    "    include_prompt = True,\n",
    "    #start_offset = 0,\n",
    "    end_offset = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************** j=0 *****************\n",
      "generated_answer The English Roses\n",
      "prompt_len:  341\n",
      "***************** j=1 *****************\n",
      "generated_answer Literacy\n",
      "prompt_len:  341\n",
      "**********************************\n",
      "full_input_sequence ['[INST] <<SYS>>\\nJust give the answer, without a complete sentence. Reply with \\'Impossible to answer\\' if answer not in context.\\n<<SYS>>\\n\\nContext:\\nMadonna gave another provocative performance later that year at the 2003 MTV Video Music Awards, while singing \"Hollywood\" with Britney Spears, Christina Aguilera, and Missy Elliott. Madonna sparked controversy for kissing Spears and Aguilera suggestively during the performance. In October 2003, Madonna provided guest vocals on Spears\\' single \"Me Against the Music\". It was followed with the release of Remixed & Revisited. The EP contained remixed versions of songs from American Life and included \"Your Honesty\", a previously unreleased track from the Bedtime Stories recording sessions. Madonna also signed a contract with Callaway Arts & Entertainment to be the author of five children\\'s books. The first of these books, titled The English Roses, was published in September 2003. The story was about four English schoolgirls and their envy and jealousy of each other. Kate Kellway from The Guardian commented, \"[Madonna] is an actress playing at what she can never be—a JK Rowling, an English rose.\" The book debuted at the top of The New York Times Best Seller list and became the fastest-selling children\\'s picture book of all time.\\n\\nQuestion:\\nWhat was the title of the first book Madonna penned?\\n\\nAnswer:\\n[/INST]The English Roses', '[INST] <<SYS>>\\nJust give the answer, without a complete sentence. Reply with \\'Impossible to answer\\' if answer not in context.\\n<<SYS>>\\n\\nContext:\\nFollowing the success of Le Journal de Mickey (1934–44), dedicated comics magazines and full-colour comics albums became the primary outlet for comics in the mid-20th century. As in the US, at the time comics were seen as infantile and a threat to culture and literacy; commentators stated that \"none bear up to the slightest serious analysis\",[c] and that comics were \"the sabotage of all art and all literature\".[d]\\n\\nQuestion:\\nIn the United States in the middle of the 20th century comics were seen as a risk to culture and what?\\n\\nAnswer:\\n[/INST]Literacy']\n",
      "full_inputs {'input_ids': tensor([[    1,   518, 25580, 29962,  3532, 14816, 29903,  6778,    13, 14084,\n",
      "          2367,   278,  1234, 29892,  1728,   263,  4866, 10541, 29889, 10088,\n",
      "           368,   411,   525,  1888, 27338,   304,  1234, 29915,   565,  1234,\n",
      "           451,   297,  3030, 29889,    13,  9314, 14816, 29903,  6778,    13,\n",
      "            13,  2677, 29901,    13, 21878, 11586,  4846,  1790, 25725,  1230,\n",
      "          4180,  2678,   393,  1629,   472,   278, 29871, 29906, 29900, 29900,\n",
      "         29941, 28982, 13987,  6125,  9220, 29892,  1550, 23623,   376, 19984,\n",
      "         16239, 29908,   411,  3230,  3801,  5013,  1503, 29892,  2819,  1099,\n",
      "         24106,   309,  1572, 29892,   322,  4750, 29891, 26656,  1501, 29889,\n",
      "         26432, 16267,   287, 19341, 29891,   363, 20057,   292,  5013,  1503,\n",
      "           322, 24106,   309,  1572,  4368,  3598,  2645,   278,  4180, 29889,\n",
      "           512,  5533, 29871, 29906, 29900, 29900, 29941, 29892, 26432,  4944,\n",
      "         17838, 17985,   373,  5013,  1503, 29915,  2323,   376,  6816, 11454,\n",
      "           303,   278,  6125,  1642,   739,   471,  5643,   411,   278,  6507,\n",
      "           310,  5240, 11925,   669,   830,  1730,  1573, 29889,   450, 16502,\n",
      "         11122,  1083, 11925,  6910,   310, 12516,   515,  3082,  4634,   322,\n",
      "          5134,   376, 10858,  7906, 14596,   613,   263,  9251,   443,   276,\n",
      "          4611,  5702,   515,   278, 14195,  2230,   624,  3842, 16867, 21396,\n",
      "         29889, 26432,   884,  8794,   263,  8078,   411,  3037,   433,  1582,\n",
      "         11401,   669, 18189,   304,   367,   278,  4148,   310,  5320,  4344,\n",
      "         29915, 29879,  8277, 29889,   450,   937,   310,  1438,  8277, 29892,\n",
      "         25278,   450,  4223,  5678,   267, 29892,   471,  6369,   297,  3839,\n",
      "         29871, 29906, 29900, 29900, 29941, 29889,   450,  5828,   471,  1048,\n",
      "          3023,  4223,  3762, 29887,  9968,   322,  1009,  8829, 29891,   322,\n",
      "          1444, 20521, 29891,   310,  1269,   916, 29889, 23738,   476,   514,\n",
      "          1582,   515,   450, 29429, 19952, 29892, 14704, 21878, 11586, 29962,\n",
      "           338,   385, 20993,  8743,   472,   825,  1183,   508,  2360,   367,\n",
      "         30003, 29874,   435, 29968, 11438,  1847, 29892,   385,  4223, 11492,\n",
      "          1213,   450,  3143,  2553,  3860,   472,   278,  2246,   310,   450,\n",
      "          1570,  3088, 10277,  6407,   317,  4539,  1051,   322,  3897,   278,\n",
      "          5172,   342, 29899, 29879,  7807,  4344, 29915, 29879,  7623,  3143,\n",
      "           310,   599,   931, 29889,    13,    13, 16492, 29901,    13,  5618,\n",
      "           471,   278,  3611,   310,   278,   937,  3143, 26432,   282,  2108,\n",
      "           287, 29973,    13,    13, 22550, 29901,    13, 29961, 29914, 25580,\n",
      "         29962,  1576,  4223,  5678,   267],\n",
      "        [    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     1,   518, 25580, 29962,\n",
      "          3532, 14816, 29903,  6778,    13, 14084,  2367,   278,  1234, 29892,\n",
      "          1728,   263,  4866, 10541, 29889, 10088,   368,   411,   525,  1888,\n",
      "         27338,   304,  1234, 29915,   565,  1234,   451,   297,  3030, 29889,\n",
      "            13,  9314, 14816, 29903,  6778,    13,    13,  2677, 29901,    13,\n",
      "         29943,  2952,   292,   278,  2551,   310,   951,  8237,   316, 20279,\n",
      "          1989,   313, 29896, 29929, 29941, 29946, 29994, 29946, 29946,   511,\n",
      "         16955,   419,  1199,  2320,   834,  1475,   322,  2989, 29899,  1054,\n",
      "           473,   419,  1199, 20618,  3897,   278,  7601,   714,  1026,   363,\n",
      "           419,  1199,   297,   278,  7145, 29899, 29906, 29900,   386,  6462,\n",
      "         29889,  1094,   297,   278,  3148, 29892,   472,   278,   931,   419,\n",
      "          1199,   892,  3595,   408, 28056,   488,   322,   263, 28469,   304,\n",
      "          9257,   322,  4631,  4135, 29936,  3440,  4097,  8703,   393,   376,\n",
      "          9290, 11460,   701,   304,   278,  7248,   342, 10676,  7418,   613,\n",
      "         29961, 29883, 29962,   322,   393,   419,  1199,   892,   376,  1552,\n",
      "         15296,   327,   482,   310,   599,  1616,   322,   599, 12845,  1642,\n",
      "         29961, 29881, 29962,    13,    13, 16492, 29901,    13,   797,   278,\n",
      "          3303,  3900,   297,   278,  7256,   310,   278, 29871, 29906, 29900,\n",
      "           386,  6462,   419,  1199,   892,  3595,   408,   263, 12045,   304,\n",
      "          9257,   322,   825, 29973,    13,    13, 22550, 29901,    13, 29961,\n",
      "         29914, 25580, 29962, 24938,  4135]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========DECODING==========\n",
      "target_indices[k] tensor([341, 344], device='cuda:0')\n",
      "\n",
      "Decoding START target_indices[k]:-----The English Roses-----Decoding end\n",
      "\n",
      "=========END DECODING==========\n",
      "=========DECODING==========\n",
      "target_indices[k] tensor([344, 344], device='cuda:0')\n",
      "\n",
      "Decoding START target_indices[k]:-----acy-----Decoding end\n",
      "\n",
      "=========END DECODING==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_extract_answer_token_activations(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=id_fit_dataset,\n",
    "    batch_size=2,\n",
    "    idx_start_sample= 2,\n",
    "    max_samples= 2,\n",
    "    output_path = \"outputs/all_batch_results.pkl\",\n",
    "    build_prompt_fn=build_prompt,\n",
    "    get_layer_output_fn=get_layer_output,\n",
    "    layer_idx = -1,  \n",
    "    extract_token_activations_fn=extract_average_token_activations,\n",
    "    include_prompt = True,\n",
    "    #start_offset = 0,\n",
    "    end_offset = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizer, PreTrainedModel, BatchEncoding\n",
    "import torch\n",
    "from datasets import  Dataset\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Any, Callable, Tuple\n",
    "import time\n",
    "from src.inference.inference_utils import (\n",
    "    build_prompt,\n",
    "    get_layer_output,\n",
    "    generate_answers, \n",
    "    extract_batch, \n",
    "    extract_last_token_activations,\n",
    "    extract_average_token_activations,\n",
    "    extract_max_token_activations\n",
    ")\n",
    "from src.data_reader.pickle_io import append_to_pickle\n",
    "\n",
    "\n",
    "def batch_extract_answer_token_activations(\n",
    "    model: PreTrainedModel,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    dataset: Dataset,\n",
    "    batch_size: int = 4,\n",
    "    idx_start_sample: int = 0,\n",
    "    max_samples: int = 1000,\n",
    "    save_to_pkl: bool = False,\n",
    "    output_path: str = \"outputs/all_batch_results.pkl\",\n",
    "    build_prompt_fn: Callable[[str, str], str] = None,\n",
    "    get_layer_output_fn: Callable = None,\n",
    "    layer_idx: int = -1,  \n",
    "    extract_token_activations_fn: Callable = None, \n",
    "    include_prompt: bool = True,\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs batched inference on a dataset using a decoder-only language model.\n",
    "    For each batch, generates answers, extracts token-level activations for the generated answer,\n",
    "    and appends the results to a pickle file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : PreTrainedModel\n",
    "        The causal language model to evaluate (e.g., LLaMA).\n",
    "    tokenizer : PreTrainedTokenizer\n",
    "        The corresponding tokenizer.\n",
    "    dataset : Dataset\n",
    "        The input dataset.\n",
    "    batch_size : int\n",
    "        Number of samples per batch.\n",
    "    idx_start_sample : int\n",
    "        Index of the first sample to process from the dataset.\n",
    "    max_samples : int\n",
    "        Total number of examples to process from the dataset, starting from idx_start_sample. \n",
    "    save_to_pkl : bool\n",
    "        If True, activations are appended to the pickle file at output_path.\n",
    "        If False, the function returns a list of activations.\n",
    "    output_path : str\n",
    "        Path to the pickle file for saving intermediate results.\n",
    "    build_prompt_fn : Callable\n",
    "        Function to build a prompt from context and question.\n",
    "    get_layer_output_fn : Callable\n",
    "        Function to extract the output of a specific model layer. \n",
    "    layer_idx : int\n",
    "        Index of the transformer layer to extract activations from (default: -1 for last layer).\n",
    "    extract_token_activations_fn : Callable\n",
    "        Function to extract token activations from a model layer (default is average).\n",
    "    include_prompt : bool\n",
    "        Whether to include the prompt in the embedding extraction.\n",
    "        *Note:* Tokenization will always include the prompt.  \n",
    "    **kwargs :\n",
    "        Extra keyword arguments passed to extract_token_activations_fn, including start_offset.\n",
    "    \"\"\"    \n",
    "    batch_activations = []  # Chosen token activation vectors\n",
    "\n",
    "    for i in tqdm(range(idx_start_sample, idx_start_sample + max_samples, batch_size)):\n",
    "        batch_answers = []   # Generated answers\n",
    " \n",
    "        # Extract a batch from the dataset\n",
    "        batch = extract_batch(dataset, i, batch_size)\n",
    "        prompts = [build_prompt_fn(s[\"context\"], s[\"question\"]) for s in batch]\n",
    "\n",
    "        # Tokenize the prompt \n",
    "        inputs = tokenizer(prompts, padding=True, truncation=True, return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        # Compute the number of non-padding tokens in each prompt (true prompt length)\n",
    "        prompt_non_pad_len = inputs[\"attention_mask\"].sum(dim=1).tolist()  # Shape (batch_size,)\n",
    "        print(\"prompt_lens\", prompt_non_pad_len)\n",
    "\n",
    "        # Generate the answers for the batch\n",
    "        output_ids = generate_answers(model, inputs, tokenizer)\n",
    "\n",
    "        # Build full sequences (prompt + generated answer) for each sample in the batch\n",
    "        full_sequences = []\n",
    "        for j in range(len(prompts)):\n",
    "            # --- Total length of the tokenized prompt, padding included ---\n",
    "            prompt_len = len(inputs[\"input_ids\"][j])  # Length of the prompt for example j\n",
    "            # --- Decode token IDs into text ---\n",
    "            generated_answer_ids = output_ids[j][prompt_len:]  # Remove prompt part\n",
    "            generated_answer = tokenizer.decode(generated_answer_ids, skip_special_tokens=True).strip()\n",
    "            # --- Decode the prompt tokens to text ---\n",
    "            prompt_text = tokenizer.decode(inputs[\"input_ids\"][j], skip_special_tokens=True)\n",
    "            # --- Combine prompt and answer for full sequence ---\n",
    "            full_sequences.append(prompt_text + generated_answer)\n",
    "            # --- Store generated answers ---\n",
    "            batch_answers.append(generated_answer)\n",
    "            print(\"generated_answer:\",generated_answer )\n",
    "\n",
    "        # Tokenize the full sequences (prompt + answer) again, with padding and truncation\n",
    "        # We need to retokenize and cannot directly use `output_ids` since we need attention_mask for get_layer_output_fn\n",
    "        full_inputs = tokenizer(full_sequences, padding=True, truncation=True, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        # Extract activations from the specified model layer for all sequences in the batch\n",
    "        selected_layer = get_layer_output_fn(model, full_inputs, layer_idx)\n",
    "\n",
    "        # Compute the start offsets for activation extraction\n",
    "        if include_prompt:\n",
    "            # --- If include_prompt is True, use the value from kwargs (or zeros if not provided) ---\n",
    "            start_offsets = kwargs.get(\"start_offset\", torch.zeros(len(prompts), device=selected_layer.device)) # Shape (batch_size,) \n",
    "        else:\n",
    "            # --- If include_prompt is False, use the true prompt length (non-padding tokens) ---\n",
    "            start_offsets = torch.tensor(prompt_non_pad_len, device=selected_layer.device)  # Shape (batch_size,) \n",
    "\n",
    "        # Remove start_offset from kwargs to avoid passing it twice to the extraction function\n",
    "        kwargs.pop(\"start_offset\", None)\n",
    "\n",
    "        # Call the specified activation extraction function\n",
    "        selected_token_vecs, target_indices = extract_token_activations_fn(\n",
    "            selected_layer,\n",
    "            full_inputs[\"attention_mask\"],\n",
    "            device=selected_layer.device,\n",
    "            start_offset=start_offsets,  # Shape (batch_size,) \n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "        # 9. (Optional) Decode for verification \n",
    "        if False:\n",
    "            for k in range(len(prompts)):\n",
    "                print(\"=========DECODING==========\")\n",
    "                print(\"target_indices[k]\", target_indices[k])\n",
    "                print(f\"\\nDecoding START target_indices[k]:-----{tokenizer.decode(full_inputs['input_ids'][k][int(target_indices[k][0]):int(target_indices[k][1])+1])}-----Decoding end\\n\")\n",
    "                print(\"=========END DECODING==========\")\n",
    "\n",
    "        # --- Store everything ---\n",
    "        batch_dataset_ids = [s['id'] for s in batch]  # 'id' field from dataset\n",
    "        batch_dataset_original_idx = [s['original_index'] for s in batch] # Original indices from dataset\n",
    "        activations = [selected_token_vecs[j].unsqueeze(0).cpu() for j in range(selected_token_vecs.size(0))] # Embeddings of generated answers\n",
    "\n",
    "       \n",
    "        # --- Save progress to pickle after each batch ---\n",
    "        batch_results = {\n",
    "            \"id\": batch_dataset_ids,\n",
    "            \"original_indices\": batch_dataset_original_idx,\n",
    "            \"gen_answers\": batch_answers,\n",
    "            \"activations\": activations\n",
    "        }\n",
    "\n",
    "        if save_to_pkl:\n",
    "            append_to_pickle(output_path, batch_results)\n",
    "        else:\n",
    "            batch_activations.extend(activations)\n",
    "        \n",
    "    if not save_to_pkl:\n",
    "        return batch_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:06<00:00,  1.33s/it]\n"
     ]
    }
   ],
   "source": [
    "from src.inference.inference_utils import batch_extract_answer_token_activations\n",
    "batch_extract_answer_token_activations(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=id_fit_dataset,\n",
    "    batch_size=2,\n",
    "    idx_start_sample= 5,\n",
    "    max_samples=10,\n",
    "    save_to_pkl = False, \n",
    "    output_path = \"../raw/TEST/all_batch_results.pkl\",\n",
    "    build_prompt_fn=build_prompt,\n",
    "    get_layer_output_fn=get_layer_output,\n",
    "    layer_idx = -1,  \n",
    "    extract_token_activations_fn=extract_max_token_activations,\n",
    "    include_prompt = False,\n",
    "    start_offset = 40,\n",
    "    end_offset = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refaire les fonctions extract_*_token_activations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_average_token_activations(\n",
    "    selected_layer: torch.Tensor,\n",
    "    attention_mask: torch.Tensor,\n",
    "    device: torch.device,\n",
    "    start_offset: int = 0,\n",
    "    end_offset: int = 0\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Extract the mean activation vector over a token span for each sequence in a batch.\n",
    "    The span is defined by applying start_offset (from the first non-padding token)\n",
    "    and end_offset (from the last non-padding token).\n",
    "    Supports left/right/mixed padding.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    selected_layer : torch.Tensor\n",
    "        Output tensor from the selected model layer (batch_size x seq_len x hidden_size).\n",
    "    attention_mask : torch.Tensor\n",
    "        Attention mask (batch_size x seq_len).\n",
    "    device : torch.device\n",
    "        Device for computation.\n",
    "    start_offset : int\n",
    "        Offset from first non-padding token (to skip e.g. [INST]).\n",
    "    end_offset : int\n",
    "        Offset from last non-padding token (to skip e.g. [/INST]).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        Averaged embeddings (batch_size x hidden_size)\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, _ = selected_layer.shape\n",
    "\n",
    "    # Compute first and last valid token positions (regardless of padding side)\n",
    "    #--- First non-padding token is at index: number of padding tokens\n",
    "    first_indices = attention_mask.float().argmax(dim=1)  \n",
    "    #--- Last non-padding token is at the end: compute its index by flipping and computing position from end\n",
    "    flipped_mask = attention_mask.flip(dims=[1])\n",
    "    last_offsets = flipped_mask.float().argmax(dim=1)\n",
    "    last_indices = seq_len - 1 - last_offsets\n",
    "\n",
    "    # Move to device\n",
    "    first_indices = first_indices.to(device)\n",
    "    last_indices = last_indices.to(device)\n",
    "\n",
    "    # Apply offsets (e.g., skip <s> [INST] or [\\INST])\n",
    "    target_first_indices = first_indices + start_offset \n",
    "    target_last_indices = last_indices + end_offset \n",
    "\n",
    "    # Clamp indices to valid range\n",
    "    target_first_indices = torch.clamp(target_first_indices, min=0, max=seq_len - 1)\n",
    "    target_last_indices = torch.clamp(target_last_indices, min=0, max=seq_len - 1)\n",
    "\n",
    "    # Compute mask for averaging\n",
    "    #--- Create a tensor of positions: shape (1, seq_len), then expand to (batch_size, seq_len)\n",
    "    positions = torch.arange(seq_len, device=device).unsqueeze(0).expand(batch_size, seq_len)\n",
    "    #--- Build a boolean mask: True where the position is within [target_first_indices, target_last_indices] for each sequence\n",
    "    mask = (positions >= target_first_indices.unsqueeze(1)) & (positions <= target_last_indices.unsqueeze(1))\n",
    "    #--- Convert the boolean mask to float and add a singleton dimension for broadcasting with selected_layer\n",
    "    mask = mask.float().unsqueeze(-1)  # (batch_size, seq_len, 1)\n",
    "\n",
    "    # Apply mask and compute mean\n",
    "    #--- Apply the mask to the activations: zero out tokens outside the target interval\n",
    "    masked = selected_layer * mask\n",
    "    #--- Count the number of selected tokens for each sequence (avoid division by zero with clamp)\n",
    "    counts = mask.sum(dim=1).clamp(min=1e-6)\n",
    "    #--- Compute the mean activation vector for each sequence over the selected interval\n",
    "    avg = masked.sum(dim=1) / counts # (batch_size, hidden_size)\n",
    "\n",
    "    # Optionally, return also the indices used\n",
    "    indices = torch.stack([target_first_indices, target_last_indices], dim=1)\n",
    "\n",
    "    return avg, indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "import torch\n",
    "\n",
    "def extract_token_activations(\n",
    "    selected_layer: torch.Tensor,\n",
    "    attention_mask: torch.Tensor,\n",
    "    device: torch.device,\n",
    "    mode: Literal[\"average\", \"last\", \"max\"] = \"average\",\n",
    "    start_offset: int = 0,\n",
    "    end_offset: int = 0\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Extract token-level activations using one of three modes: average, last, or max.\n",
    "    Extract the activations vector over a token span for each sequence in a batch.\n",
    "    The span is defined by applying start_offset (from the first non-padding token)\n",
    "    and end_offset (from the last non-padding token).\n",
    "    Supports left/right/mixed padding.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    selected_layer : torch.Tensor\n",
    "        Output tensor from the selected model layer of shape (batch_size, seq_len, hidden_size).\n",
    "    attention_mask : torch.Tensor\n",
    "        Attention mask of shape (batch_size, seq_len).\n",
    "    device : torch.device\n",
    "        Device for computation.\n",
    "    mode : str\n",
    "        Aggregation method: \"average\", \"last\", or \"max\".\n",
    "    start_offset : int\n",
    "        Offset from the first non-padding token, used in \"average\"/\"max\" (to skip e.g. [INST]).\n",
    "    end_offset : int\n",
    "        Offset from the last non-padding token (to skip e.g. [\\INST]).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        Aggregated embeddings of shape (batch_size, hidden_size)\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, _ = selected_layer.shape\n",
    "\n",
    "    # =======================================\n",
    "    # Compute first and last valid token positions (regardless of padding side)\n",
    "    # =======================================\n",
    "    # First non-padding token is at index: number of padding tokens\n",
    "    first_indices = attention_mask.float().argmax(dim=1)  \n",
    "    # Last non-padding token is at the end: compute its index by flipping and computing position from end\n",
    "    flipped_mask = attention_mask.flip(dims=[1])\n",
    "    last_offsets = flipped_mask.float().argmax(dim=1)\n",
    "    last_indices = seq_len - 1 - last_offsets\n",
    "    # Move to device\n",
    "    first_indices = first_indices.to(device)\n",
    "    last_indices = last_indices.to(device)\n",
    "\n",
    "    # =======================================\n",
    "    # Select the last token with optional offset\n",
    "    # =======================================\n",
    "    if mode == \"last\":\n",
    "        # Compute the target index using the offset and convert to integer type\n",
    "        target_last_indices = (last_indices + end_offset).to(torch.long) \n",
    "        # Clamp indices to valid range \n",
    "        target_last_indices = torch.clamp(target_last_indices, min=0, max=seq_len - 1)\n",
    "        batch_indices = torch.arange(batch_size, device=device)\n",
    "        #--- Optionally, return also the indices used\n",
    "        indices = target_last_indices\n",
    "        last = selected_layer[batch_indices, target_last_indices] # Shape: (batch_size, hidden_size)\n",
    "        return last, indices\n",
    "    \n",
    "    # =======================================\n",
    "    # Build mask to select the token span \n",
    "    # =======================================\n",
    "    # Compute target indices using the offsets and convert to integer type\n",
    "    target_first_indices = (first_indices + start_offset).to(torch.long) \n",
    "    target_last_indices  = (last_indices + end_offset).to(torch.long) \n",
    "\n",
    "    # Clamp indices to valid range\n",
    "    target_first_indices = torch.clamp(target_first_indices, min=0, max=seq_len - 1)\n",
    "    target_last_indices  = torch.clamp(target_last_indices, min=0, max=seq_len - 1)\n",
    "\n",
    "    # Compute span mask\n",
    "    #--- Create a tensor of positions: shape (1, seq_len), then expand to (batch_size, seq_len)\n",
    "    positions = torch.arange(seq_len, device=device).unsqueeze(0).expand(batch_size, seq_len)\n",
    "    #--- Build a boolean mask: True where the position is within [target_first_indices, target_last_indices] for each sequence\n",
    "    mask = (positions >= target_first_indices.unsqueeze(1)) & (positions <= target_last_indices.unsqueeze(1))\n",
    "    #--- Convert the boolean mask to float and add a singleton dimension for broadcasting with selected_layer\n",
    "    mask = mask.float().unsqueeze(-1)  # Shape: (batch_size, seq_len, 1)\n",
    "\n",
    "    # =======================================\n",
    "    # Apply mask and compute aggregation over the selected span  \n",
    "    # =======================================\n",
    "    if mode == \"average\":\n",
    "        #--- Apply the mask to the activations: zero out tokens outside the target interval\n",
    "        masked = selected_layer * mask\n",
    "        #--- Count the number of selected tokens for each sequence (avoid division by zero with clamp)\n",
    "        counts = mask.sum(dim=1).clamp(min=1e-6)\n",
    "        #--- Compute the mean activation vector for each sequence over the selected interval\n",
    "        avg = masked.sum(dim=1) / counts # Shape: (batch_size, hidden_size)\n",
    "        #--- Optionally, return also the indices used\n",
    "        indices = torch.stack([target_first_indices, target_last_indices], dim=1)\n",
    "        return avg, indices\n",
    "\n",
    "    elif mode == \"max\":\n",
    "        #--- Apply the mask to the activations: zero out tokens outside the target interval\n",
    "        masked = selected_layer * mask.float()\n",
    "        #--- Replace padding with -inf to exclude from max calculation\n",
    "        masked = masked.masked_fill(mask.logical_not(), float('-inf'))\n",
    "        #--- Extract maximum values across sequence dimension\n",
    "        max, _ = masked.max(dim=1) # Shape: (batch_size, hidden_size)\n",
    "        #--- Optionally, return also the indices used\n",
    "        indices = torch.stack([target_first_indices, target_last_indices], dim=1)\n",
    "        return max, indices\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported mode: {mode}. Choose from 'average', 'last', or 'max'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'generate_answers' from 'src.inference.inference_utils' (/home/lila.roig/projet_ood/ood_for_hallucination_detection/src/inference/inference_utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtime\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mevaluation\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msimilarity_metrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m rouge_l_simScore, sentence_bert_simScore\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minference\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minference_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m extract_batch, generate_answers\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbatch_extract_token_activations_with_generation\u001b[39m(\n\u001b[32m     12\u001b[39m     model: PreTrainedModel,\n\u001b[32m     13\u001b[39m     tokenizer: PreTrainedTokenizer,\n\u001b[32m   (...)\u001b[39m\u001b[32m     23\u001b[39m     **kwargs\n\u001b[32m     24\u001b[39m ):\n\u001b[32m     25\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[33;03m    Runs batched inference on a dataset using a decoder-only language model.\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[33;03m    For each batch, generates answers, computes semantic similarity scores, extracts token-level activations,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     55\u001b[39m \u001b[33;03m        Extra keyword arguments passed to extract_token_activations_fn.\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'generate_answers' from 'src.inference.inference_utils' (/home/lila.roig/projet_ood/ood_for_hallucination_detection/src/inference/inference_utils.py)"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizer, PreTrainedModel, BatchEncoding\n",
    "import torch\n",
    "from datasets import  Dataset\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Any, Callable\n",
    "import time\n",
    "\n",
    "from src.evaluation.similarity_metrics import rouge_l_simScore, sentence_bert_simScore\n",
    "from src.inference.inference_utils import extract_batch, generate_answers\n",
    "\n",
    "def batch_extract_token_activations_with_generation(\n",
    "    model: PreTrainedModel,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    dataset: Dataset,\n",
    "    batch_size: int = 2,\n",
    "    idx_start_sample: int = 0,\n",
    "    max_samples: int = 2,\n",
    "    output_path: str = \"outputs/all_batch_results.pkl\",\n",
    "    build_prompt_fn: Callable[[str, str], str] = None,\n",
    "    get_layer_output_fn: Callable = None,\n",
    "    layer_idx: int = -1,  \n",
    "    extract_token_activations_fn: Callable = None,\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs batched inference on a dataset using a decoder-only language model.\n",
    "    For each batch, generates answers, computes semantic similarity scores, extracts token-level activations,\n",
    "    and appends the results to a pickle file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : PreTrainedModel\n",
    "        The causal language model to evaluate (e.g., LLaMA).\n",
    "    tokenizer : PreTrainedTokenizer\n",
    "        The corresponding tokenizer.\n",
    "    dataset : Dataset\n",
    "        The input dataset.\n",
    "    batch_size : int\n",
    "        Number of samples per batch.\n",
    "    idx_start_sample : int\n",
    "        Index of the first sample to process from the dataset.\n",
    "    max_samples : int\n",
    "        Total number of examples to process from the dataset, starting from idx_start_sample. \n",
    "    output_path : str\n",
    "        Path to the pickle file for saving intermediate results.\n",
    "    build_prompt_fn : Callable\n",
    "        Function to build a prompt from context and question.\n",
    "    get_layer_output_fn : Callable\n",
    "        Function to extract the output of a specific model layer.\n",
    "    layer_idx : int\n",
    "        Index of the transformer layer to extract activations from (default: -1 for last layer).\n",
    "    extract_token_activations_fn : Callable\n",
    "        Function to extract token activations from a model layer.\n",
    "    **kwargs :\n",
    "        Extra keyword arguments passed to extract_token_activations_fn.\n",
    "    \"\"\"\n",
    "    for i in tqdm(range(idx_start_sample, idx_start_sample + max_samples, batch_size)):\n",
    "        batch_answers = []               # Generated answers\n",
    "        batch_gt_answers = []            # Ground-truth answers\n",
    "        batch_is_correct = []            # 0/1 labels indicating correctness\n",
    "        batch_dataset_ids = []           # 'id' field from dataset\n",
    "        batch_dataset_original_idx = []  # Original indices from dataset\n",
    "        batch_rouge_scores = []          # Rouge-L scores\n",
    "        batch_sbert_scores = []          # Sentence-Bert scores\n",
    "\n",
    "        batch = extract_batch(dataset, i, batch_size)\n",
    "        prompts = [build_prompt_fn(s[\"context\"], s[\"question\"]) for s in batch]\n",
    "        '''prompts = [\n",
    "            \"The capital of France is\",\n",
    "            \"What is the name of the French Queen who was decapitated?\",\n",
    "            \"Tell me why the sky is blue?\",\n",
    "        ]'''\n",
    "        print(f\"prompts: {prompts[0]},\\n{prompts[1]}\")\n",
    "        answers = [s[\"answers\"][\"text\"] for s in batch]\n",
    "        inputs = tokenizer(prompts, padding=True, truncation=True, return_tensors=\"pt\").to(model.device)\n",
    "        prompt_len = inputs[\"input_ids\"].shape[1] # Assumes prompts padded to same length\n",
    "        print(f\"inputs:\", inputs)\n",
    "        selected_layer = get_layer_output_fn(model, inputs, layer_idx)\n",
    "        \n",
    "        attention_mask =  inputs[\"attention_mask\"]\n",
    "        #attention_mask[2][-2:]=False\n",
    "        #print(\"attention_mask:\", attention_mask)\n",
    "        \n",
    "        selected_token_vecs, target_indices = extract_token_activations_fn(\n",
    "                selected_layer, \n",
    "                attention_mask, \n",
    "                device=selected_layer.device,\n",
    "                **kwargs) \n",
    "        print(\"selected_token_vecs\", selected_token_vecs)\n",
    "        \n",
    "        for k in range(len(prompts)):\n",
    "            print(\"=========DECODING==========\")\n",
    "            print(\"target_indices[k]\", target_indices[k])\n",
    "            '''\n",
    "            print(f\"\\nDecoding START target_indices[k]:-----{tokenizer.decode(inputs['input_ids'][k][target_indices[k][0]:target_indices[k][1]+1])}-----Decoding end\\n\")\n",
    "            print(f\"SELECTED TOKENS ---{tokenizer.decode(inputs['input_ids'][k][target_indices[k][0]])}---and---{tokenizer.decode(inputs['input_ids'][k][target_indices[k][1]])}---\")\n",
    "            '''\n",
    "            print(f\"\\nDecoding START target_indices[k]:-----{tokenizer.decode(inputs['input_ids'][k][:target_indices[k]+1])}-----Decoding end\\n\")\n",
    "            print(f\"SELECTED TOKENS ---{tokenizer.decode(inputs['input_ids'][k][target_indices[k]])}------\")\n",
    "            \n",
    "        output_ids = generate_answers(model, inputs, tokenizer)\n",
    "        \n",
    "        for j in range(len(prompts)):\n",
    "            # --- Decode token IDs into text ---\n",
    "            prompt_len = len(inputs[\"input_ids\"][j]) # Length of prompt j\n",
    "            generated_answer_ids = output_ids[j][prompt_len:] # Remove prompt prefix to isolate the generated answer\n",
    "            generated_answer = tokenizer.decode(generated_answer_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "            # --- Compute semantic similarity between model's answer and ground-truth ---    \n",
    "            rouge_l_score = rouge_l_simScore(generated_answer, answers[j])\n",
    "            if rouge_l_score >= 0.5:\n",
    "                is_correct = True\n",
    "                sbert_score = None\n",
    "            else:\n",
    "                sbert_score = sentence_bert_simScore(generated_answer, answers[j])\n",
    "                is_correct = (sbert_score >= 0.4)\n",
    "\n",
    "            # --- Store everything ---\n",
    "            batch_dataset_ids.append(batch[j]['id'])\n",
    "            batch_dataset_original_idx.append(batch[j]['original_index'])\n",
    "            batch_answers.append(generated_answer)\n",
    "            batch_gt_answers.append(answers[j])\n",
    "            batch_is_correct.append(int(is_correct))\n",
    "            batch_rouge_scores.append(rouge_l_score)\n",
    "            batch_sbert_scores.append(sbert_score)\n",
    "\n",
    "        # --- Save progress to pickle after each batch ---\n",
    "        batch_results = {\n",
    "            \"id\": batch_dataset_ids,\n",
    "            \"original_indices\": batch_dataset_original_idx,\n",
    "            \"gen_answers\": batch_answers,\n",
    "            \"ground_truths\": batch_gt_answers,\n",
    "            \"activations\": [selected_token_vecs[i].unsqueeze(0).cpu() for i in range(selected_token_vecs.size(0))],\n",
    "            \"is_correct\": batch_is_correct,\n",
    "            \"sbert_scores\": batch_sbert_scores,\n",
    "            \"rouge_scores\": batch_rouge_scores\n",
    "        }\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompts: [INST] <<SYS>>\n",
      "Just give the answer, without a complete sentence. Reply with 'Impossible to answer' if answer not in context.\n",
      "<<SYS>>\n",
      "\n",
      "Context:\n",
      "Under a front page headline \"The Truth\", the paper printed allegations provided to them that some fans picked the pockets of crushed victims, that others urinated on members of the emergency services as they tried to help and that some even assaulted a police constable \"whilst he was administering the kiss of life to a patient.\" Despite the headline, written by Kelvin MacKenzie, the story was based on allegations either by unnamed and unattributable sources, or hearsay accounts of what named individuals had said – a fact made clear to MacKenzie by Harry Arnold, the reporter who wrote the story.\n",
      "\n",
      "Question:\n",
      "What was the story based on?\n",
      "\n",
      "Answer:\n",
      "[/INST],\n",
      "[INST] <<SYS>>\n",
      "Just give the answer, without a complete sentence. Reply with 'Impossible to answer' if answer not in context.\n",
      "<<SYS>>\n",
      "\n",
      "Context:\n",
      "The term Carnival is traditionally used in areas with a large Catholic presence. However, the Philippines, a predominantly Roman Catholic country, does not celebrate Carnival anymore since the dissolution of the Manila Carnival after 1939, the last carnival in the country. In historically Lutheran countries, the celebration is known as Fastelavn, and in areas with a high concentration of Anglicans and Methodists, pre-Lenten celebrations, along with penitential observances, occur on Shrove Tuesday. In Eastern Orthodox nations, Maslenitsa is celebrated during the last week before Great Lent. In German-speaking Europe and the Netherlands, the Carnival season traditionally opens on 11/11 (often at 11:11 a.m.). This dates back to celebrations before the Advent season or with harvest celebrations of St. Martin's Day.\n",
      "\n",
      "Question:\n",
      "When does Fastelavn occur?\n",
      "\n",
      "Answer:\n",
      "[/INST]\n",
      "inputs: {'input_ids': tensor([[    2,     2,     2,  ..., 29914, 25580, 29962],\n",
      "        [    2,     2,     2,  ..., 29914, 25580, 29962],\n",
      "        [    1,   518, 25580,  ..., 29914, 25580, 29962]], device='cuda:0'), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n",
      "selected_token_vecs tensor([[-0.5283, -1.0078,  0.9492,  ..., -1.1025,  0.5601,  1.0039],\n",
      "        [-0.8530, -1.4004,  0.5322,  ..., -1.0371,  0.3523,  1.2422],\n",
      "        [-0.5718, -1.5938,  1.0820,  ..., -0.5156,  0.2842,  0.6470]],\n",
      "       device='cuda:1', dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.39it/s]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from src.inference.inference_utils import build_prompt\n",
    "from src.inference.activation_utils import compute_token_offsets, get_layer_output\n",
    "start_offset = 0\n",
    "end_offset = 0\n",
    "from functools import partial\n",
    "\n",
    "batch_extract_token_activations_with_generation(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=id_fit_dataset,\n",
    "    batch_size=3,\n",
    "    idx_start_sample= 0,\n",
    "    max_samples= 3,\n",
    "    output_path = \"outputs/all_batch_results.pkl\",\n",
    "    build_prompt_fn=build_prompt,\n",
    "    get_layer_output_fn=get_layer_output,\n",
    "    layer_idx = -1,  \n",
    "    extract_token_activations_fn=partial(extract_token_activations, mode=\"last\"),\n",
    "    start_offset = start_offset,\n",
    "    end_offset = end_offset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify the get layer output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def compute_offset_token_mask(\n",
    "    attention_mask: torch.Tensor,\n",
    "    start_offset: int = 0,\n",
    "    end_offset: int = 0\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Compute valid token spans for each sequence in a batch, based on padding-aware offsets.\n",
    "    For each sequence:\n",
    "    - Identify the first and last non-padding tokens using the attention mask.\n",
    "    - Apply `start_offset` and `end_offset` relative to those positions.\n",
    "    - Return:\n",
    "        1. The start and end indices of the valid span (exclusive end).\n",
    "        2. A boolean mask marking tokens inside the span.\n",
    "\n",
    "    Offsets are defined relative to real (non-padding) tokens:\n",
    "    - `start_offset` (>= 0) shifts the span start forward from the first real token.\n",
    "    - `end_offset` (<= 0) shifts the span end backward from the last real token (inclusive).\n",
    "    Examples:\n",
    "    - `start_offset=0, end_offset=0`: full non-padding span.\n",
    "    - `start_offset=5, end_offset=-3`: skip 5 tokens from start, exclude last 3.\n",
    "            \n",
    "    Raises\n",
    "    ------\n",
    "    ValueError:\n",
    "        If offsets result in empty spans for any sequence.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    attention_mask : torch.Tensor\n",
    "        Shape (batch_size, seq_len). 1 for real tokens, 0 for padding.\n",
    "    start_offset : int\n",
    "        Offset from the first non-padding token (>= 0).\n",
    "    end_offset : int\n",
    "        Offset from the last non-padding token (<= 0).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    span_mask : torch.Tensor\n",
    "        Boolean tensor (batch_size, seq_len). True for selected tokens.\n",
    "    start : torch.Tensor\n",
    "        Start indices of the span for each sequence (shape: [batch_size]).\n",
    "    end : torch.Tensor\n",
    "        End indices (exclusive) of the span for each sequence (shape: [batch_size]).\n",
    "    \"\"\"\n",
    "    # =======================================\n",
    "    # Validate offsets: start_offset must be non-negative, end_offset must be zero or negative.\n",
    "    # =======================================\n",
    "    if start_offset < 0:\n",
    "        raise ValueError(f\"`start_offset` must be non-negative, got: {start_offset}\")\n",
    "    if end_offset > 0:\n",
    "        raise ValueError(f\"`end_offset` must be zero or negative, got: {end_offset}\")\n",
    "\n",
    "    batch_size, seq_len = attention_mask.shape\n",
    "    device = attention_mask.device\n",
    "    # =======================================\n",
    "    # Compute first and last valid token positions (regardless of padding side)\n",
    "    # =======================================\n",
    "    # First non-padding token is at index: number of padding tokens\n",
    "    first_indices = attention_mask.float().argmax(dim=1)  \n",
    "    # Last non-padding token is at the end: compute its index by flipping and computing position from end\n",
    "    flipped_mask = attention_mask.flip(dims=[1])\n",
    "    last_offsets = flipped_mask.float().argmax(dim=1)\n",
    "    last_indices = seq_len - 1 - last_offsets\n",
    "    # Move to device\n",
    "    first_indices = first_indices.to(device)\n",
    "    last_indices = last_indices.to(device)\n",
    "\n",
    "    # =======================================\n",
    "    # Compute start and end indices and clamp to valid range\n",
    "    # =======================================\n",
    "    # Compute target indices using the offsets and convert to integer type\n",
    "    start = (first_indices + start_offset).to(torch.long)\n",
    "    end = (last_indices + 1 + end_offset).to(torch.long)  # +1 for exclusive\n",
    "\n",
    "    # Clamp to valid, non-padding region\n",
    "    start = torch.clamp(start, min=first_indices, max=last_indices + 1)\n",
    "    end = torch.clamp(end, min=start, max=last_indices + 1)\n",
    "\n",
    "    # =======================================\n",
    "    # Validate that the slice is non-empty\n",
    "    # =======================================\n",
    "    empty = (start >= end).nonzero(as_tuple=True)[0]\n",
    "    if len(empty) > 0:\n",
    "        raise ValueError(\n",
    "            f\"Token offsets result in an empty slice for at least one sequence: \"\n",
    "            f\"start_offset={start_offset}, end_offset={end_offset}, \"\n",
    "            f\"indices={[(int(start[i]), int(end[i])) for i in empty]}\"\n",
    "        )\n",
    "\n",
    "    # =======================================\n",
    "    # Build boolean span selection mask\n",
    "    # =======================================\n",
    "    # Create a tensor of positions: shape (1, seq_len), then expand to (batch_size, seq_len)\n",
    "    positions = torch.arange(seq_len, device=device).unsqueeze(0).expand(batch_size, seq_len)\n",
    "    # Build a boolean mask: True where the position is within [target_first_indices, target_last_indices] for each sequence\n",
    "    span_mask = (positions >= start.unsqueeze(1)) & (positions < end.unsqueeze(1)) # Shape (batch_size, seq_len)\n",
    "\n",
    "    return span_mask.int(), start, end\n",
    "\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "def extract_token_activations(\n",
    "    selected_layer: torch.Tensor,\n",
    "    attention_mask: torch.Tensor,\n",
    "    device: torch.device,\n",
    "    mode: Literal[\"average\", \"last\", \"max\", \"first_generated\"] = \"average\",\n",
    "    start_offset: int = 0,\n",
    "    end_offset: int = 0,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Extract token-level activations over a specified token span for each sequence in a batch,\n",
    "    using one of several aggregation modes: \"average\", \"last\", \"max\", or \"first_generated\".\n",
    "\n",
    "    The token span is defined by applying `start_offset` (relative to the first non-padding token)\n",
    "    and `end_offset` (relative to the last non-padding token) to each sequence, as determined by the\n",
    "    `attention_mask`. This allows robust selection of sub-sequences regardless of left, right, \n",
    "    or mixed padding.\n",
    "\n",
    "    Modes\n",
    "    -----\n",
    "    - \"average\": Computes the mean activation vector over the selected token span for each sequence.\n",
    "    - \"max\": Computes the element-wise maximum activation vector over the selected token span.\n",
    "    - \"last\": Selects the activation vector of the last token in the selected span (after offsets and padding).\n",
    "    - \"first_generated\": Selects the activation vector of the first token in the selected span (after offsets and padding).\n",
    "        - For prompt+generation sequences, set `start_offset=prompt_len` to target the first generated token.\n",
    "        - For generation-only sequences, use the default `start_offset=0`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    selected_layer : torch.Tensor\n",
    "        Tensor of shape (batch_size, seq_len, hidden_size) containing model activations for each token.\n",
    "    attention_mask : torch.Tensor\n",
    "        Attention mask of shape (batch_size, seq_len),  1 for real tokens, 0 for padding.\n",
    "    device : torch.device\n",
    "        Device for computation.\n",
    "    mode : str\n",
    "        Aggregation method: \"average\", \"last\", \"max\", or \"first_generated\".\n",
    "    start_offset : int\n",
    "        Offset from the first non-padding token (>=0). \n",
    "        Used to skip initial tokens (e.g. special tokens or prompt).\n",
    "    end_offset : int\n",
    "        Offset from the last non-padding token (<=0). \n",
    "        Used to exclude trailing tokens (e.g. EOS or special tokens).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        Aggregated activations of shape (batch_size, hidden_size), \n",
    "        according to the selected mode.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Handles any padding scheme (left, right, or mixed).\n",
    "    - Offsets are always applied relative to the first/last real (non-padding) token, not absolute positions.\n",
    "    - For \"first_generated\", the function always returns the first token of the computed span, making \n",
    "      it robust to padding and offsets.\n",
    "    - Raises ValueError if the computed span is empty for any sequence (e.g. offsets are out of bounds).\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, _ = selected_layer.shape\n",
    "\n",
    "    # =======================================\n",
    "    # Build mask to select the valid token span \n",
    "    # =======================================\n",
    "    mask, start_indices, end_indices = compute_offset_token_mask(\n",
    "        attention_mask=attention_mask, \n",
    "        start_offset=start_offset, \n",
    "        end_offset=end_offset\n",
    "    ) # Shape (batch_size, seq_len), (batch_size,), (batch_size,)\n",
    "    mask = mask.float().unsqueeze(-1)  # (batch_size, seq_len, 1)\n",
    "\n",
    "    # Move to device \n",
    "    start_indices = start_indices.to(selected_layer.device)\n",
    "    end_indices = end_indices.to(selected_layer.device)\n",
    "    mask = mask.to(selected_layer.device)\n",
    "\n",
    "    # =======================================\n",
    "    # Select the first token with offset\n",
    "    # =======================================\n",
    "    if mode == \"first_generated\":\n",
    "        batch_indices = torch.arange(batch_size, device=device)\n",
    "        first = selected_layer[batch_indices, start_indices] # Shape: (batch_size, hidden_size)\n",
    "        aggregated_tokens = first\n",
    "\n",
    "    # =======================================\n",
    "    # Select the last token with optional offset\n",
    "    # =======================================\n",
    "    elif mode == \"last\":\n",
    "        # Use the last valid token index (end_indices-1, since end is exclusive)\n",
    "        last_indices = (end_indices - 1).clamp(min=0, max=seq_len - 1)\n",
    "        batch_indices = torch.arange(batch_size, device=device)\n",
    "        last = selected_layer[batch_indices, last_indices]  # Shape: (batch_size, hidden_size)\n",
    "        aggregated_tokens = last\n",
    "    \n",
    "    # =======================================\n",
    "    # Apply mask and compute aggregation over the selected span  \n",
    "    # =======================================\n",
    "    elif mode == \"average\":\n",
    "        # Apply the mask to the activations: zero out tokens outside the target interval\n",
    "        masked = selected_layer * mask\n",
    "        #  Count the number of selected tokens for each sequence (avoid division by zero with clamp)\n",
    "        counts = mask.sum(dim=1).clamp(min=1e-6)\n",
    "        #  Compute the mean activation vector for each sequence over the selected interval\n",
    "        avg = masked.sum(dim=1) / counts # Shape: (batch_size, hidden_size)\n",
    "        aggregated_tokens =  avg\n",
    "\n",
    "    elif mode == \"max\":\n",
    "        #  Apply the mask to the activations: zero out tokens outside the target interval\n",
    "        masked = selected_layer * mask.float()\n",
    "        #  Replace padding with -inf to exclude from max calculation\n",
    "        masked = masked.masked_fill(mask.logical_not(), float('-inf'))\n",
    "        #  Extract maximum values across sequence dimension\n",
    "        max, _ = masked.max(dim=1) # Shape: (batch_size, hidden_size)\n",
    "        aggregated_tokens = max\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported mode: {mode}. Choose from 'average', 'last', 'max', or 'first_generated'.\")\n",
    "\n",
    "    return aggregated_tokens, start_indices, end_indices\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_offset_attention_mask(\n",
    "    attention_mask: torch.Tensor,\n",
    "    start_offset: int = 0,\n",
    "    end_offset: int = 0\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Returns a modified attention mask selecting a token span based on padding-aware offsets.\n",
    "\n",
    "    For each sequence in the batch:\n",
    "    - The function identifies the span of real (non-padding) tokens using the `attention_mask`.\n",
    "    - It then shifts the start and end of this real span using:\n",
    "        - `start_offset` >= 0: number of tokens to skip from the beginning of the real tokens.\n",
    "        - `end_offset` <= 0: number of tokens to exclude from the end of the real tokens.\n",
    "    - It outputs a new boolean attention mask of the same shape as `attention_mask`, \n",
    "      marking only the tokens inside the selected subspan as `1`, and everything else \n",
    "      (including original padding) as `0`.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    - If a sequence has real tokens at positions 10 to 50:\n",
    "        - `start_offset=0, end_offset=0` selects tokens 10 to 50 (inclusive start, exclusive end).\n",
    "        - `start_offset=5, end_offset=-3` selects tokens 15 to 47.\n",
    "    - The resulting span always lies within the non-padding region.\n",
    "            \n",
    "    Parameters\n",
    "    ----------\n",
    "    attention_mask : torch.Tensor\n",
    "        Shape (batch_size, seq_len). 1 for real tokens, 0 for padding.\n",
    "    start_offset : int\n",
    "        Offset from the first non-padding token (must be >= 0). \n",
    "    end_offset : int\n",
    "        Offset from the last non-padding token (must be <= 0, e.g., -3 to remove 3 tokens).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    offset_attention_mask : torch.Tensor\n",
    "        Boolean tensor of shape (batch_size, seq_len). \n",
    "        `True` (or `1`) where tokens fall inside the offset-adjusted span.\n",
    "    start : torch.Tensor\n",
    "        Tensor of shape (batch_size,) indicating the inclusive start index of the span for each sequence.\n",
    "    end : torch.Tensor\n",
    "        Tensor of shape (batch_size,) indicating the exclusive end index of the span for each sequence.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If start_offset is negative or end_offset is positive.\n",
    "        If the offsets result in an empty or invalid span for any sequence.\n",
    "    \"\"\"\n",
    "    # =======================================\n",
    "    # Validate offsets: start_offset must be non-negative, end_offset must be zero or negative.\n",
    "    # =======================================\n",
    "    if start_offset < 0:\n",
    "        raise ValueError(f\"`start_offset` must be non-negative, got: {start_offset}\")\n",
    "    if end_offset > 0:\n",
    "        raise ValueError(f\"`end_offset` must be zero or negative, got: {end_offset}\")\n",
    "\n",
    "    batch_size, seq_len = attention_mask.shape\n",
    "    device = attention_mask.device\n",
    "    # =======================================\n",
    "    # Compute first and last valid token positions (regardless of padding side)\n",
    "    # =======================================\n",
    "    # First non-padding token is at index: number of padding tokens\n",
    "    first_indices = attention_mask.float().argmax(dim=1)  \n",
    "    # Last non-padding token is at the end: compute its index by flipping and computing position from end\n",
    "    flipped_mask = attention_mask.flip(dims=[1])\n",
    "    last_offsets = flipped_mask.float().argmax(dim=1)\n",
    "    last_indices = seq_len - 1 - last_offsets\n",
    "    # Move to device\n",
    "    first_indices = first_indices.to(device)\n",
    "    last_indices = last_indices.to(device)\n",
    "\n",
    "    # =======================================\n",
    "    # Compute start and end indices and clamp to valid range\n",
    "    # =======================================\n",
    "    # Compute target indices using the offsets and convert to integer type\n",
    "    start = (first_indices + start_offset).to(torch.long)\n",
    "    end = (last_indices + 1 + end_offset).to(torch.long)  # +1 for exclusive\n",
    "\n",
    "    # Clamp to valid, non-padding region\n",
    "    start = torch.clamp(start, min=first_indices, max=last_indices + 1)\n",
    "    end = torch.clamp(end, min=start, max=last_indices + 1)\n",
    "\n",
    "    # =======================================\n",
    "    # Validate that the slice is non-empty\n",
    "    # =======================================\n",
    "    empty = (start >= end).nonzero(as_tuple=True)[0]\n",
    "    if len(empty) > 0:\n",
    "        raise ValueError(\n",
    "            f\"Token offsets result in an empty slice for at least one sequence: \"\n",
    "            f\"start_offset={start_offset}, end_offset={end_offset}, \"\n",
    "            f\"indices={[(int(start[i]), int(end[i])) for i in empty]}\"\n",
    "        )\n",
    "\n",
    "    # =======================================\n",
    "    # Build boolean span selection mask\n",
    "    # =======================================\n",
    "    # Create a tensor of positions: shape (1, seq_len), then expand to (batch_size, seq_len)\n",
    "    positions = torch.arange(seq_len, device=device).unsqueeze(0).expand(batch_size, seq_len)\n",
    "    # Build a boolean mask: True where the position is within [target_first_indices, target_last_indices] for each sequence\n",
    "    offset_attention_mask = (positions >= start.unsqueeze(1)) & (positions < end.unsqueeze(1)) # Shape (batch_size, seq_len)\n",
    "\n",
    "    return offset_attention_mask.int(), start, end\n",
    "\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "def extract_token_activations(\n",
    "    selected_layer: torch.Tensor,\n",
    "    attention_mask: torch.Tensor,\n",
    "    device: torch.device,\n",
    "    mode: Literal[\"average\", \"last\", \"max\", \"first_generated\"] = \"average\",\n",
    "    skip_length: Optional[int] = None,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"   \n",
    "    Aggregate token-level activations over a specified span for each sequence in a batch,\n",
    "    using attention mask.\n",
    "\n",
    "    This function takes as input:\n",
    "      - The layer activations (selected_layer) for each token in a batch of sequences,\n",
    "      - An attention mask (attention_mask) of the same shape, where 1 indicates tokens to include\n",
    "        in the aggregation and 0 marks tokens to ignore.\n",
    "\n",
    "    The attention mask may be the original model mask, or a custom mask generated using\n",
    "    `compute_offset_attention_mask` to dynamically select a sub-span of tokens.\n",
    "\n",
    "    Modes\n",
    "    -----\n",
    "    - \"average\": Computes the mean activation over all tokens selected by `attention_mask`.\n",
    "    - \"max\": Computes the element-wise maximum activation vector over all tokens selected by `attention_mask`.\n",
    "    - \"last\": Selects the activation of the last token selected by the `attention_mask` (the last 1).\n",
    "    - \"first_generated\": Selects the activation of the first token selected by `attention_mask`,\n",
    "        or at index `skip_length` if provided.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    selected_layer : torch.Tensor\n",
    "        Tensor of shape (batch_size, seq_len, hidden_size) containing model activations for each token.\n",
    "    attention_mask : torch.Tensor\n",
    "        Attention mask of shape (batch_size, seq_len),  1 for real tokens, 0 for padding.\n",
    "    device : torch.device\n",
    "        Device for computation.\n",
    "    mode : str\n",
    "        Aggregation method: \"average\", \"last\", \"max\", or \"first_generated\".\n",
    "    skip_length : Optional[int]\n",
    "        If provided, used to explicitly select the first generated token (useful for \"first_generated\" mode).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        Aggregated activations of shape (batch_size, hidden_size), \n",
    "        according to the selected mode.\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, _ = selected_layer.shape\n",
    "\n",
    "    # Move to device \n",
    "    attention_mask = attention_mask.to(selected_layer.device)\n",
    "\n",
    "    # =======================================\n",
    "    # Select the first token with offset\n",
    "    # =======================================\n",
    "    if mode == \"first_generated\":\n",
    "        batch_indices = torch.arange(batch_size, device=device)\n",
    "        if skip_length is not None:\n",
    "            first_indices = torch.full((batch_size,), skip_length, device=device, dtype=torch.long)\n",
    "        else:\n",
    "            first_indices = (attention_mask == 1).float().argmax(dim=1)\n",
    "        print(\"--- Function extract_token_activations ==> first_indices: \", first_indices)\n",
    "        first = selected_layer[batch_indices, first_indices] # Shape: (batch_size, hidden_size)\n",
    "        aggregated_tokens = first\n",
    "\n",
    "    # =======================================\n",
    "    # Select the last token with optional offset\n",
    "    # =======================================\n",
    "    elif mode == \"last\":\n",
    "        last_indices = attention_mask.shape[1] - 1 - attention_mask.flip(dims=[1]).float().argmax(dim=1)\n",
    "        batch_indices = torch.arange(batch_size, device=device)\n",
    "        last = selected_layer[batch_indices, last_indices]  # Shape: (batch_size, hidden_size)\n",
    "        print(\"--- Function extract_token_activations ==> last_indices: \", last_indices)\n",
    "        aggregated_tokens = last\n",
    "    \n",
    "    # =======================================\n",
    "    # Apply mask and compute aggregation over the selected span  \n",
    "    # =======================================\n",
    "    elif mode == \"average\":\n",
    "        # Add one dimension for the broadcast on hidden_size\n",
    "        attention_mask = attention_mask.float().unsqueeze(-1)  # (batch_size, seq_len, 1)\n",
    "        # Apply the mask to the activations: zero out tokens outside the target interval\n",
    "        masked = selected_layer * attention_mask\n",
    "        #  Count the number of selected tokens for each sequence (avoid division by zero with clamp)\n",
    "        counts = attention_mask.sum(dim=1).clamp(min=1e-6)\n",
    "        #  Compute the mean activation vector for each sequence over the selected interval\n",
    "        avg = masked.sum(dim=1) / counts # Shape: (batch_size, hidden_size)\n",
    "        aggregated_tokens =  avg\n",
    "\n",
    "    elif mode == \"max\":\n",
    "        # Add one dimension for the broadcast on hidden_size\n",
    "        attention_mask = attention_mask.float().unsqueeze(-1)  # (batch_size, seq_len, 1)\n",
    "        #  Apply the mask to the activations: zero out tokens outside the target interval\n",
    "        masked = selected_layer * attention_mask.float()\n",
    "        #  Replace padding with -inf to exclude from max calculation\n",
    "        masked = masked.masked_fill(attention_mask.logical_not(), float('-inf'))\n",
    "        #  Extract maximum values across sequence dimension\n",
    "        max, _ = masked.max(dim=1) # Shape: (batch_size, hidden_size)\n",
    "        aggregated_tokens = max\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported mode: {mode}. Choose from 'average', 'last', 'max', or 'first_generated'.\")\n",
    "\n",
    "    return aggregated_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizer, PreTrainedModel, BatchEncoding\n",
    "import torch\n",
    "from datasets import  Dataset\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Any, Callable, Tuple, Literal\n",
    "from src.evaluation.similarity_metrics import rouge_l_simScore, sentence_bert_simScore\n",
    "from src.data_reader.pickle_io import save_batch_pickle\n",
    "from torch.utils.hooks import RemovableHandle\n",
    "from src.inference.inference_utils import build_prompt, extract_batch\n",
    "from src.inference.activation_utils import compute_token_offsets, extract_token_activations\n",
    "from functools import partial\n",
    "\n",
    "def register_forward_activation_hook(\n",
    "    model: PreTrainedModel,\n",
    "    captured_hidden: dict,\n",
    "    layer_idx: int = -1\n",
    ") -> RemovableHandle:\n",
    "    \"\"\"\n",
    "    Attaches a forward hook to a specific transformer layer to capture hidden states \n",
    "    during a single forward pass (more memory-efficient than using output_hidden_states=True).\n",
    "    Transformer layer = self-attention + FFN + normalization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : PreTrainedModel\n",
    "        The Hugging Face causal language model (e.g., GPT, LLaMA).\n",
    "    captured_hidden : dict\n",
    "        Dictionary used to store the hidden states from the forward pass (will be overwritten).\n",
    "        captured_hidden[\"activations\"] of shape (batch_size, seq_len, hidden_size).\n",
    "    layer_idx : int\n",
    "        Index of the transformer block to hook. Defaults to -1 (the last layer).\n",
    "        Use a positive integer if you want to hook an intermediate layer instead.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    RemovableHandle : A handle object\n",
    "        Call `handle.remove()` after generation to remove the hook.\n",
    "    call_counter : int \n",
    "        Stores the number of times the hook is activated.\n",
    "    \"\"\"\n",
    "    # Raise error if layer_idx not in correct range\n",
    "    num_layers = len(model.model.layers)\n",
    "    if not (layer_idx == -1 or 0 <= layer_idx < num_layers):\n",
    "        raise ValueError(\n",
    "            f\"`layer_idx` must be -1 or in [0, {num_layers - 1}], but got {layer_idx}.\"\n",
    "        )\n",
    "    \n",
    "    call_counter = {\"count\": 0} # count how many times the hook is triggered\n",
    "    \n",
    "    def hook_fn(module, input, output):\n",
    "        \"\"\"Function called automatically by PyTorch just after\n",
    "        the layer has produced its output during the forward pass.\"\"\"\n",
    "        \n",
    "        call_counter[\"count\"] += 1 \n",
    "        \n",
    "        # output is a tuple (hidden_states,) → keep [0]\n",
    "        if layer_idx == -1:\n",
    "            captured_hidden[\"activations\"] = model.model.norm(output[0])  # post RMSNorm!\n",
    "        else:\n",
    "            captured_hidden[\"activations\"] = output[0]\n",
    "\n",
    "    # Register hook on the transformer block\n",
    "    # When Pytorch pass through this layer during a forward pass, it also execute hook_fn.\n",
    "    handle = model.model.layers[layer_idx].register_forward_hook(hook_fn)\n",
    "\n",
    "    return handle, call_counter\n",
    "\n",
    "\n",
    "# =====================================\n",
    "from typing import Union\n",
    "def run_prompt_activation_extraction(\n",
    "    model: PreTrainedModel,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    dataset: Dataset,\n",
    "    batch_size: int = 4,\n",
    "    idx_start_sample: int = 0,\n",
    "    max_samples: int = 1000,\n",
    "    save_to_pkl: bool = False,\n",
    "    output_path: str = \"outputs/all_batch_results.pkl\",\n",
    "    build_prompt_fn: Callable[[str, str], str] = None,\n",
    "    register_forward_activation_hook_fn: Callable = None,\n",
    "    layer_idx: int = -1,  \n",
    "    extract_token_activations_fn: Callable = None,\n",
    "    start_offset: int = 0,\n",
    "    end_offset: int = 0,\n",
    ") -> Union [Tuple[List[torch.Tensor]], None]:\n",
    "    \"\"\"\n",
    "    Runs batched inference on a dataset using a decoder-only language model.\n",
    "    For each batch, extracts token-level activations, and appends the results to a pickle file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : PreTrainedModel\n",
    "        The causal language model to evaluate (e.g., LLaMA).\n",
    "    tokenizer : PreTrainedTokenizer\n",
    "        The corresponding tokenizer.\n",
    "    dataset : Dataset\n",
    "        The input dataset.\n",
    "    batch_size : int\n",
    "        Number of samples per batch.\n",
    "    idx_start_sample : int\n",
    "        Index of the first sample to process from the dataset.\n",
    "    max_samples : int\n",
    "        Total number of examples to process from the dataset, starting from idx_start_sample. \n",
    "    save_to_pkl : bool\n",
    "        If True, activations are appended to the pickle file at output_path.\n",
    "        If False, the function returns a list of activations.\n",
    "    output_path : str\n",
    "        Path to the pickle file for saving intermediate results.\n",
    "    build_prompt_fn : Callable\n",
    "        Function to build a prompt from context and question.\n",
    "    register_forward_activation_hook_fn : Callable\n",
    "        Function that registers a forward hook on the model during a forward pass. \n",
    "    layer_idx : int\n",
    "        Index of the transformer layer to extract activations from (default: -1 for last layer).\n",
    "    extract_token_activations_fn : Callable\n",
    "        Function to extract token activations from a model layer.\n",
    "    start_offset : int\n",
    "        Offset from the first non-padding token (must be >= 0). \n",
    "    end_offset : int\n",
    "        Offset from the last non-padding token (must be <= 0, e.g., -3 to remove 3 tokens).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Union[\n",
    "        Tuple[List[torch.Tensor],\n",
    "        None\n",
    "    ]\n",
    "        If save_to_pkl is False:\n",
    "            Returns one list:\n",
    "                - batch_prompt_activations: list of length `num_samples`, each element is a tensor \n",
    "                of shape (prompt_len, hidden_size), containing the selected prompt token activations.\n",
    "        If save_to_pkl is True:\n",
    "            Returns None (activations are saved incrementally to output_path).\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_prompt_activations = []  # Chosen prompt token activation vectors\n",
    "\n",
    "    for i in tqdm(range(idx_start_sample, idx_start_sample + max_samples, batch_size)):\n",
    "        \n",
    "        # ==============================\n",
    "        # Prepare input batch\n",
    "        # ==============================\n",
    "        batch = extract_batch(dataset, i, batch_size)\n",
    "        prompts = [build_prompt_fn(s[\"context\"], s[\"question\"]) for s in batch]\n",
    "        inputs = tokenizer(prompts, padding=True, truncation=True, return_tensors=\"pt\").to(model.device)\n",
    "        prompt_len = inputs[\"input_ids\"].shape[1] # Assumes prompts padded to same length\n",
    "        prompt_attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "        print(\"inputs['input_ids'].shape\", inputs['input_ids'].shape)\n",
    "        print(inputs)\n",
    "\n",
    "        # ==============================\n",
    "        # Register forward hook to capture layer output\n",
    "        # ==============================\n",
    "        # Hook to collect the hidden states after the forward pass\n",
    "        captured_hidden = {}\n",
    "        handle, call_counter = register_forward_activation_hook_fn(model, captured_hidden, layer_idx=layer_idx)\n",
    "        \n",
    "        # ==============================\n",
    "        # Run model forward pass (hook captures activations)\n",
    "        # ==============================\n",
    "        # Pass inputs through the model. When the target layer is reached,\n",
    "        # the hook executes and saves its output in captured_hidden.\n",
    "        with torch.no_grad():\n",
    "            _ = model(**inputs, return_dict=True)\n",
    "        # Remove the hook to avoid memory leaks or duplicate logging\n",
    "        handle.remove() \n",
    "        \n",
    "        #print(f\"Hook was called {call_counter['count']} times.\")\n",
    "        if \"activations\" not in captured_hidden:\n",
    "            raise RuntimeError(\"Hook failed to capture activations.\")\n",
    "\n",
    "        layer_output = captured_hidden[\"activations\"]  # Shape: (batch_size, seq_len, hidden_size)\n",
    "        print(\"layer_output : \" , layer_output.shape)\n",
    "\n",
    "        # ===============================\n",
    "        # Modify prompt attention mask with offsets\n",
    "        # ===============================\n",
    "        print(\"=======================================\")\n",
    "        if start_offset !=0 or end_offset !=0:\n",
    "            print(\"prompt_attention_mask before OFFSET:\", prompt_attention_mask)\n",
    "            print(\"prompt_attention_mask.shape before OFFSET:\", prompt_attention_mask.shape)\n",
    "            prompt_attention_mask, start_indices, end_indices = compute_offset_attention_mask(\n",
    "                attention_mask=prompt_attention_mask, \n",
    "                start_offset=start_offset, \n",
    "                end_offset=end_offset\n",
    "            ) # Shape (batch_size, seq_len), (batch_size,), (batch_size,)\n",
    "            print(\"start_indices:\", start_indices)\n",
    "            print(\"end_indices:\", end_indices)\n",
    "            print(\"prompt_attention_mask after OFFSET:\", prompt_attention_mask)\n",
    "            print(\"prompt_attention_mask.shape after OFFSET:\", prompt_attention_mask.shape)\n",
    "            print(\"=======================================\")\n",
    "            for i, seq in enumerate(inputs[\"input_ids\"]):\n",
    "                print(\"=============inputs before offsets:=============\")\n",
    "                print(tokenizer.decode(seq, skip_special_tokens=True))\n",
    "                print(\"=============inputs after offsets:=============\")\n",
    "                start_idx, end_idx = start_indices[i], end_indices[i]\n",
    "                seq_trunc = seq[start_idx:end_idx]\n",
    "                print(tokenizer.decode(seq_trunc, skip_special_tokens=True))\n",
    "                print(f\"First selected character: ---{tokenizer.decode(seq_trunc[0])}---\")\n",
    "                print(f\"Last selected character: ---{tokenizer.decode(seq_trunc[-1])}---\")      \n",
    "        \n",
    "\n",
    "        # ==============================\n",
    "        # Extract token activations from captured layer\n",
    "        # ==============================\n",
    "        selected_token_vecs = extract_token_activations_fn(\n",
    "                selected_layer=layer_output , \n",
    "                attention_mask=prompt_attention_mask, \n",
    "                device=layer_output.device\n",
    "                ) # Shape (batch_size, hidden_size)\n",
    "        \n",
    "        print(\"=======================================\")\n",
    "        print(\"Length of layer_output:\", len(layer_output))\n",
    "        print(\"shape of layer_output:\", layer_output.shape)\n",
    "        print(\"Shape of layer_output[0]:\", layer_output[0].shape)\n",
    "        print(\"Shape of layer_output[1]:\", layer_output[1].shape)\n",
    "        print(\"Shape of layer_output[-1]:\", layer_output[-1].shape)\n",
    "        print(\"=======================================\")\n",
    "\n",
    "        # ==============================\n",
    "        # Store results (to file or memory)\n",
    "        # ==============================\n",
    "        activations = [selected_token_vecs[j].unsqueeze(0).cpu() for j in range(selected_token_vecs.size(0))]\n",
    "        batch_dataset_ids = [s['id'] for s in batch]\n",
    "        batch_dataset_original_idx = [s['original_index'] for s in batch]\n",
    "        \n",
    "        batch_results = {\n",
    "            \"id\": batch_dataset_ids,\n",
    "            \"original_indices\": batch_dataset_original_idx,\n",
    "            \"activations\": activations \n",
    "        }\n",
    "\n",
    "        if save_to_pkl:\n",
    "            save_batch_pickle(batch_data=batch_results, output_dir=output_path, batch_idx=i)\n",
    "        else:\n",
    "            batch_prompt_activations.extend(activations)\n",
    "        \n",
    "    if not save_to_pkl:\n",
    "        return batch_prompt_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs['input_ids'].shape torch.Size([2, 279])\n",
      "{'input_ids': tensor([[    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     1,   518, 25580,\n",
      "         29962,  3532, 14816, 29903,  6778,    13, 29954,  5428,   278,  1494,\n",
      "         13382,   322,  1139, 29892,  1234,   278,  1139,   491,   871,  6820,\n",
      "           278,  1234,  1728,   263,  4866, 10541, 29889,    13,  3644,   372,\n",
      "          2609,   367,  7699,  2729,   373,   278, 13382, 29892,  8908,   525,\n",
      "           348, 12011,   519,  2396,    13, 29966,   829, 14816, 29903,  6778,\n",
      "            13,    13,  7129,   482, 29901,  7634,   263,  4565,  1813,  2343,\n",
      "          1220,   376,  1576,  1605,  2806,   613,   278,  5650, 13350, 16831,\n",
      "           800,  4944,   304,   963,   393,   777, 24909, 18691,   278,   772,\n",
      "          9737,   310,  2181, 15392,  6879,  9893, 29892,   393,  4045,  5065,\n",
      "           262,   630,   373,  5144,   310,   278, 11176, 14703,  5786,   408,\n",
      "           896,  1898,   304,  1371,   322,   393,   777,  1584, 29159,   287,\n",
      "           263, 10974,  1040,   519,   376,  1332, 16613,   540,   471,  4113,\n",
      "          1531,   292,   278, 20057,   310,  2834,   304,   263, 16500,  1213,\n",
      "         19454,   278,  2343,  1220, 29892,  3971,   491, 27326,  3845,  4326,\n",
      "         29968,   264,  3914, 29892,   278,  5828,   471,  2729,   373, 16831,\n",
      "           800,  2845,   491,   443, 17514,   322,   443,  1131,  1091,  9246,\n",
      "          8974, 29892,   470,   540,  1503,   388, 15303,   310,   825,  4257,\n",
      "         15724,   750,  1497,   785,   263,  2114,  1754,  2821,   304,  4326,\n",
      "         29968,   264,  3914,   491, 10686, 20720, 29892,   278,  1634,  9555,\n",
      "          1058,  5456,   278,  5828, 29889,    13, 16492, 29901,  1724,   471,\n",
      "           278,  5828,  2729,   373, 29973,   518, 29914, 25580, 29962],\n",
      "        [    1,   518, 25580, 29962,  3532, 14816, 29903,  6778,    13, 29954,\n",
      "          5428,   278,  1494, 13382,   322,  1139, 29892,  1234,   278,  1139,\n",
      "           491,   871,  6820,   278,  1234,  1728,   263,  4866, 10541, 29889,\n",
      "            13,  3644,   372,  2609,   367,  7699,  2729,   373,   278, 13382,\n",
      "         29892,  8908,   525,   348, 12011,   519,  2396,    13, 29966,   829,\n",
      "         14816, 29903,  6778,    13,    13,  7129,   482, 29901,   450,  1840,\n",
      "          1704, 29876,  2561,   338, 11399,   635,  1304,   297, 10161,   411,\n",
      "           263,  2919, 11865, 10122, 29889,  2398, 29892,   278, 26260, 29892,\n",
      "           263,   758, 24130, 10835,  5917, 11865,  4234, 29892,   947,   451,\n",
      "         10894,   403,  1704, 29876,  2561, 15128,  1951,   278, 23556,   918,\n",
      "           310,   278,  2315,  4233,  1704, 29876,  2561,  1156, 29871, 29896,\n",
      "         29929, 29941, 29929, 29892,   278,  1833,  1559, 29876,  2561,   297,\n",
      "           278,  4234, 29889,   512,  3603,  1711, 24760,   273, 10916, 29892,\n",
      "           278, 10894,   362,   338,  2998,   408, 23786,   295,   485, 29876,\n",
      "         29892,   322,   297, 10161,   411,   263,  1880, 26702,   310,  3218,\n",
      "           506,   550,   322,  8108,  2879, 29892,   758, 29899, 29931, 19889,\n",
      "         10894,   800, 29892,  3412,   411,  6584,   277,  2556,  5820,  2925,\n",
      "         29892,  6403,   373,  1383,   307,   345,   323,  1041,  3250, 29889,\n",
      "           512, 16162, 23757, 17324, 19079, 29892,  8622,  2435,  1169, 29874,\n",
      "           338, 26301,  2645,   278,  1833,  4723,  1434,  7027,   365,   296,\n",
      "         29889,   512,  5332, 29899,  5965,  5086,  4092,   322,   278, 24553,\n",
      "         29892,   278,  1704, 29876,  2561,  4259, 11399,   635, 13246,   373,\n",
      "         29871, 29896, 29896, 29914, 29896, 29896,   313, 29877, 15535,   472,\n",
      "         29871, 29896, 29896, 29901, 29896, 29896,   263, 29889, 29885,  6250,\n",
      "           910, 10116,  1250,   304, 10894,   800,  1434,   278, 21255,  4259,\n",
      "           470,   411,  4023, 10147, 10894,   800,   310,   624, 29889,  6502,\n",
      "         29915, 29879,  8373, 29889,    13, 16492, 29901,  1932,   947, 23786,\n",
      "           295,   485, 29876,  6403, 29973,   518, 29914, 25580, 29962]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: CausalLMOutputWithPast(loss={'logits': tensor([[[-3.4980,  0.3792,  7.9180,  ..., -3.3477, -2.2324, -1.1318],\n",
      "         [-3.5410,  0.2820,  7.7617,  ..., -3.3613, -2.2402, -1.1875],\n",
      "         [-3.4980,  0.2712,  7.6602,  ..., -3.3340, -2.1855, -1.2949],\n",
      "         ...,\n",
      "         [ 1.8496, -0.1842,  1.5684,  ...,  0.5571,  0.6968, -0.6689],\n",
      "         [-5.5195, -5.2148,  8.2656,  ..., -4.2188, -1.1328, -0.9873],\n",
      "         [-3.9277, -2.8691,  6.2539,  ...,  1.0898, -2.5762,  0.2416]],\n",
      "\n",
      "        [[ 0.1238, -0.1808,  0.3096,  ...,  1.3418,  1.8965,  0.6592],\n",
      "         [-6.5078, -2.0664,  1.4424,  ..., -4.5352, -5.9062, -2.6426],\n",
      "         [-3.8613, -4.3320, -2.1016,  ..., -4.0508, -4.6992, -0.8018],\n",
      "         ...,\n",
      "         [ 1.8301, -0.9966, -1.2441,  ..., -0.1854,  1.2549,  0.1678],\n",
      "         [-6.3477, -4.8320,  7.8320,  ..., -1.6904, -3.6055, -1.7197],\n",
      "         [-4.3203, -2.9395,  7.1289,  ...,  0.9219, -2.2266, -0.4097]]],\n",
      "       device='cuda:0', dtype=torch.float16), 'past_key_values': <transformers.cache_utils.DynamicCache object at 0x7a35f0f0aa10>}, logits=tensor([[[-3.4980,  0.3792,  7.9180,  ..., -3.3477, -2.2324, -1.1318],\n",
      "         [-3.5410,  0.2820,  7.7617,  ..., -3.3613, -2.2402, -1.1875],\n",
      "         [-3.4980,  0.2712,  7.6602,  ..., -3.3340, -2.1855, -1.2949],\n",
      "         ...,\n",
      "         [ 1.8496, -0.1842,  1.5684,  ...,  0.5571,  0.6968, -0.6689],\n",
      "         [-5.5195, -5.2148,  8.2656,  ..., -4.2188, -1.1328, -0.9873],\n",
      "         [-3.9277, -2.8691,  6.2539,  ...,  1.0898, -2.5762,  0.2416]],\n",
      "\n",
      "        [[ 0.1238, -0.1808,  0.3096,  ...,  1.3418,  1.8965,  0.6592],\n",
      "         [-6.5078, -2.0664,  1.4424,  ..., -4.5352, -5.9062, -2.6426],\n",
      "         [-3.8613, -4.3320, -2.1016,  ..., -4.0508, -4.6992, -0.8018],\n",
      "         ...,\n",
      "         [ 1.8301, -0.9966, -1.2441,  ..., -0.1854,  1.2549,  0.1678],\n",
      "         [-6.3477, -4.8320,  7.8320,  ..., -1.6904, -3.6055, -1.7197],\n",
      "         [-4.3203, -2.9395,  7.1289,  ...,  0.9219, -2.2266, -0.4097]]],\n",
      "       device='cuda:0', dtype=torch.float16), past_key_values=<transformers.cache_utils.DynamicCache object at 0x7a35f0f0aa10>, hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m start_offset=\u001b[32m40\u001b[39m\n\u001b[32m      2\u001b[39m end_offset=-\u001b[32m4\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mrun_prompt_activation_extraction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mid_fit_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43midx_start_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_to_pkl\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moutputs/all_batch_results.pkl\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbuild_prompt_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbuild_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mregister_forward_activation_hook_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mregister_forward_activation_hook\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextract_token_activations_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextract_token_activations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfirst_generated\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstart_offset\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mend_offset\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_offset\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 178\u001b[39m, in \u001b[36mrun_prompt_activation_extraction\u001b[39m\u001b[34m(model, tokenizer, dataset, batch_size, idx_start_sample, max_samples, save_to_pkl, output_path, build_prompt_fn, register_forward_activation_hook_fn, layer_idx, extract_token_activations_fn, start_offset, end_offset)\u001b[39m\n\u001b[32m    174\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mHook failed to capture activations.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    176\u001b[39m \u001b[38;5;66;03m# Retrieve text of generated answers\u001b[39;00m\n\u001b[32m    177\u001b[39m gen_answers = tokenizer.batch_decode(\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m     \u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_len\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m, \n\u001b[32m    179\u001b[39m     skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    180\u001b[39m ) \u001b[38;5;66;03m# Shape: [batch_size,]\u001b[39;00m\n\u001b[32m    182\u001b[39m layer_output = captured_hidden[\u001b[33m\"\u001b[39m\u001b[33mactivations\u001b[39m\u001b[33m\"\u001b[39m]  \u001b[38;5;66;03m# Shape: (batch_size, seq_len, hidden_size)\u001b[39;00m\n\u001b[32m    183\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mlayer_output : \u001b[39m\u001b[33m\"\u001b[39m , layer_output.shape)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.env/ood_env/lib/python3.11/site-packages/transformers/utils/generic.py:447\u001b[39m, in \u001b[36mModelOutput.__getitem__\u001b[39m\u001b[34m(self, k)\u001b[39m\n\u001b[32m    445\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_dict[k]\n\u001b[32m    446\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m447\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mto_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: tuple indices must be integers or slices, not tuple"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "start_offset=40\n",
    "end_offset=-4\n",
    "\n",
    "run_prompt_activation_extraction(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=id_fit_dataset,\n",
    "    batch_size=2,\n",
    "    idx_start_sample= 0,\n",
    "    max_samples= 2,\n",
    "    save_to_pkl = False,\n",
    "    output_path = \"outputs/all_batch_results.pkl\",\n",
    "    build_prompt_fn=build_prompt,\n",
    "    register_forward_activation_hook_fn=register_forward_activation_hook,\n",
    "    layer_idx = -1,  \n",
    "    extract_token_activations_fn=partial(extract_token_activations, mode=\"first_generated\"),\n",
    "    start_offset = start_offset,\n",
    "    end_offset = end_offset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redo properly the token extraction of the answer without teacher forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizer, PreTrainedModel, BatchEncoding\n",
    "import torch\n",
    "from datasets import  Dataset\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Any, Callable, Tuple, Literal, Union\n",
    "from src.evaluation.similarity_metrics import rouge_l_simScore, sentence_bert_simScore\n",
    "from src.data_reader.pickle_io import save_batch_pickle\n",
    "from torch.utils.hooks import RemovableHandle\n",
    "from src.inference.inference_utils import build_prompt, extract_batch\n",
    "from src.inference.activation_utils import compute_token_offsets, extract_token_activations, compute_offset_attention_mask\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def register_generation_activation_hook(\n",
    "    model: PreTrainedModel,\n",
    "    captured_hidden_list: List[torch.Tensor],\n",
    "    layer_idx: int = -1\n",
    ") -> RemovableHandle:\n",
    "    \"\"\"\n",
    "    Attaches a forward hook to a specific transformer layer to capture hidden states\n",
    "    during autoregressive text generation i.e., at each decoding step.\n",
    "    (more memory-efficient than using output_hidden_states=True).\n",
    "    Transformer layer = self-attention + FFN + normalization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : PreTrainedModel\n",
    "        The Hugging Face causal language model (e.g., GPT, LLaMA).\n",
    "    captured_hidden_list : List[torch.Tensor]\n",
    "        A list that will be filled with hidden states for each generation step. \n",
    "        Each tensor has shape (batch_size * num_beams, seq_len, hidden_size).\n",
    "    layer_idx : int\n",
    "        Index of the transformer block to hook. Defaults to -1 (the last layer).\n",
    "        Use a positive integer if you want to hook an intermediate layer instead.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    RemovableHandle : A handle object\n",
    "        Call `handle.remove()` after generation to remove the hook.\n",
    "    call_counter : int \n",
    "        Stores the number of times the hook is activated.\n",
    "    \"\"\"\n",
    "    # Raise error if layer_idx not in correct range\n",
    "    num_layers = len(model.model.layers)\n",
    "    if not (layer_idx == -1 or 0 <= layer_idx < num_layers):\n",
    "        raise ValueError(\n",
    "            f\"`layer_idx` must be -1 or in [0, {num_layers - 1}], but got {layer_idx}.\"\n",
    "        )\n",
    "    \n",
    "    call_counter = {\"count\": 0} # count how many times the hook is triggered\n",
    "\n",
    "    def hook_fn(module, input, output):\n",
    "        \"\"\"Function called automatically by PyTorch just after\n",
    "            the layer has produced its output during the forward pass.\"\"\"\n",
    "        \n",
    "        call_counter[\"count\"] += 1 \n",
    "\n",
    "        # output is a tuple (hidden_states,) → keep [0]\n",
    "        if layer_idx == -1:\n",
    "            # Capture the final normalized output \n",
    "            captured_hidden_list.append(model.model.norm(output[0]).detach().cpu())  # post RMSNorm!\n",
    "        else:\n",
    "            # Capture raw hidden states before layer normalization\n",
    "            captured_hidden_list.append(output[0].detach().cpu()) \n",
    "    \n",
    "    # Register hook on the transformer block\n",
    "    # When Pytorch pass through this layer during forward pass, it also execute hook_fn.\n",
    "    handle = model.model.layers[layer_idx].register_forward_hook(hook_fn)\n",
    "    \n",
    "    return handle, call_counter\n",
    "\n",
    "\n",
    "def generate(\n",
    "    model: PreTrainedModel,\n",
    "    inputs: BatchEncoding,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    max_new_tokens: int = 50,\n",
    "    k_beams: int = 1,\n",
    "    **generate_kwargs\n",
    ") -> Union[torch.Tensor, Dict[str, torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Generate sequences from the model with optional beam search.\n",
    "    Supports advanced options via **generate_kwargs (e.g., output_attentions).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : PreTrainedModel\n",
    "        The language model to use for generation.\n",
    "    inputs : BatchEncoding\n",
    "        Tokenized input prompts.\n",
    "    tokenizer : PreTrainedTokenizer\n",
    "        Tokenizer providing eos and pad token IDs.\n",
    "    max_new_tokens : int, optional\n",
    "        Maximum number of new tokens to generate.\n",
    "    k_beams : int, optional\n",
    "        Number of beams to use. If 1, uses sampling. If >1, beam search is enabled.\n",
    "    **generate_kwargs : dict\n",
    "        Additional keyword arguments passed to `model.generate()`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Union[torch.Tensor, Dict[str, torch.Tensor]]\n",
    "        - If k_beams == 1:\n",
    "            Returns a tensor of generated token IDs: shape (batch_size, prompt_len + gen_len)\n",
    "        - If k_beams > 1:\n",
    "            Returns a dictionary with keys:\n",
    "                - \"sequences\": the generated token IDs\n",
    "                - \"beam_indices\": the beam path for each token\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True    if k_beams == 1 else False,\n",
    "            temperature=0.6   if k_beams == 1 else None,\n",
    "            top_p=0.9         if k_beams == 1 else None,\n",
    "            top_k=50          if k_beams == 1 else None,\n",
    "            num_beams=k_beams,\n",
    "            use_cache=True, \n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.eos_token_id, # Ensures clean padding (right padding using eos token)\n",
    "            output_hidden_states=True,      # We rely on the hook to extract hidden states instead (more memory efficient)\n",
    "            return_dict_in_generate=True,    # Needed for access to beam_indices when num_beams > 1\n",
    "            early_stopping=False if k_beams == 1 else True, #Generation stops as soon as any sequence hits EOS, even if other candidates have not yet finished.\n",
    "            **generate_kwargs                # For future flexibility (e.g., output_attentions, output_scores)\n",
    "        )\n",
    "        return outputs \n",
    "\n",
    "\n",
    "def build_generation_attention_mask(\n",
    "    gen_ids: torch.Tensor,\n",
    "    eos_token_id: int\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Build an attention mask for the generated part of sequences, marking all tokens up to and \n",
    "    including the first EOS token as valid (True), and the rest as padding (False).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gen_ids : torch.Tensor\n",
    "        Tensor of shape (batch_size, gen_len) containing only generated sequences.\n",
    "    eos_token_id : int\n",
    "        ID of the EOS token used for padding and stopping generation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        Boolean tensor of shape (batch_size, gen_len), where True marks valid generated tokens.\n",
    "    \"\"\"\n",
    "    batch_size, _ = gen_ids.shape\n",
    "\n",
    "    # Extract only the generated tokens IDs (excluding the prompt part)\n",
    "    gen_len = gen_ids.shape[1]\n",
    "    print(\"--- Function build_generation_attention_mask => gen_len:\", gen_len)\n",
    "    print(\"--- Function build_generation_attention_mask => gen_ids.shape:\", gen_ids.shape)\n",
    "\n",
    "    # Create a boolean mask with True values where tokens equal to eos_token_id\n",
    "    eos_mask = (gen_ids == eos_token_id) # Shape: (batch_size, gen_len)\n",
    "    \n",
    "    # Default eos position = gen_len (means: no eos -> whole sequence is valid)\n",
    "    eos_positions = torch.full((batch_size,), gen_len, dtype=torch.long, device=gen_ids.device)\n",
    "\n",
    "    # Find first eos position for sequences that have one\n",
    "    any_eos = eos_mask.any(dim=1)  # Find which sequences actually contain at least one eos_token_id - Shape: (batch_size,)\n",
    "    eos_positions[any_eos] = eos_mask[any_eos].float().argmax(dim=1) # argmax returns the 1st position where eos_token_id == True\n",
    "\n",
    "    # Generate a position index tensor (e.g., [0, 1, ..., gen_len-1]), repeated for each batch item\n",
    "    position_ids = torch.arange(gen_len, device=gen_ids.device).unsqueeze(0).expand(batch_size, -1) # Shape: (batch_size, gen_len)\n",
    "\n",
    "    # Final generation attetion mask: True for all positions <= first eos (included)\n",
    "    generation_attention_mask = position_ids <= eos_positions.unsqueeze(1) # Shape (batch_size, gen_len)\n",
    "\n",
    "    return generation_attention_mask.int()\n",
    "\n",
    "\n",
    "def align_generation_hidden_states(\n",
    "    generation_activations: List[torch.Tensor],\n",
    "    beam_indices: torch.Tensor = None,\n",
    "    k_beams: int = 1\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    If k_beams > 1, aligns (extracts) the hidden states from `activations` that \n",
    "    correspond to the generated sequence selected by the beam search algorithm. \n",
    "    If k_beams == 1, returns stacked outputs for greedy/top-k decoding.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    generation_activations : List[torch.Tensor]\n",
    "        List of activation tensors per generation step. \n",
    "        activations = [prompt] + [gen_step_1, gen_step_2, ..., gen_step_49], if max_new_tokens=50.\n",
    "        so generation_activations = [gen_step_1, gen_step_2, ..., gen_step_49] (without prompt),\n",
    "        Each tensor has shape (batch_size * k_beams, seq_len, hidden_size) if k_beams > 1,\n",
    "        or (batch_size, seq_len, hidden_size) if k_beams == 1.\n",
    "\n",
    "    beam_indices : torch.Tensor\n",
    "        Tensor of shape (batch_size, gen_len) indicating which beam was selected at each step.\n",
    "        Need to be specified only if k_beams > 1. \n",
    "\n",
    "    k_beams : int\n",
    "        Number of beams used during generation (1 = greedy/top-k, >1 = beam search).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        Aligned hidden states of shape (batch_size, gen_len (= max_new_tokens - 1), hidden_size).\n",
    "    \"\"\"\n",
    "    \n",
    "    gen_len = len(generation_activations) # gen_len = max_new_tokens - 1 from `model.generate()`\n",
    "    hidden_size = generation_activations[0].shape[-1]\n",
    "    batch_size = beam_indices.shape[0] if k_beams > 1 else generation_activations[0].shape[0]\n",
    "\n",
    "    if k_beams > 1:\n",
    "        # Allocate tensor for aligned hidden states for selected beams\n",
    "        aligned_hidden_states = torch.zeros((batch_size, gen_len, hidden_size), dtype=generation_activations[0].dtype)\n",
    "        \n",
    "        # Align hidden states for the selected beams\n",
    "        for step in range(gen_len):\n",
    "            h = generation_activations[step]  # Shape: (batch_size * k_beams, seq_len, hidden_size)\n",
    "            indices = beam_indices[:, step].to(h.device)  # Shape: (batch_size,)\n",
    "            valid = indices >= 0\n",
    "            if valid.any():\n",
    "                # For each batch item, select the last generated hidden state at this step, for the selected beam sequence \n",
    "                aligned_hidden_states[valid, step, :] = h[indices[valid], -1, :] # Shape (batch_size_valid, hidden_size)\n",
    "    else:\n",
    "        # No beam alignment needed, output comes directly from top-k sampling\n",
    "        # For each batch item, take the last generated hidden state at this step\n",
    "        aligned_hidden_states = torch.stack(\n",
    "            [h[:, -1, :] for h in generation_activations], dim=1\n",
    "        ).to(generation_activations[0].device)  # Shape: (batch_size, gen_len, hidden_size)\n",
    "\n",
    "    return aligned_hidden_states\n",
    "\n",
    "\n",
    "\n",
    "def align_prompt_hidden_states(\n",
    "    prompt_activations: torch.Tensor,\n",
    "    k_beams: int\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    If k_beams > 1, aligns (extracts) the prompt hidden states to match the original batch size\n",
    "    by removing beam duplication introduced for decoding. \n",
    "    If k_beams == 1, returns the prompt hidden states as-is.\n",
    "\n",
    "    During generation with beam search (k_beams > 1), the encoder prompt is computed once per input \n",
    "    and duplicated `k_beams` times to initialize multiple decoding paths. This function removes \n",
    "    the beam-level redundancy from the prompt representations to restore a shape that matches \n",
    "    the actual number of input samples.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    prompt_activations : torch.Tensor\n",
    "        List of activation tensors per generation step. \n",
    "        activations = [prompt] + [gen_step_1, gen_step_2, ..., gen_step_49], if max_new_tokens=50.\n",
    "        so prompt_activations = prompt,  only the prompt part\n",
    "        Each tensor has shape (batch_size * k_beams, seq_len, hidden_size) if k_beams > 1,\n",
    "        or (batch_size, seq_len, hidden_size) if k_beams == 1.\n",
    "    \n",
    "    k_beams : int\n",
    "        Number of beams used during generation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        Aligned prompt hidden states of shape (batch_size, prompt_len, hidden_size).\n",
    "    \"\"\"\n",
    "\n",
    "    if k_beams == 1:\n",
    "        return prompt_activations  # Already (batch_size, prompt_len, hidden_size)\n",
    "    \n",
    "    # If beam search: keep only the first beam per batch\n",
    "    # Since beam search only applies to the generation part, the prompt \n",
    "    # is encoded once, then duplicated k_beams times to initialize generation.\n",
    "    return prompt_activations[::k_beams]  # Take every k_beams-th item\n",
    "\n",
    "\n",
    "\n",
    "def run_prompt_and_generation_activation_extraction(\n",
    "    model: PreTrainedModel,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    dataset: Dataset,\n",
    "    batch_size: int = 4,\n",
    "    idx_start_sample: int = 0,\n",
    "    max_samples: int = 1000,\n",
    "    save_to_pkl: bool = False,\n",
    "    output_path: str = \"outputs/all_batch_results.pkl\",\n",
    "    build_prompt_fn: Callable[[str, str], str] = None,\n",
    "    register_generation_activation_hook_fn: Callable = None,\n",
    "    layer_idx: int = -1,  \n",
    "    extract_token_activations_fn: Callable = None,\n",
    "    activation_source: Literal[\"prompt\", \"generation\", \"prompt+generation\"] = \"generation\",\n",
    "    k_beams : int = 1,\n",
    "    start_offset : int = 0,\n",
    "    end_offset : int = 0,\n",
    "    #**kwargs\n",
    ") -> Union[List[torch.Tensor], None]:\n",
    "    \"\"\"\n",
    "    Runs batched inference on a dataset using a decoder-only language model.\n",
    "    For each batch, it performs text generation and extracts token-level hidden activations \n",
    "    (both from the prompt and the generated text depending on `activation_source`) \n",
    "    from a specified transformer layer.\n",
    "\n",
    "    Hidden states are captured via a forward hook during generation, then aligned and \n",
    "    filtered using attention masks. These representations can be saved to a pickle file \n",
    "    or returned directly.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : PreTrainedModel\n",
    "        The causal language model to evaluate (e.g., LLaMA).\n",
    "    tokenizer : PreTrainedTokenizer\n",
    "        The corresponding tokenizer.\n",
    "    dataset : Dataset\n",
    "        The input dataset.\n",
    "    batch_size : int\n",
    "        Number of samples per batch.\n",
    "    idx_start_sample : int\n",
    "        Index of the first sample to process from the dataset.\n",
    "    max_samples : int\n",
    "        Total number of examples to process from the dataset, starting from idx_start_sample. \n",
    "    save_to_pkl : bool\n",
    "        If True, activations are appended to the pickle file at output_path.\n",
    "        If False, the function returns a list of activations.\n",
    "    output_path : str\n",
    "        Path to the pickle file for saving intermediate results.\n",
    "    build_prompt_fn : Callable\n",
    "        Function to build a prompt from context and question.\n",
    "    register_generation_activation_hook_fn : Callable\n",
    "        Function that registers a forward hook on the model during autoregressive text generation.\n",
    "    layer_idx : int\n",
    "        Index of the transformer layer to extract activations from (default: -1 for last layer).\n",
    "    extract_token_activations_fn : Callable\n",
    "        Function to extract token activations from a model layer.\n",
    "    activation_source : {\"prompt\", \"generation\", \"prompt+generation\"}\n",
    "        Which part of the sequence to extract activations from:\n",
    "        - \"prompt\": only from the prompt\n",
    "        - \"generation\": only from the generated answer\n",
    "        - \"prompt+generation\": both concatenated\n",
    "    k_beams : int, optional\n",
    "        Number of beams for beam search during generation (default: 1). If 1, uses sampling. \n",
    "    start_offset : int\n",
    "        Offset from the first non-padding token (must be >= 0). \n",
    "    end_offset : int\n",
    "        Offset from the last non-padding token (must be <= 0, e.g., -3 to remove 3 tokens).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Union[\n",
    "        List[torch.Tensor],\n",
    "        None\n",
    "    ]\n",
    "        If save_to_pkl is False \n",
    "            Returns batch_activations: list of length `num_samples`, each element is a tensor \n",
    "            of shape (1, hidden_size), containing the selected and aggragated token activations.\n",
    "        If save_to_pkl is True:\n",
    "            Returns None (activations are saved incrementally to output_path).\n",
    "    \"\"\"\n",
    "\n",
    "    batch_activations = []  # Chosen token activation vectors\n",
    "\n",
    "    for i in tqdm(range(idx_start_sample, idx_start_sample + max_samples, batch_size)):\n",
    "        \n",
    "        print(f\"*************************** LOOP {i} ***************************\")\n",
    "        # ==============================\n",
    "        # Prepare input batch\n",
    "        # ==============================\n",
    "        batch = extract_batch(dataset, i, batch_size)\n",
    "        prompts = [build_prompt_fn(s[\"context\"], s[\"question\"]) for s in batch]\n",
    "\n",
    "        '''prompts = [\n",
    "            \"The capital of France is\",\n",
    "            \"What is the name of the French Queen who was decapitated?\",\n",
    "            \"Tell me why the sky is blue?\",\n",
    "            \"What is the color of the eyes of Elisabeth Taylor? (answer one word)\"\n",
    "        ]'''\n",
    "        \n",
    "        inputs = tokenizer(prompts, padding=True, truncation=True, return_tensors=\"pt\").to(model.device)\n",
    "        prompt_len = inputs[\"input_ids\"].shape[1] # Assumes prompts padded to same length\n",
    "        \n",
    "        print(\"prompt_len: \", prompt_len)\n",
    "        print(\"inputs['input_ids'].shape\", inputs['input_ids'].shape)\n",
    "        print(\"k_beams:\", k_beams)\n",
    "        print(\"=======================================\")\n",
    "\n",
    "        # ==============================\n",
    "        # Register forward hook to capture layer output\n",
    "        # ==============================\n",
    "        # This hook collects the hidden states at each decoding step\n",
    "        # activations = [prompt] + [gen_step_1, gen_step_2, ..., gen_step_49], len(activations)=50, if max_new_tokens=50.\n",
    "        activations = [] # activations[k] of Shape: (batch_size * k_beams, seq_len, hidden_size)\n",
    "        handle, call_counter = register_generation_activation_hook_fn(model, activations, layer_idx=layer_idx)\n",
    "\n",
    "        # ==============================\n",
    "        # Run model forward pass (hook captures activations)\n",
    "        # ==============================\n",
    "        # Generate text from prompts using beam search or sampling. \n",
    "        # When the target layer is reached, the hook executes and saves its output in activations.\n",
    "        outputs = generate(model, inputs, tokenizer, max_new_tokens=50, k_beams=k_beams)\n",
    "\n",
    "        gen_answers = tokenizer.batch_decode(outputs.sequences[:, prompt_len:], skip_special_tokens=True) # Shape: [batch_size,]\n",
    "        \n",
    "        print(f\"Hook was called {call_counter['count']} times.\")\n",
    "        print(\"=======================================\")\n",
    "        print(\"outputs.sequences.shape[1]: \", outputs.sequences.shape[1])\n",
    "        if k_beams > 1:\n",
    "            print(\"outputs.beam_indices:\", outputs.beam_indices)\n",
    "            print(\"outputs.beam_indices.shape[1] :\", outputs.beam_indices.shape[1]) \n",
    "        print(\"outputs.sequences[:,prompt_len:] :\", outputs.sequences[: , prompt_len:])\n",
    "        print(\" outputs.sequences[: , prompt_len:].shape[1]:\", outputs.sequences[: , prompt_len:].shape[1])\n",
    "        print(\"gen_answers :\", gen_answers)\n",
    "        print(\"=======================================\")\n",
    "        print(\"Length of activations:\", len(activations))\n",
    "        print(\"Shape of activations[0]:\", activations[0].shape)\n",
    "        print(\"Shape of activations[1]:\", activations[1].shape)\n",
    "        print(\"Shape of activations[-1]:\", activations[-1].shape)\n",
    "        print(\"============\")\n",
    "        print(\"Length of outputs.hidden_states: \", len(outputs.hidden_states))\n",
    "        print(\"Shape of outputs.hidden_states[0][-1]:\", outputs.hidden_states[0][-1].shape)\n",
    "        print(\"Shape of activations[1][-1]:\", outputs.hidden_states[1][-1].shape)\n",
    "        print(\"Shape of activations[-1][-1]:\", outputs.hidden_states[-1][-1].shape)\n",
    "        print(\"=======================================\")\n",
    "\n",
    "        # Remove the hook to avoid memory leaks or duplicate logging\n",
    "        handle.remove() \n",
    "        \n",
    "        #print(f\"Hook was called {call_counter['count']} times.\")\n",
    "        if len(activations)==0:\n",
    "            raise RuntimeError(\"Hook failed to capture activations.\")\n",
    "        \n",
    "        # Define prompt and generation hidden states \n",
    "        prompt_activations=activations[0]      # `[0]` to include only the prompt part \n",
    "        generation_activations=activations[1:] # `[1:]` to exclude the prompt part \n",
    "\n",
    "        # ===============================\n",
    "        # Truncate activations to match real generation steps (cf. Understanding Note #1)\n",
    "        # ===============================\n",
    "        # During generation, the model may run extra forward passes (especially with beam search)\n",
    "        # beyond the number of tokens in the final output. This results in activations being longer\n",
    "        # than needed — we need to truncate them accordingly.\n",
    "        # (see Understanding Note #1).\n",
    "        if k_beams > 1:\n",
    "            # In beam search, we use beam_indices.shape[1] to determine the actual number of generation steps\n",
    "            gen_len = outputs.beam_indices.shape[1]\n",
    "            print(\"gen_len:\", gen_len)\n",
    "\n",
    "        else:\n",
    "            # In greedy/top-k sampling, gen_len is simply the number of new tokens beyond the prompt\n",
    "            gen_len = outputs.sequences.shape[1] - prompt_len\n",
    "            print(\"gen_len:\", gen_len)\n",
    "\n",
    "    \n",
    "        # Sometimes, activations may include extra \"ghost\" steps (e.g., due to internal padding/sync in beam search)\n",
    "        bool_truncate_activations = (len(generation_activations) >= gen_len) \n",
    " \n",
    "        if bool_truncate_activations:\n",
    "            print(\"len(generation_activations) BEFORE truncation\", len(generation_activations))\n",
    "            # Truncate extra steps to ensure alignment with generated tokens\n",
    "            generation_activations = generation_activations[:gen_len]\n",
    "            print(\"len(generation_activations) AFTER truncation\", len(generation_activations))\n",
    "\n",
    "        \"\"\"\n",
    "        ==================================\n",
    "        Understanding Note #1:\n",
    "        ==================================\n",
    "        When using beam search in Hugging Face Transformers, the number of decoder hidden states\n",
    "        (len(outputs.hidden_states)) can be greater than the number of tokens in the final generated \n",
    "        sequence (outputs.sequences[:,prompt_len:].shape[1] = outputs.beam_indices.shape[1]). \n",
    "        This happens because, during beam search, the model explores multiple candidate sequences \n",
    "        (beams) at each generation step and continues generating until a stopping condition is met \n",
    "        (such as all beams reaching EOS or the maximum number of tokens). But because beams can \n",
    "        finish at different steps (some hitting EOS early, others continuing), the model must keep\n",
    "        generating for the remaining active beams. \n",
    "        *Note* that in our code, outputs.hidden_states and activations are the same. \n",
    "      \n",
    "        Explanation from Hugging Face, January 2023: \n",
    "        (https://github.com/huggingface/transformers/issues/21374)\n",
    "        \"Beam Search: Here it's trickier. In essence, beam search looks for candidate outputs until it hits \n",
    "        a stopping condition. The candidate outputs can have fewer tokens than the total number of generation \n",
    "        steps -- for instance, in an encoder-decoder text model, if your input is How much is 2 + 2? and the \n",
    "        model generates as candidates <BOS>4<EOS> (3 tokens) and <BOS>The answer is potato<EOS> \n",
    "        (for argument's sake, 6 tokens) before deciding to stop, you should see sequences with shape [1, 3] \n",
    "        and decoder_hidden_states with length 5, because 5 tokens were generated internally before settling \n",
    "        on the 1st candidate.\"    \n",
    "        \"\"\"\n",
    "\n",
    "        # ===============================\n",
    "        # Truncate generated token IDs to match activations (cf. Understanding Note #2) \n",
    "        # ===============================\n",
    "        # - When N tokens are generated, only the first N-1 tokens have corresponding hidden states.\n",
    "        #   So activations[1:] covers only the first N-1 steps (cf. Understanding Note #2).\n",
    "        #   Therefore, we exclude the last generated token from outputs.sequences and beam_indices\n",
    "        #   to match activations[1:]\n",
    "        # - Exception: if activations were truncated earlier (bool_truncate_activations = True),\n",
    "        #   then we already lost activations of the final decoding step(s), and our activations[1:]\n",
    "        #   only cover the available tokens. In that case, we keep the full `gen_len` to match.\n",
    "        # (see Understanding Note #2)\n",
    "        if bool_truncate_activations:\n",
    "            expected_gen_len = gen_len  # All generated tokens have hidden states\n",
    "            print(\"expected_gen_len = gen_len\")\n",
    "        else: \n",
    "            expected_gen_len  = gen_len - 1 # Drop final token to match activations[1:]\n",
    "            print(\"expected_gen_len  = gen_len - 1\")\n",
    "\n",
    "        # Truncate generated sequences and beam paths accordingly\n",
    "        truncated_gen_sequences = outputs.sequences[:, prompt_len : prompt_len + expected_gen_len]\n",
    "        print(\"truncated_sequences.shape : \", truncated_gen_sequences.shape)\n",
    "\n",
    "        if k_beams > 1:\n",
    "            truncated_beam_indices = outputs.beam_indices[:, :expected_gen_len] \n",
    "            print(\"truncated_beam_indices.shape : \", truncated_beam_indices.shape)\n",
    "\n",
    "        \"\"\"\n",
    "        ==================================\n",
    "        Understanding Note #2:\n",
    "        ==================================\n",
    "        When using model.generate() with output_hidden_states=True (what we are replicating here with the hook),\n",
    "        use_cache=True and max_new_tokens=30, there is always an offset between the length of the \n",
    "        generated sequence (outputs.sequences.shape[1][prompt_len:]) and the length of len(outputs.hidden_states) : \n",
    "        * outputs.sequences.shape[1] = prompt_len (17) + max_new_tokens (30) = 47\n",
    "        * len(outputs.hidden_states) = max_new_tokens (30)\n",
    "            With : \n",
    "            * outputs.hidden_states[0][layer_idx].shape = (batch_size, prompt_len, hidden_size)           --> includes the prompt ! \n",
    "            * outputs.hidden_states[i][layer_idx].shape = (batch_size, 1, hidden_size) with 1 <= i <= 29  --> stops at 29 ! \n",
    "        *Note* that in our code, outputs.hidden_states and activations are the same. \n",
    "            \n",
    "        Explanation from Hugging Face, April 2024 \n",
    "        (https://github.com/huggingface/transformers/issues/30036):\n",
    "        \"If you have 30 tokens at the end of generation, you'll always have 29 hidden states.\n",
    "        The token with index N is used to produce hidden states with index N, which is then used \n",
    "        to get the token with index N+1. The generation ends as soon as the target number of \n",
    "        tokens is obtained so, when we obtain the 30th token, we don't spend compute to get the 30th \n",
    "        set of hidden states. You can, however, manually run an additional forward pass to obtain the \n",
    "        30th set of hidden states, corresponding to the 30th token and used to obtain the 31st token.\n",
    "        \"\"\"\n",
    "\n",
    "        # ===============================\n",
    "        # Align generated and prompt hidden states\n",
    "        # ===============================\n",
    "        # Extract the hidden states that correspond to the generated sequence\n",
    "        # selected by the beam search (or top-k sampling if k_beams = 1)\n",
    "        aligned_generation_hidden_states = align_generation_hidden_states(\n",
    "            generation_activations=generation_activations, \n",
    "            beam_indices=truncated_beam_indices if k_beams > 1 else None,\n",
    "            k_beams=k_beams\n",
    "        ) # Shape: (batch_size, gen_len, hidden_size)\n",
    "        \n",
    "        # Extract the hidden states that correspond to the prompt\n",
    "        aligned_prompt_hidden_states = align_prompt_hidden_states(\n",
    "            prompt_activations=prompt_activations, \n",
    "            k_beams=k_beams\n",
    "        ) # Shape: (batch_size, prompt_len, hidden_size)\n",
    "\n",
    "        # Concatenate the prompt and generation aligned hidden states  \n",
    "        aligned_prompt_and_gen_hidden_states = torch.cat([aligned_prompt_hidden_states, aligned_generation_hidden_states], dim=1)\n",
    "        # Shape: (batch_size, prompt_len + gen_len, hidden_size)\n",
    "\n",
    "        if k_beams > 1:\n",
    "            print(\"=======================================\")\n",
    "            print(\"aligned_generation_hidden_states.shape:\", aligned_generation_hidden_states.shape)\n",
    "            print(\"aligned_prompt_hidden_states.shape:\", aligned_prompt_hidden_states.shape)\n",
    "            print(\"aligned_prompt_and_gen_hidden_states.shape:\", aligned_prompt_and_gen_hidden_states.shape)\n",
    "\n",
    "\n",
    "        # ===============================\n",
    "        # Build generation and prompt attention mask\n",
    "        # ===============================\n",
    "        # This mask marks which generated tokens are valid (i.e., not padding).\n",
    "        # Positions are marked True up to and including the first eos_token_id\n",
    "        generation_attention_mask = build_generation_attention_mask(\n",
    "            gen_ids=truncated_gen_sequences, \n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        ) # Shape (batch_size, gen_len)\n",
    "        print(\"=======================================\")\n",
    "        print(\"generation_attention_mask:\", generation_attention_mask)\n",
    "        print(\"generation_attention_mask.shape :\", generation_attention_mask.shape)\n",
    "\n",
    "        # Prompt attention mask\n",
    "        prompt_attention_mask = inputs[\"attention_mask\"] \n",
    "        # Shape (batch_size, prompt_len)\n",
    "        print(\"=======================================\")\n",
    "        print(\"prompt_attention_mask:\", prompt_attention_mask)\n",
    "        print(\"prompt_attention_mask.shape:\", prompt_attention_mask.shape)\n",
    "\n",
    "        # ===============================\n",
    "        # Modify prompt attention mask with offsets\n",
    "        # ===============================\n",
    "        if start_offset !=0 or end_offset !=0:\n",
    "            prompt_attention_mask, start_indices, end_indices = compute_offset_attention_mask(\n",
    "                attention_mask=prompt_attention_mask, \n",
    "                start_offset=start_offset, \n",
    "                end_offset=end_offset\n",
    "            ) # Shape (batch_size, seq_len), (batch_size,), (batch_size,)\n",
    "            print(\"=======================================\")\n",
    "            print(\"start_indices:\", start_indices)\n",
    "            print(\"end_indices:\", end_indices)\n",
    "            print(\"prompt_attention_mask after OFFSET:\", prompt_attention_mask)\n",
    "            print(\"prompt_attention_mask.shape after OFFSET:\", prompt_attention_mask.shape)\n",
    "\n",
    "            for i, seq in enumerate(inputs[\"input_ids\"]):\n",
    "                print(\"=============inputs before offsets:=============\")\n",
    "                print(tokenizer.decode(seq, skip_special_tokens=True))\n",
    "                print(\"=============inputs after offsets:=============\")\n",
    "                start_idx, end_idx = start_indices[i], end_indices[i]\n",
    "                seq_trunc = seq[start_idx:end_idx]\n",
    "                print(tokenizer.decode(seq_trunc, skip_special_tokens=True))\n",
    "                print(f\"First selected character: ---{tokenizer.decode(seq_trunc[0])}---\")\n",
    "                print(f\"Last selected character: ---{tokenizer.decode(seq_trunc[-1])}---\")      \n",
    "        \n",
    "\n",
    "        # Concatenate the prompt and generation attention mask\n",
    "        prompt_and_gen_attention_mask = torch.cat([prompt_attention_mask, generation_attention_mask], dim=1)\n",
    "        # Shape (batch_size, prompt_len + gen_len)\n",
    "\n",
    "        print(\"=======================================\")\n",
    "        print(\"prompt_and_gen_attention_mask:\", prompt_and_gen_attention_mask)\n",
    "        print(\"prompt_and_gen_attention_mask.shape:\", prompt_and_gen_attention_mask.shape)\n",
    "\n",
    "        # ==============================\n",
    "        # Extract token activations from captured layer, based on source\n",
    "        # ==============================\n",
    "        if activation_source == \"generation\":\n",
    "            # Return only the token activations from the generated answer \n",
    "            selected_token_vecs = extract_token_activations_fn(\n",
    "                    selected_layer=aligned_generation_hidden_states, \n",
    "                    attention_mask=generation_attention_mask, \n",
    "                    device=aligned_generation_hidden_states.device,\n",
    "                    ) # Shape (batch_size, hidden_size)\n",
    "            \n",
    "        elif activation_source == \"prompt\":    \n",
    "            # Return only the token activations from the prompt\n",
    "            selected_token_vecs = extract_token_activations_fn(\n",
    "                    selected_layer=aligned_prompt_hidden_states, \n",
    "                    attention_mask=prompt_attention_mask, \n",
    "                    device=aligned_prompt_hidden_states.device,\n",
    "                    ) # Shape (batch_size, hidden_size)\n",
    "            \n",
    "        elif activation_source == \"prompt+generation\":\n",
    "            # Return token activations from concatenated prompt + generated answer \n",
    "            selected_token_vecs  = extract_token_activations_fn(\n",
    "                    selected_layer=aligned_prompt_and_gen_hidden_states, \n",
    "                    attention_mask=prompt_and_gen_attention_mask, \n",
    "                    device=aligned_prompt_and_gen_hidden_states.device,\n",
    "                    skip_length=prompt_len,\n",
    "                    ) # Shape (batch_size, hidden_size)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Invalid value for `activation_source`: '{activation_source}'. \"\n",
    "                f\"Expected one of: ['prompt', 'generation', 'prompt+generation'].\"\n",
    "            )  \n",
    "        \n",
    "        print(\"selected_token_vecs.shape\", selected_token_vecs.shape)\n",
    "        # ==============================\n",
    "        # Store results (to file or memory)\n",
    "        # ==============================\n",
    "        activations = [selected_token_vecs[j].unsqueeze(0).cpu() for j in range(selected_token_vecs.size(0))]\n",
    "\n",
    "        batch_dataset_ids = [s['id'] for s in batch]\n",
    "        batch_dataset_original_idx = [s['original_index'] for s in batch]\n",
    "        \n",
    "        batch_results = {\n",
    "            \"id\": batch_dataset_ids,\n",
    "            \"original_indices\": batch_dataset_original_idx,\n",
    "            \"activations\": activations,\n",
    "            \"gen_answers\": gen_answers\n",
    "        }\n",
    "\n",
    "        print('batch_results: ', batch_results)\n",
    "\n",
    "        if save_to_pkl:\n",
    "            #append_to_pickle(output_path, batch_results)\n",
    "            save_batch_pickle(batch_data=batch_results, output_dir=output_path, batch_idx=i)\n",
    "        else:\n",
    "            batch_activations.extend(activations)\n",
    "        \n",
    "    if not save_to_pkl:\n",
    "        return batch_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************** LOOP 50 ***************************\n",
      "prompt_len:  329\n",
      "inputs['input_ids'].shape torch.Size([2, 329])\n",
      "k_beams: 1\n",
      "=======================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:00<00:00,  3.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hook was called 7 times.\n",
      "=======================================\n",
      "outputs.sequences.shape[1]:  336\n",
      "outputs.sequences[:,prompt_len:] : tensor([[29871,  8602,   284,  4628,   449, 29889,     2],\n",
      "        [29871,  7861, 29265,     2,     2,     2,     2]], device='cuda:0')\n",
      " outputs.sequences[: , prompt_len:].shape[1]: 7\n",
      "gen_answers : [' Trial blackout.', ' Rhythm']\n",
      "=======================================\n",
      "Length of activations: 7\n",
      "Shape of activations[0]: torch.Size([2, 329, 4096])\n",
      "Shape of activations[1]: torch.Size([2, 1, 4096])\n",
      "Shape of activations[-1]: torch.Size([2, 1, 4096])\n",
      "============\n",
      "Length of outputs.hidden_states:  7\n",
      "Shape of outputs.hidden_states[0][-1]: torch.Size([2, 329, 4096])\n",
      "Shape of activations[1][-1]: torch.Size([2, 1, 4096])\n",
      "Shape of activations[-1][-1]: torch.Size([2, 1, 4096])\n",
      "=======================================\n",
      "gen_len: 7\n",
      "expected_gen_len  = gen_len - 1\n",
      "truncated_sequences.shape :  torch.Size([2, 6])\n",
      "--- Function build_generation_attention_mask => gen_len: 6\n",
      "--- Function build_generation_attention_mask => gen_ids.shape: torch.Size([2, 6])\n",
      "=======================================\n",
      "generation_attention_mask: tensor([[1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 0, 0]], device='cuda:0', dtype=torch.int32)\n",
      "generation_attention_mask.shape : torch.Size([2, 6])\n",
      "=======================================\n",
      "prompt_attention_mask: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "prompt_attention_mask.shape: torch.Size([2, 329])\n",
      "=======================================\n",
      "start_indices: tensor([119,  55], device='cuda:0')\n",
      "end_indices: tensor([325, 325], device='cuda:0')\n",
      "prompt_attention_mask after OFFSET: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]], device='cuda:0',\n",
      "       dtype=torch.int32)\n",
      "prompt_attention_mask.shape after OFFSET: torch.Size([2, 329])\n",
      "=============inputs before offsets:=============\n",
      "[INST] <<SYS>>\n",
      "Given the following passage and question, answer the question by only giving the answer without a complete sentence.\n",
      "If it cannot be answered based on the passage, reply 'unanswerable':\n",
      "<</SYS>>\n",
      "\n",
      "Passage: The government planned to voluntarily evacuate four million people—mostly women and children—from urban areas, including 1.4 million from London. It expected about 90% of evacuees to stay in private homes, and conducted an extensive survey to determine available space. Detailed preparations for transporting them were developed. A trial blackout was held on 10 August 1939, and when Germany invaded Poland on 1 September a blackout began at sunset. Lights would not be allowed after dark for almost six years, and the blackout became by far the most unpopular aspect of the war for civilians, more than rationing.:51,106 The relocation of the government and the civil service was also planned, but would only have occurred if necessary so as not to damage civilian morale.:33\n",
      "Question: What trial took place on August 10 1939? [/INST]\n",
      "=============inputs after offsets:=============\n",
      "Passage: The government planned to voluntarily evacuate four million people—mostly women and children—from urban areas, including 1.4 million from London. It expected about 90% of evacuees to stay in private homes, and conducted an extensive survey to determine available space. Detailed preparations for transporting them were developed. A trial blackout was held on 10 August 1939, and when Germany invaded Poland on 1 September a blackout began at sunset. Lights would not be allowed after dark for almost six years, and the blackout became by far the most unpopular aspect of the war for civilians, more than rationing.:51,106 The relocation of the government and the civil service was also planned, but would only have occurred if necessary so as not to damage civilian morale.:33\n",
      "Question: What trial took place on August 10 1939?\n",
      "First selected character: ---Pass---\n",
      "Last selected character: ---?---\n",
      "=============inputs before offsets:=============\n",
      "[INST] <<SYS>>\n",
      "Given the following passage and question, answer the question by only giving the answer without a complete sentence.\n",
      "If it cannot be answered based on the passage, reply 'unanswerable':\n",
      "<</SYS>>\n",
      "\n",
      "Passage: The key characteristic of classical music that distinguishes it from popular music and folk music is that the repertoire tends to be written down in musical notation, creating a musical part or score. This score typically determines details of rhythm, pitch, and, where two or more musicians (whether singers or instrumentalists) are involved, how the various parts are coordinated. The written quality of the music has enabled a high level of complexity within them: J.S. Bach's fugues, for instance, achieve a remarkable marriage of boldly distinctive melodic lines weaving in counterpoint yet creating a coherent harmonic logic that would be impossible in the heat of live improvisation. The use of written notation also preserves a record of the works and enables Classical musicians to perform music from many centuries ago. Musical notation enables 2000s-era performers to sing a choral work from the 1300s Renaissance era or a 1700s Baroque concerto with many of the features of the music (the melodies, lyrics, forms, and rhythms) being reproduced.\n",
      "Question: The score determines how various parts are coordinated, pitch, and what other detail? [/INST]\n",
      "=============inputs after offsets:=============\n",
      "Passage: The key characteristic of classical music that distinguishes it from popular music and folk music is that the repertoire tends to be written down in musical notation, creating a musical part or score. This score typically determines details of rhythm, pitch, and, where two or more musicians (whether singers or instrumentalists) are involved, how the various parts are coordinated. The written quality of the music has enabled a high level of complexity within them: J.S. Bach's fugues, for instance, achieve a remarkable marriage of boldly distinctive melodic lines weaving in counterpoint yet creating a coherent harmonic logic that would be impossible in the heat of live improvisation. The use of written notation also preserves a record of the works and enables Classical musicians to perform music from many centuries ago. Musical notation enables 2000s-era performers to sing a choral work from the 1300s Renaissance era or a 1700s Baroque concerto with many of the features of the music (the melodies, lyrics, forms, and rhythms) being reproduced.\n",
      "Question: The score determines how various parts are coordinated, pitch, and what other detail?\n",
      "First selected character: ---Pass---\n",
      "Last selected character: ---?---\n",
      "=======================================\n",
      "prompt_and_gen_attention_mask: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0]],\n",
      "       device='cuda:0', dtype=torch.int32)\n",
      "prompt_and_gen_attention_mask.shape: torch.Size([2, 335])\n",
      "selected_token_vecs.shape torch.Size([2, 4096])\n",
      "ACTIVATION:  [tensor([[ 0.4565,  0.3552, -0.6592,  ..., -0.8218, -1.0459, -0.9658]],\n",
      "       dtype=torch.float16), tensor([[-0.7993, -1.6865,  0.4473,  ..., -1.5859, -0.3281,  1.2539]],\n",
      "       dtype=torch.float16)]\n",
      "batch_results:  {'id': ['572fa4b4a23a5019007fc818', '56f6f1073d8e2e1400e372cf'], 'original_indices': [72893, 18415], 'activations': [tensor([[ 0.4565,  0.3552, -0.6592,  ..., -0.8218, -1.0459, -0.9658]],\n",
      "       dtype=torch.float16), tensor([[-0.7993, -1.6865,  0.4473,  ..., -1.5859, -0.3281,  1.2539]],\n",
      "       dtype=torch.float16)], 'gen_answers': [' Trial blackout.', ' Rhythm']}\n",
      "*************************** LOOP 52 ***************************\n",
      "prompt_len:  496\n",
      "inputs['input_ids'].shape torch.Size([2, 496])\n",
      "k_beams: 1\n",
      "=======================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  3.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hook was called 4 times.\n",
      "=======================================\n",
      "outputs.sequences.shape[1]:  500\n",
      "outputs.sequences[:,prompt_len:] : tensor([[29871, 27486,     2,     2],\n",
      "        [29871,  5430,   277,     2]], device='cuda:0')\n",
      " outputs.sequences[: , prompt_len:].shape[1]: 4\n",
      "gen_answers : [' Identity', ' Gruit']\n",
      "=======================================\n",
      "Length of activations: 4\n",
      "Shape of activations[0]: torch.Size([2, 496, 4096])\n",
      "Shape of activations[1]: torch.Size([2, 1, 4096])\n",
      "Shape of activations[-1]: torch.Size([2, 1, 4096])\n",
      "============\n",
      "Length of outputs.hidden_states:  4\n",
      "Shape of outputs.hidden_states[0][-1]: torch.Size([2, 496, 4096])\n",
      "Shape of activations[1][-1]: torch.Size([2, 1, 4096])\n",
      "Shape of activations[-1][-1]: torch.Size([2, 1, 4096])\n",
      "=======================================\n",
      "gen_len: 4\n",
      "expected_gen_len  = gen_len - 1\n",
      "truncated_sequences.shape :  torch.Size([2, 3])\n",
      "--- Function build_generation_attention_mask => gen_len: 3\n",
      "--- Function build_generation_attention_mask => gen_ids.shape: torch.Size([2, 3])\n",
      "=======================================\n",
      "generation_attention_mask: tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], device='cuda:0', dtype=torch.int32)\n",
      "generation_attention_mask.shape : torch.Size([2, 3])\n",
      "=======================================\n",
      "prompt_attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "prompt_attention_mask.shape: torch.Size([2, 496])\n",
      "=======================================\n",
      "start_indices: tensor([ 55, 254], device='cuda:0')\n",
      "end_indices: tensor([492, 492], device='cuda:0')\n",
      "prompt_attention_mask after OFFSET: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]], device='cuda:0',\n",
      "       dtype=torch.int32)\n",
      "prompt_attention_mask.shape after OFFSET: torch.Size([2, 496])\n",
      "=============inputs before offsets:=============\n",
      "[INST] <<SYS>>\n",
      "Given the following passage and question, answer the question by only giving the answer without a complete sentence.\n",
      "If it cannot be answered based on the passage, reply 'unanswerable':\n",
      "<</SYS>>\n",
      "\n",
      "Passage: The implications are multiple as various research traditions are now[when?] heavily utilizing the lens of identity to examine phenomena.[citation needed] One implication of identity and of identity construction can be seen in occupational settings. This becomes increasing challenging in stigmatized jobs or \"dirty work\" (Hughes, 1951). Tracy and Trethewey (2005) state that \"individuals gravitate toward and turn away from particular jobs depending in part, on the extent to which they validate a \"preferred organizational self\" (Tracy & Tretheway 2005, p. 169). Some jobs carry different stigmas or acclaims. In her analysis Tracy uses the example of correctional officers trying to shake the stigma of \"glorified maids\" (Tracy & Tretheway 2005). \"The process by which people arrive at justifications of and values for various occupational choices.\" Among these are workplace satisfaction and overall quality of life (Tracy & Scott 2006, p. 33). People in these types of jobs are forced to find ways in order to create an identity they can live with. \"Crafting a positive sense of self at work is more challenging when one's work is considered \"dirty\" by societal standards\" (Tracy & Scott 2006, p. 7). \"In other words, doing taint management is not just about allowing the employee to feel good in that job. \"If employees must navigate discourses that question the viability of their work, and/ or experience obstacles in managing taint through transforming dirty work into a badge of honor, it is likely they will find blaming the client to be an efficacious route in affirming their identity\" (Tracy & Scott 2006, p. 33).\n",
      "Question: Various research traditions are using what lens to examine phenomena? [/INST]\n",
      "=============inputs after offsets:=============\n",
      "Passage: The implications are multiple as various research traditions are now[when?] heavily utilizing the lens of identity to examine phenomena.[citation needed] One implication of identity and of identity construction can be seen in occupational settings. This becomes increasing challenging in stigmatized jobs or \"dirty work\" (Hughes, 1951). Tracy and Trethewey (2005) state that \"individuals gravitate toward and turn away from particular jobs depending in part, on the extent to which they validate a \"preferred organizational self\" (Tracy & Tretheway 2005, p. 169). Some jobs carry different stigmas or acclaims. In her analysis Tracy uses the example of correctional officers trying to shake the stigma of \"glorified maids\" (Tracy & Tretheway 2005). \"The process by which people arrive at justifications of and values for various occupational choices.\" Among these are workplace satisfaction and overall quality of life (Tracy & Scott 2006, p. 33). People in these types of jobs are forced to find ways in order to create an identity they can live with. \"Crafting a positive sense of self at work is more challenging when one's work is considered \"dirty\" by societal standards\" (Tracy & Scott 2006, p. 7). \"In other words, doing taint management is not just about allowing the employee to feel good in that job. \"If employees must navigate discourses that question the viability of their work, and/ or experience obstacles in managing taint through transforming dirty work into a badge of honor, it is likely they will find blaming the client to be an efficacious route in affirming their identity\" (Tracy & Scott 2006, p. 33).\n",
      "Question: Various research traditions are using what lens to examine phenomena?\n",
      "First selected character: ---Pass---\n",
      "Last selected character: ---?---\n",
      "=============inputs before offsets:=============\n",
      "[INST] <<SYS>>\n",
      "Given the following passage and question, answer the question by only giving the answer without a complete sentence.\n",
      "If it cannot be answered based on the passage, reply 'unanswerable':\n",
      "<</SYS>>\n",
      "\n",
      "Passage: The first historical mention of the use of hops in beer was from 822 AD in monastery rules written by Adalhard the Elder, also known as Adalard of Corbie, though the date normally given for widespread cultivation of hops for use in beer is the thirteenth century. Before the thirteenth century, and until the sixteenth century, during which hops took over as the dominant flavouring, beer was flavoured with other plants; for instance, grains of paradise or alehoof. Combinations of various aromatic herbs, berries, and even ingredients like wormwood would be combined into a mixture known as gruit and used as hops are now used. Some beers today, such as Fraoch' by the Scottish Heather Ales company and Cervoise Lancelot by the French Brasserie-Lancelot company, use plants other than hops for flavouring.\n",
      "Question: What would you call a mixture of ingredients used for brewing before the 16th century? [/INST]\n",
      "=============inputs after offsets:=============\n",
      "Passage: The first historical mention of the use of hops in beer was from 822 AD in monastery rules written by Adalhard the Elder, also known as Adalard of Corbie, though the date normally given for widespread cultivation of hops for use in beer is the thirteenth century. Before the thirteenth century, and until the sixteenth century, during which hops took over as the dominant flavouring, beer was flavoured with other plants; for instance, grains of paradise or alehoof. Combinations of various aromatic herbs, berries, and even ingredients like wormwood would be combined into a mixture known as gruit and used as hops are now used. Some beers today, such as Fraoch' by the Scottish Heather Ales company and Cervoise Lancelot by the French Brasserie-Lancelot company, use plants other than hops for flavouring.\n",
      "Question: What would you call a mixture of ingredients used for brewing before the 16th century?\n",
      "First selected character: ---Pass---\n",
      "Last selected character: ---?---\n",
      "=======================================\n",
      "prompt_and_gen_attention_mask: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1]],\n",
      "       device='cuda:0', dtype=torch.int32)\n",
      "prompt_and_gen_attention_mask.shape: torch.Size([2, 499])\n",
      "selected_token_vecs.shape torch.Size([2, 4096])\n",
      "ACTIVATION:  [tensor([[ 1.6455,  1.3467, -0.8770,  ..., -1.1436, -2.5332,  0.3921]],\n",
      "       dtype=torch.float16), tensor([[ 0.4897, -1.5293, -1.7236,  ..., -0.6133, -2.7090,  0.3794]],\n",
      "       dtype=torch.float16)]\n",
      "batch_results:  {'id': ['57097697ed30961900e84192', '570a40346d058f1900182d1a'], 'original_indices': [26844, 27480], 'activations': [tensor([[ 1.6455,  1.3467, -0.8770,  ..., -1.1436, -2.5332,  0.3921]],\n",
      "       dtype=torch.float16), tensor([[ 0.4897, -1.5293, -1.7236,  ..., -0.6133, -2.7090,  0.3794]],\n",
      "       dtype=torch.float16)], 'gen_answers': [' Identity', ' Gruit']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_offset = 55\n",
    "end_offset = -4\n",
    "result = run_prompt_and_generation_activation_extraction(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=id_fit_dataset,\n",
    "    batch_size=2,\n",
    "    idx_start_sample= 50,\n",
    "    max_samples= 4,\n",
    "    save_to_pkl = False,\n",
    "    output_path = \"outputs/all_batch_results.pkl\",\n",
    "    build_prompt_fn=build_prompt,\n",
    "    register_generation_activation_hook_fn=register_generation_activation_hook,\n",
    "    layer_idx = -1,  \n",
    "    extract_token_activations_fn=partial(extract_token_activations, mode=\"first_generated\"), \n",
    "    activation_source = \"prompt+generation\",\n",
    "    k_beams=1,\n",
    "    start_offset = start_offset,\n",
    "    end_offset = end_offset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 0.4565,  0.3552, -0.6592,  ..., -0.8218, -1.0459, -0.9658]],\n",
       "        dtype=torch.float16),\n",
       " tensor([[-0.7993, -1.6865,  0.4473,  ..., -1.5859, -0.3281,  1.2539]],\n",
       "        dtype=torch.float16),\n",
       " tensor([[ 1.6455,  1.3467, -0.8770,  ..., -1.1436, -2.5332,  0.3921]],\n",
       "        dtype=torch.float16),\n",
       " tensor([[ 0.4897, -1.5293, -1.7236,  ..., -0.6133, -2.7090,  0.3794]],\n",
       "        dtype=torch.float16)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************** LOOP 50 ***************************\n",
      "prompt_len:  328\n",
      "inputs['input_ids'].shape torch.Size([2, 328])\n",
      "k_beams: 3\n",
      "=======================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:00<00:00,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hook was called 7 times.\n",
      "=======================================\n",
      "outputs.sequences.shape[1]:  335\n",
      "outputs.beam_indices: tensor([[ 0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 3,  3,  3,  3,  3, -1, -1]], device='cuda:0', dtype=torch.int32)\n",
      "outputs.beam_indices.shape[1] : 7\n",
      "outputs.sequences[:,prompt_len:] : tensor([[29871,  8602,   284,  4628,   449, 29889,     2],\n",
      "        [29871,  7861, 29265, 29889,     2,     2,     2]], device='cuda:0')\n",
      " outputs.sequences[: , prompt_len:].shape[1]: 7\n",
      "=======================================\n",
      "Length of activations: 7\n",
      "Shape of activations[0]: torch.Size([6, 328, 4096])\n",
      "Shape of activations[1]: torch.Size([6, 1, 4096])\n",
      "Shape of activations[-1]: torch.Size([6, 1, 4096])\n",
      "============\n",
      "Length of outputs.hidden_states:  7\n",
      "Shape of outputs.hidden_states[0][-1]: torch.Size([6, 328, 4096])\n",
      "Shape of activations[1][-1]: torch.Size([6, 1, 4096])\n",
      "Shape of activations[-1][-1]: torch.Size([6, 1, 4096])\n",
      "=======================================\n",
      "gen_len: 7\n",
      "expected_gen_len  = gen_len - 1\n",
      "truncated_sequences.shape :  torch.Size([2, 6])\n",
      "truncated_beam_indices.shape :  torch.Size([2, 6])\n",
      "=======================================\n",
      "aligned_generation_hidden_states.shape: torch.Size([2, 6, 4096])\n",
      "aligned_prompt_hidden_states.shape: torch.Size([2, 328, 4096])\n",
      "aligned_prompt_and_gen_hidden_states.shape: torch.Size([2, 334, 4096])\n",
      "--- Function build_generation_attention_mask => gen_len: 6\n",
      "--- Function build_generation_attention_mask => gen_ids.shape: torch.Size([2, 6])\n",
      "=======================================\n",
      "generation_attention_mask: tensor([[1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 0]], device='cuda:0', dtype=torch.int32)\n",
      "generation_attention_mask.shape : torch.Size([2, 6])\n",
      "=======================================\n",
      "prompt_attention_mask: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "prompt_attention_mask.shape: torch.Size([2, 328])\n",
      "=======================================\n",
      "prompt_and_gen_attention_mask: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]],\n",
      "       device='cuda:0')\n",
      "prompt_and_gen_attention_mask.shape: torch.Size([2, 334])\n",
      "--- Function extract_token_activations ==> last_indices:  tensor([[5],\n",
      "        [4]])\n",
      "selected_token_vecs.shape torch.Size([2, 2, 4096])\n",
      "batch_results:  {'id': ['572fa4b4a23a5019007fc818', '56f6f1073d8e2e1400e372cf'], 'original_indices': [72893, 18415], 'activations': [tensor([[[-2.0508, -2.3496,  0.1203,  ..., -1.4443,  0.2798, -2.6465],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "       dtype=torch.float16), tensor([[[-1.0703,  0.9766, -0.4729,  ..., -0.5195, -0.1182, -3.1836],\n",
      "         [-0.8818, -0.8779,  3.7559,  ..., -3.2559,  1.0020, -1.7510]]],\n",
      "       dtype=torch.float16)]}\n",
      "*************************** LOOP 52 ***************************\n",
      "prompt_len:  495\n",
      "inputs['input_ids'].shape torch.Size([2, 495])\n",
      "k_beams: 3\n",
      "=======================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  2.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hook was called 5 times.\n",
      "=======================================\n",
      "outputs.sequences.shape[1]:  499\n",
      "outputs.beam_indices: tensor([[0, 0, 0, 0],\n",
      "        [3, 3, 3, 3]], device='cuda:0', dtype=torch.int32)\n",
      "outputs.beam_indices.shape[1] : 4\n",
      "outputs.sequences[:,prompt_len:] : tensor([[29871, 27486, 29889,     2],\n",
      "        [29871,  5430,   277,     2]], device='cuda:0')\n",
      " outputs.sequences[: , prompt_len:].shape[1]: 4\n",
      "=======================================\n",
      "Length of activations: 5\n",
      "Shape of activations[0]: torch.Size([6, 495, 4096])\n",
      "Shape of activations[1]: torch.Size([6, 1, 4096])\n",
      "Shape of activations[-1]: torch.Size([6, 1, 4096])\n",
      "============\n",
      "Length of outputs.hidden_states:  5\n",
      "Shape of outputs.hidden_states[0][-1]: torch.Size([6, 495, 4096])\n",
      "Shape of activations[1][-1]: torch.Size([6, 1, 4096])\n",
      "Shape of activations[-1][-1]: torch.Size([6, 1, 4096])\n",
      "=======================================\n",
      "gen_len: 4\n",
      "len(generation_activations) BEFORE truncation 4\n",
      "len(generation_activations) AFTER truncation 4\n",
      "expected_gen_len = gen_len\n",
      "truncated_sequences.shape :  torch.Size([2, 4])\n",
      "truncated_beam_indices.shape :  torch.Size([2, 4])\n",
      "=======================================\n",
      "aligned_generation_hidden_states.shape: torch.Size([2, 4, 4096])\n",
      "aligned_prompt_hidden_states.shape: torch.Size([2, 495, 4096])\n",
      "aligned_prompt_and_gen_hidden_states.shape: torch.Size([2, 499, 4096])\n",
      "--- Function build_generation_attention_mask => gen_len: 4\n",
      "--- Function build_generation_attention_mask => gen_ids.shape: torch.Size([2, 4])\n",
      "=======================================\n",
      "generation_attention_mask: tensor([[1, 1, 1, 1],\n",
      "        [1, 1, 1, 1]], device='cuda:0', dtype=torch.int32)\n",
      "generation_attention_mask.shape : torch.Size([2, 4])\n",
      "=======================================\n",
      "prompt_attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "prompt_attention_mask.shape: torch.Size([2, 495])\n",
      "=======================================\n",
      "prompt_and_gen_attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')\n",
      "prompt_and_gen_attention_mask.shape: torch.Size([2, 499])\n",
      "--- Function extract_token_activations ==> last_indices:  tensor([[3],\n",
      "        [3]])\n",
      "selected_token_vecs.shape torch.Size([2, 2, 4096])\n",
      "batch_results:  {'id': ['57097697ed30961900e84192', '570a40346d058f1900182d1a'], 'original_indices': [26844, 27480], 'activations': [tensor([[[-2.3105, -0.5649,  1.1689,  ..., -2.0176,  0.7080,  0.3142],\n",
      "         [-3.3496, -1.5254,  1.2930,  ..., -1.9180, -0.0315, -1.2510]]],\n",
      "       dtype=torch.float16), tensor([[[-2.3105, -0.5649,  1.1689,  ..., -2.0176,  0.7080,  0.3142],\n",
      "         [-3.3496, -1.5254,  1.2930,  ..., -1.9180, -0.0315, -1.2510]]],\n",
      "       dtype=torch.float16)]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_offset = 0\n",
    "end_offset = 0\n",
    "\n",
    "result = run_prompt_and_generation_activation_extraction(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=id_fit_dataset,\n",
    "    batch_size=2,\n",
    "    idx_start_sample= 50,\n",
    "    max_samples= 4,\n",
    "    save_to_pkl = False,\n",
    "    output_path = \"outputs/all_batch_results.pkl\",\n",
    "    build_prompt_fn=build_prompt,\n",
    "    register_generation_activation_hook_fn=register_generation_activation_hook,\n",
    "    layer_idx = -1,  \n",
    "    extract_token_activations_fn=partial(extract_token_activations, mode=\"last\"), \n",
    "    activation_source = \"generation\",\n",
    "    k_beams=3,\n",
    "    start_offset = start_offset,\n",
    "    end_offset = end_offset\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[-2.0508, -2.3496,  0.1203,  ..., -1.4443,  0.2798, -2.6465],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
       "        dtype=torch.float16),\n",
       " tensor([[[-1.0703,  0.9766, -0.4729,  ..., -0.5195, -0.1182, -3.1836],\n",
       "          [-0.8818, -0.8779,  3.7559,  ..., -3.2559,  1.0020, -1.7510]]],\n",
       "        dtype=torch.float16)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redo properly the comparison with the GT response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def run_filter_generated_answers_by_similarity(\n",
    "    model: PreTrainedModel,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    dataset: Dataset,\n",
    "    batch_size: int = 4,\n",
    "    idx_start_sample: int = 0,\n",
    "    max_samples: int = 1000,\n",
    "    output_path: str = \"outputs/all_batch_results.pkl\",\n",
    "    build_prompt_fn: Callable[[str, str], str] = None,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Generates answers in batch using a decoder-only language model, evaluates their semantic \n",
    "    similarity against ground-truth answers using ROUGE-L and SBERT, and stores the results\n",
    "    to a pickle file.\n",
    "\n",
    "    An answer is considered correct if:\n",
    "        - ROUGE-L ≥ 0.5, or\n",
    "        - SBERT ≥ 0.4 (only computed if ROUGE-L < 0.5)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : PreTrainedModel\n",
    "        The causal language model to evaluate (e.g., LLaMA).\n",
    "    tokenizer : PreTrainedTokenizer\n",
    "        The corresponding tokenizer.\n",
    "    dataset : Dataset\n",
    "        The input dataset.\n",
    "    batch_size : int\n",
    "        Number of samples per batch.\n",
    "    idx_start_sample : int\n",
    "        Index of the first sample to process from the dataset.\n",
    "    max_samples : int\n",
    "        Total number of examples to process from the dataset, starting from idx_start_sample. \n",
    "    output_path : str\n",
    "        Path to the pickle file for saving intermediate results.\n",
    "    build_prompt_fn : Callable\n",
    "        Function to build a prompt from context and question.\n",
    "    \"\"\"\n",
    "    for i in tqdm(range(idx_start_sample, idx_start_sample + max_samples, batch_size)):\n",
    "        \n",
    "        # ==============================\n",
    "        # Initialize batch containers\n",
    "        # ==============================\n",
    "        batch_answers = []               # Generated answers\n",
    "        batch_gt_answers = []            # Ground-truth answers\n",
    "        batch_is_correct = []            # 0/1 labels indicating correctness\n",
    "        batch_dataset_ids = []           # 'id' field from dataset\n",
    "        batch_dataset_original_idx = []  # Original indices from dataset\n",
    "        batch_rouge_scores = []          # Rouge-L scores\n",
    "        batch_sbert_scores = []          # Sentence-Bert scores\n",
    "\n",
    "        # ==============================\n",
    "        # Prepare input batch\n",
    "        # ==============================\n",
    "        batch = extract_batch(dataset, i, batch_size)\n",
    "        prompts = [build_prompt_fn(s[\"context\"], s[\"question\"]) for s in batch]\n",
    "        answers = [s[\"answers\"][\"text\"] for s in batch]\n",
    "        inputs = tokenizer(prompts, padding=True, truncation=True, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        # ==============================\n",
    "        # Generate model predictions\n",
    "        # ==============================\n",
    "        outputs = generate(model, inputs, tokenizer)\n",
    "        outputs_ids = outputs.sequences \n",
    "        \n",
    "        for j in range(len(prompts)):\n",
    "            # ==============================\n",
    "            # Decode generated tokens\n",
    "            # ==============================\n",
    "            prompt_len = len(inputs[\"input_ids\"][j]) # Length of prompt j\n",
    "            generated_answer_ids = outputs_ids[j][prompt_len:] # Remove prompt prefix to isolate the generated answer\n",
    "            generated_answer = tokenizer.decode(generated_answer_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "            # ==============================\n",
    "            # Compute semantic similarity between model's answer and ground-truth\n",
    "            # ==============================\n",
    "            rouge_l_score = rouge_l_simScore(generated_answer, answers[j])\n",
    "            if rouge_l_score >= 0.5:\n",
    "                is_correct = True\n",
    "                sbert_score = None\n",
    "            else:\n",
    "                sbert_score = sentence_bert_simScore(generated_answer, answers[j])\n",
    "                is_correct = (sbert_score >= 0.4)\n",
    "\n",
    "            # ==============================\n",
    "            # Store per-example results\n",
    "            # ==============================\n",
    "            batch_dataset_ids.append(batch[j]['id'])\n",
    "            batch_dataset_original_idx.append(batch[j]['original_index'])\n",
    "            batch_answers.append(generated_answer)\n",
    "            batch_gt_answers.append(answers[j])\n",
    "            batch_is_correct.append(int(is_correct))\n",
    "            batch_rouge_scores.append(rouge_l_score)\n",
    "            batch_sbert_scores.append(sbert_score)\n",
    "\n",
    "        # ==============================\n",
    "        # Store results (to file)\n",
    "        # ==============================\n",
    "        batch_results = {\n",
    "            \"id\": batch_dataset_ids,\n",
    "            \"original_indices\": batch_dataset_original_idx,\n",
    "            \"gen_answers\": batch_answers,\n",
    "            \"ground_truths\": batch_gt_answers,\n",
    "            \"is_correct\": batch_is_correct,\n",
    "            \"sbert_scores\": batch_sbert_scores,\n",
    "            \"rouge_scores\": batch_rouge_scores\n",
    "        }\n",
    "        \n",
    "        #append_to_pickle(output_path, batch_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.30it/s]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "run_filter_generated_answers_by_similarity(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=id_fit_dataset,\n",
    "    batch_size=4,\n",
    "    idx_start_sample= 0,\n",
    "    max_samples=10,\n",
    "    output_path=\"outputs/all_batch_results.pkl\",\n",
    "    build_prompt_fn=build_prompt\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PreTrainedModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34manalyze_single_generation\u001b[39m(\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     model: \u001b[43mPreTrainedModel\u001b[49m,\n\u001b[32m      3\u001b[39m     tokenizer: PreTrainedTokenizer,\n\u001b[32m      4\u001b[39m     dataset: Dataset,\n\u001b[32m      5\u001b[39m     sample_idx: \u001b[38;5;28mint\u001b[39m = \u001b[32m0\u001b[39m,\n\u001b[32m      6\u001b[39m     build_prompt_fn: Callable[[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m      7\u001b[39m     register_forward_activation_hook_fn: Callable = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m      8\u001b[39m     layer_idx: \u001b[38;5;28mint\u001b[39m = -\u001b[32m1\u001b[39m,\n\u001b[32m      9\u001b[39m     extract_token_activations_fn: Callable = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     10\u001b[39m     **kwargs\n\u001b[32m     11\u001b[39m ) -> Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[32m     12\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[33;03m    Analyze a single sample from the dataset through the full inference pipeline:\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[33;03m    - Build prompt\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     51\u001b[39m \u001b[33;03m        - timing breakdown\u001b[39;00m\n\u001b[32m     52\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m     53\u001b[39m     sample = dataset[sample_idx]\n",
      "\u001b[31mNameError\u001b[39m: name 'PreTrainedModel' is not defined"
     ]
    }
   ],
   "source": [
    "def analyze_single_generation(\n",
    "    model: PreTrainedModel,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    dataset: Dataset,\n",
    "    sample_idx: int = 0,\n",
    "    build_prompt_fn: Callable[[str, str], str] = None,\n",
    "    register_forward_activation_hook_fn: Callable = None,\n",
    "    layer_idx: int = -1,\n",
    "    extract_token_activations_fn: Callable = None,\n",
    "    **kwargs\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Analyze a single sample from the dataset through the full inference pipeline:\n",
    "    - Build prompt\n",
    "    - Run forward pass and capture hidden states\n",
    "    - Extract token-level activations from a specific layer\n",
    "    - Generate an answer\n",
    "    - Decode output and compute similarity scores with ground-truth\n",
    "\n",
    "    Also returns timing information for each processing stage.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : PreTrainedModel\n",
    "        The causal language model to evaluate (e.g., LLaMA).\n",
    "    tokenizer : PreTrainedTokenizer\n",
    "        The corresponding tokenizer.\n",
    "    dataset : Dataset\n",
    "        The input dataset.\n",
    "    sample_idx : int\n",
    "        Index of the sample to analyze.\n",
    "    build_prompt_fn : Callable\n",
    "        Function to build a prompt from context and question.\n",
    "    register_forward_activation_hook_fn : Callable\n",
    "        Function that registers a forward hook on the model during a forward pass. \n",
    "    layer_idx : int\n",
    "        Index of the transformer layer to extract activations from (default -1: last layer).\n",
    "    extract_token_activations_fn : Callable\n",
    "        Function that selects and aggregates token-level activations.\n",
    "    **kwargs :\n",
    "        Additional keyword arguments passed to extract_token_activations_fn.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, Any]\n",
    "        Dictionary with:\n",
    "        - prompt, generated and ground-truth answers\n",
    "        - similarity scores (ROUGE-L, Sentence-BERT)\n",
    "        - whether generation is deemed correct\n",
    "        - activations and tensor shapes\n",
    "        - timing breakdown\n",
    "    \"\"\"\n",
    "    sample = dataset[sample_idx]\n",
    "\n",
    "    print(\"========= Analyze one generation  =========\")\n",
    "    times = {}\n",
    "\n",
    "    # ==============================\n",
    "    # 1. Prompt construction\n",
    "    # ==============================\n",
    "    t0 = time.time()\n",
    "    prompt = build_prompt_fn(sample[\"context\"], sample[\"question\"])\n",
    "    answer = sample[\"answers\"]['text']\n",
    "    times['prompt_construction'] = time.time() - t0\n",
    "    print(f\"----- Prompt construction: {times['prompt_construction']:.3f} sec\")\n",
    "\n",
    "    # ==============================\n",
    "    # 2. Tokenization\n",
    "    # ==============================\n",
    "    t1 = time.time()\n",
    "    inputs = tokenizer(prompt, truncation=True, padding=True, return_tensors=\"pt\").to(model.device)\n",
    "    times['tokenization'] = time.time() - t1\n",
    "    print(f\"----- Tokenization: {times['tokenization']:.3f} sec\")\n",
    "\n",
    "    t2 = time.time()\n",
    "    # ==============================\n",
    "    # Capture hidden states with forward hook\n",
    "    # ==============================\n",
    "    # Hook to collect the hidden states after the forward pass\n",
    "    captured_hidden = {}\n",
    "    handle, call_counter = register_forward_activation_hook_fn(model, captured_hidden, layer_idx=layer_idx)\n",
    "\n",
    "    # Pass inputs through the model. When the target layer is reached,\n",
    "    # the hook executes and saves its output in captured_hidden.\n",
    "    with torch.no_grad():\n",
    "        _ = model(**inputs, return_dict=True)\n",
    "    # Remove the hook to avoid memory leaks or duplicate logging\n",
    "    handle.remove() \n",
    "\n",
    "    #print(f\"Hook was called {call_counter['count']} times.\")\n",
    "    if \"activations\" not in captured_hidden:\n",
    "        raise RuntimeError(\"Hook failed to capture activations.\")\n",
    "\n",
    "    layer_output = captured_hidden[\"activations\"]  # Shape: (batch_size, seq_len, hidden_size)\n",
    "\n",
    "    # ==============================\n",
    "    # Extract token activations from captured layer\n",
    "    # ==============================\n",
    "    times['layer_output'] = time.time() - t2\n",
    "    print(f\"----- Token extraction with single forward pass: {times['layer_output']:.3f} sec\")\n",
    "\n",
    "    # Token activations extraction\n",
    "    t3 = time.time()\n",
    "    selected_token_vecs = extract_token_activations_fn(\n",
    "        selected_layer=layer_output,\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        device=layer_output.device,\n",
    "        **kwargs\n",
    "    )\n",
    "    times['token_activations'] = time.time() - t3\n",
    "\n",
    "    # ==============================\n",
    "    # Run generation\n",
    "    # ==============================\n",
    "    t4 = time.time()\n",
    "    outputs = generate(model, inputs, tokenizer)\n",
    "    outputs_ids = outputs.sequences\n",
    "    times['generation'] = time.time() - t4\n",
    "    print(f\"----- Generation: {times['generation']:.3f} sec\")\n",
    "\n",
    "    # ==============================\n",
    "    # Decode generated output\n",
    "    # ==============================\n",
    "    t5 = time.time()\n",
    "    prompt_len = len(inputs[\"input_ids\"][0])\n",
    "    generated_answer_ids = outputs_ids[0][prompt_len:]\n",
    "    generated_answer = tokenizer.decode(generated_answer_ids, skip_special_tokens=True).strip()\n",
    "    times['decoding'] = time.time() - t5\n",
    "    print(f\"----- Decoding: {times['decoding']:.3f} sec\")\n",
    "\n",
    "    # ==============================\n",
    "    # Compute similarity scores\n",
    "    # ==============================\n",
    "    t6 = time.time()\n",
    "    rouge_l_score = rouge_l_simScore(generated_answer, answer) \n",
    "    sbert_sim = sentence_bert_simScore(generated_answer, answer)\n",
    "    is_correct = (rouge_l_score >= 0.5) or (sbert_sim >= 0.4)\n",
    "    times['similarity_scoring'] = time.time() - t6\n",
    "    print(f\"----- Similarity scoring: {times['similarity_scoring']:.3f} sec\")\n",
    "\n",
    "    # ==============================\n",
    "    # Display results\n",
    "    # ==============================\n",
    "    print(\"\\n=== Prompt ===\")\n",
    "    print(prompt)\n",
    "    print(\"\\n=== Shapes ===\")\n",
    "    print(f\"Shape - number of tokens: {inputs['input_ids'].shape}\")\n",
    "    print(f\"Shape - selected_layer: {layer_output.shape}\")\n",
    "    print(\"\\n=== Generated Answer ===\")\n",
    "    print(generated_answer)\n",
    "    print(\"\\n=== Ground-truth Answer ===\")\n",
    "    print(answer)\n",
    "    print(\"\\n=== Similarity Scores ===\")\n",
    "    print(f\"ROUGE-L F1: {rouge_l_score:.4f}\")\n",
    "    print(f\"Sentence-BERT Cosine Similarity: {sbert_sim:.4f}\")\n",
    "    print(f\"Is generated answer correct: {is_correct}\")\n",
    "\n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"generated_answer\": generated_answer,\n",
    "        \"ground_truth_answer\": answer,\n",
    "        \"rouge_l_score\": rouge_l_score,\n",
    "        \"sbert_score\": sbert_sim,\n",
    "        \"is_correct\": is_correct,\n",
    "        \"computation_times\": times,\n",
    "        \"input_shape\": inputs['input_ids'].shape,\n",
    "        \"layer_shape\": layer_output.shape,\n",
    "        \"token_activations\": selected_token_vecs,\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "analyze_single_generation(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=id_fit_dataset,\n",
    "    sample_idx=0,\n",
    "    build_prompt_fn=build_prompt,\n",
    "    register_forward_activation_hook_fn=register_forward_activation_hook,\n",
    "    layer_idx=-1,\n",
    "    extract_token_activations_fn=extract_token_activations,\n",
    "    start_offset=0,\n",
    "    end_offset=0\n",
    "\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redo properly the token extraction of the answer WITH teacher forcing => abandonded: it makes no sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def run_prompt_and_generation_activation_extraction_teacherForcing(\n",
    "    model: PreTrainedModel,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    dataset: Dataset,\n",
    "    batch_size: int = 4,\n",
    "    idx_start_sample: int = 0,\n",
    "    max_samples: int = 1000,\n",
    "    save_to_pkl: bool = False,\n",
    "    output_path: str = \"outputs/all_batch_results.pkl\",\n",
    "    build_prompt_fn: Callable[[str, str], str] = None,\n",
    "    register_forward_activation_hook_fn: Callable = None,\n",
    "    layer_idx: int = -1,  \n",
    "    extract_token_activations_fn: Callable = None, \n",
    "    exclude_prompt: bool = True,\n",
    "    **kwargs\n",
    ") -> Union [Tuple[List[torch.Tensor]], None]:\n",
    "    \"\"\"\n",
    "    Runs batched inference on a dataset using a decoder-only language model.\n",
    "    For each batch, generates answers, extracts token-level activations for the generated answer,\n",
    "    and appends the results to a pickle file.\n",
    "\n",
    "    Hidden states are captured via a single forward hook on the whole prompt and generated\n",
    "    prompt and generated answer. These representations can be saved to a pickle file \n",
    "    or returned directly.\n",
    "\n",
    "    **Doesn't support beam serach**\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : PreTrainedModel\n",
    "        The causal language model to evaluate (e.g., LLaMA).\n",
    "    tokenizer : PreTrainedTokenizer\n",
    "        The corresponding tokenizer.\n",
    "    dataset : Dataset\n",
    "        The input dataset.\n",
    "    batch_size : int\n",
    "        Number of samples per batch.\n",
    "    idx_start_sample : int\n",
    "        Index of the first sample to process from the dataset.\n",
    "    max_samples : int\n",
    "        Total number of examples to process from the dataset, starting from idx_start_sample. \n",
    "    save_to_pkl : bool\n",
    "        If True, activations are appended to the pickle file at output_path.\n",
    "        If False, the function returns a list of activations.\n",
    "    output_path : str\n",
    "        Path to the pickle file for saving intermediate results.\n",
    "    build_prompt_fn : Callable\n",
    "        Function to build a prompt from context and question.\n",
    "    get_layer_output_fn : Callable\n",
    "        Function to extract the output of a specific model layer. \n",
    "    layer_idx : int\n",
    "        Index of the transformer layer to extract activations from (default: -1 for last layer).\n",
    "    extract_token_activations_fn : Callable\n",
    "        Function to extract token activations from a model layer (default is average).\n",
    "    include_prompt : bool\n",
    "        Whether to include the prompt in the embedding extraction. \n",
    "        - If include_prompt=False: \n",
    "            start_offset is set to prompt length and end_offset is set to 0.\n",
    "        - If include_prompt=True: \n",
    "            uses start_offset and end_offset specified in **kwargs (defaults to 0).\n",
    "        *Note:* Tokenization will always include the prompt.  \n",
    "    **kwargs :\n",
    "        Extra keyword arguments passed to extract_token_activations_fn, including start_offset.\n",
    "    \"\"\"    \n",
    "    batch_activations = []  # Chosen token activation vectors\n",
    "    #  batch_prompt_and_gen_activations\n",
    "\n",
    "    for i in tqdm(range(idx_start_sample, idx_start_sample + max_samples, batch_size)):\n",
    "        batch_gen_answers  = []   # Generated answers\n",
    "        # batch_gen_answers\n",
    " \n",
    "        # ==============================\n",
    "        # Prepare input batch\n",
    "        # ==============================\n",
    "        batch = extract_batch(dataset, i, batch_size)\n",
    "        prompts = [build_prompt_fn(s[\"context\"], s[\"question\"]) for s in batch]\n",
    "        inputs = tokenizer(prompts, padding=True, truncation=True, return_tensors=\"pt\").to(model.device)\n",
    "        prompt_len = inputs[\"input_ids\"].shape[1] # prompt length after padding\n",
    "        \n",
    "        # Compute the number of non-padding tokens in each prompt (true prompt length)\n",
    "        prompt_non_pad_len = inputs[\"attention_mask\"].sum(dim=1).tolist()  # Shape (batch_size,)\n",
    "\n",
    "    \n",
    "        # ==============================\n",
    "        # Generate the answer\n",
    "        # ==============================\n",
    "        # Pass inputs through the model. When the target layer is reached,\n",
    "        # the hook executes and saves its output in captured_hidden.\n",
    "        outputs = generate(model, inputs, tokenizer, max_new_tokens=50, k_beams=1)\n",
    "        outputs_ids = outputs.sequences\n",
    "\n",
    "        # ==============================\n",
    "        # Decode full sequences (prompt + generated answer) \n",
    "        # ==============================\n",
    "        # Separate prompt and generated answer \n",
    "        prompt_ids = inputs[\"input_ids\"]                # Shape: (batch_size, prompt_len)\n",
    "\n",
    "        # HERE SELECT OFFSET ? I THINK YES\n",
    "\n",
    "        generated_ids = outputs_ids[:, prompt_len:]     # Shape: (batch_size, gen_len)\n",
    "        # Batch decoding of the prompt and the generated answers\n",
    "        prompt_texts = tokenizer.batch_decode(prompt_ids, skip_special_tokens=True)    \n",
    "        generated_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        generated_texts = [text.strip() for text in generated_texts] # deletes spaces at the beginning/end of the generated text\n",
    "        # Concatenate each prompt text with its generated text (for subsequent tokenization)\n",
    "        prompt_and_gen_texts = [p + g for p, g in zip(prompt_texts, generated_texts)]\n",
    "        # Store the generated answers\n",
    "        batch_gen_answers.extend(generated_texts)\n",
    "\n",
    "        # ==============================\n",
    "        # Retokenize the full sequences (prompt + answer) \n",
    "        # ==============================\n",
    "        # Tokenize the full sequences again, with padding and truncation. We need to retokenize and \n",
    "        # cannot directly use `outputs_ids` since we need the attention_mask to extract token activations.\n",
    "        prompt_and_gen_ids = tokenizer(prompt_and_gen_texts, padding=True, truncation=True, return_tensors=\"pt\").to(model.device)\n",
    "       \n",
    "        # ==============================\n",
    "        # Register forward hook to capture layer output\n",
    "        # ==============================\n",
    "        # Hook to collect the hidden states after the forward pass\n",
    "        captured_hidden = {}\n",
    "        handle, call_counter = register_forward_activation_hook_fn(model, captured_hidden, layer_idx=layer_idx)\n",
    "\n",
    "        # ==============================\n",
    "        # Run model forward pass (hook captures activations)\n",
    "        # ==============================\n",
    "        # Pass inputs through the model. When the target layer is reached,\n",
    "        # the hook executes and saves its output in captured_hidden.\n",
    "        with torch.no_grad():\n",
    "            _ = model(**prompt_and_gen_ids, return_dict=True)\n",
    "        # Remove the hook to avoid memory leaks or duplicate logging\n",
    "        handle.remove() \n",
    "        \n",
    "        #print(f\"Hook was called {call_counter['count']} times.\")\n",
    "        if \"activations\" not in captured_hidden:\n",
    "            raise RuntimeError(\"Hook failed to capture activations.\")\n",
    "\n",
    "        # ==============================\n",
    "        # Extract token activations from captured layer\n",
    "        # ==============================\n",
    "        layer_output = captured_hidden[\"activations\"]  # Shape: (batch_size, seq_len, hidden_size)\n",
    "        print(\"layer_output : \" , layer_output.shape)\n",
    "\n",
    "        # ==============================\n",
    "        # Compute start_offsets and end_offsets for activation extraction\n",
    "        # ==============================\n",
    "        if exclude_prompt:\n",
    "            # start_offset is set to the true prompt length (non-padding tokens) and end_offset=0 ---\n",
    "            start_offsets = torch.tensor(prompt_non_pad_len, device=layer_output.device)  # Shape (batch_size,) \n",
    "            end_offsets = torch.zeros(len(prompts), device=layer_output.device)           # Shape (batch_size,) \n",
    "        else:\n",
    "            # Use the value from kwargs (or zeros if not provided) ---\n",
    "            start_offsets = kwargs.get(\"start_offset\", torch.zeros(len(prompts), device=layer_output.device)) # Shape (batch_size,)\n",
    "            end_offsets = kwargs.get(\"end_offset\", torch.zeros(len(prompts), device=layer_output.device))     # Shape (batch_size,) \n",
    "\n",
    "           \n",
    "        # Remove from kwargs to avoid passing it twice to the extraction function\n",
    "        kwargs.pop(\"start_offset\", None)\n",
    "        kwargs.pop(\"end_offset\", None)\n",
    "\n",
    "        print('prompt_and_gen_ids[\"attention_mask\"]:', prompt_and_gen_ids[\"attention_mask\"])\n",
    "\n",
    "\n",
    "\n",
    "        # ==============================\n",
    "        # Extract token activations from captured layer\n",
    "        # ==============================\n",
    "       \n",
    "\n",
    "\n",
    "        selected_token_vecs = extract_token_activations_fn(\n",
    "            selected_layer=layer_output,\n",
    "            attention_mask=prompt_and_gen_ids[\"attention_mask\"],\n",
    "            device=layer_output.device,\n",
    "            start_offset=start_offsets,  # Shape (batch_size,) \n",
    "            end_offset=end_offsets,      # Shape (batch_size,) \n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "        \n",
    "\n",
    "         # ==============================\n",
    "        # Store results (to file or memory)\n",
    "        # ==============================\n",
    "        batch_dataset_ids = [s['id'] for s in batch]  # 'id' field from dataset\n",
    "        batch_dataset_original_idx = [s['original_index'] for s in batch] # Original indices from dataset\n",
    "        activations = [selected_token_vecs[j].unsqueeze(0).cpu() for j in range(selected_token_vecs.size(0))] # Embeddings of generated answers\n",
    "\n",
    "        batch_results = {\n",
    "            \"id\": batch_dataset_ids,\n",
    "            \"original_indices\": batch_dataset_original_idx,\n",
    "            \"gen_answers\": batch_gen_answers,\n",
    "            \"activations\": activations\n",
    "        }\n",
    "\n",
    "        if save_to_pkl:\n",
    "            append_to_pickle(output_path, batch_results)\n",
    "        else:\n",
    "            batch_activations.extend(activations)\n",
    "        \n",
    "    if not save_to_pkl:\n",
    "        return batch_activations\n",
    "    \n",
    "\n",
    "run_prompt_and_generation_activation_extraction_teacherForcing(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=id_fit_dataset,\n",
    "    batch_size=2,\n",
    "    idx_start_sample=0,\n",
    "    max_samples=2,\n",
    "    save_to_pkl=False,\n",
    "    output_path=\"outputs/all_batch_results.pkl\",\n",
    "    build_prompt_fn=build_prompt,\n",
    "    register_forward_activation_hook_fn=register_forward_activation_hook,\n",
    "    layer_idx=-1,  \n",
    "    extract_token_activations_fn = extract_token_activations, \n",
    "    exclude_prompt = False,\n",
    "    start_offset=0,\n",
    "    end_offset=0\n",
    ") '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Length penalty (length_penalty): the length penalty to apply to outputs. length_penalty > 0.0 promotes longer sequences, while length_penalty < 0.0 encourages shorter sequences. This parameter will not impact the beam search paths, but only influence the choice of sequences in the end towards longer or shorter sequences. ? \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ood_env",
   "language": "python",
   "name": "ood_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
