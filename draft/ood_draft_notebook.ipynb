{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OOD detection applied to Hallucination Detection\n",
    "\n",
    " The goal is to predict if an INPUT prompt  is going to produce an hallucination or not (using OOD detection methods). For now, we don’t look at the output generated by the model, we may consider this in a second time. Retrieve ID samples:  To do this, take a general (easy) QA dataset containing questions along with their true hallucination-free answers. Feed the questions to the model. Let the model generate responses and check if the a given generated response is the same as the real hallucination-free answer. All the correct generated responses will be considered ID. More precisely, the ID dataset will consist of the embeddings of the last token of the last layer of the input (or maybe middle layer) of the correct generated responses.  Test a new sample to see if this is going to be OOD=hallucination: Take another dataset containing questions susceptible to trigger hallucinations along with the true hallucination-free answers (or no answer if the model cannot know the answer by any way and all response that the model might produce will necessarily be hallucinated). Feed a question to the model and let it generate a response. Check by comparing to the hallucination-free answer is that generated response is hallucinated or not. At the same time, apply an OOD detection method on the input question (at the last token last layer) and see if there is a correspondence between a high OOD score and a generated hallucination. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "# -----------------------------------\n",
    "import torch\n",
    "import sys\n",
    "import time \n",
    "import os \n",
    "# Add the path to the src directory\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 44\n",
    "BATCH_SIZE = 16\n",
    "MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "OUTPUT_DIR = \"../results/raw/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\n",
      "Cuda version: 12.6\n",
      "Number of available de GPU : 2\n",
      "GPU 1 : NVIDIA GeForce RTX 4090\n",
      "GPU 2 : NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "# Visualize setup \n",
    "# -----------------------------------\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Cuda version: {torch.version.cuda}\")\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Number of available de GPU : {num_gpus}\")\n",
    "for i in range(num_gpus):\n",
    "    print(f\"GPU {i + 1} : {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed everything\n",
    "# -----------------------------------\n",
    "from src.utils.general import seed_all\n",
    "seed_all(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdb5a4f5fedf414f9a4d5791a1d60463",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model\n",
    "# -----------------------------------\n",
    "from src.model_loader.llama_loader import load_llama\n",
    "\n",
    "model, tokenizer = load_llama(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ID dataset\n",
    "\n",
    "For the ID general dataset, we are going to use the SQUAD 1.1 dataset: \n",
    "\n",
    "***SQuAD 1.1:** Comprises over 100,000 question-answer pairs derived from more than 500 Wikipedia articles. Each question is paired with a specific segment of text (a span) from the corresponding article that serves as the answer.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Dataset Information =====\n",
      "Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers', 'original_index', 'is_impossible'],\n",
      "    num_rows: 10000\n",
      "})\n",
      "Mean ground-truth answer length: 3.19, Max length: 30\n",
      "Mean context + question length: 129.68, Max length: 497\n"
     ]
    }
   ],
   "source": [
    "# Load ID dataset\n",
    "# -----------------------------------\n",
    "from src.data_reader.squad_loader import load_id_fit_dataset\n",
    "\n",
    "id_fit_dataset = load_id_fit_dataset()\n",
    "id_fit_dataset = id_fit_dataset.shuffle(SEED) \n",
    "id_fit_dataset = id_fit_dataset.slice(idx_start=0, idx_end=10000)\n",
    "id_fit_dataset.print_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= Analyze one generation  =========\n",
      "----- Prompt construction: 0.000 sec\n",
      "----- Tokenization: 0.003 sec\n",
      "----- Token extraction: 0.390 sec\n",
      "----- Generation: 0.606 sec\n",
      "----- Decoding: 0.000 sec\n",
      "----- Similarity scoring: 0.043 sec\n",
      "\n",
      "=== Prompt ===\n",
      "<s>[INST]\n",
      "\n",
      "Just give the answer, without a complete sentence.           \n",
      "\n",
      "Context:\n",
      "Under a front page headline \"The Truth\", the paper printed allegations provided to them that some fans picked the pockets of crushed victims, that others urinated on members of the emergency services as they tried to help and that some even assaulted a police constable \"whilst he was administering the kiss of life to a patient.\" Despite the headline, written by Kelvin MacKenzie, the story was based on allegations either by unnamed and unattributable sources, or hearsay accounts of what named individuals had said – a fact made clear to MacKenzie by Harry Arnold, the reporter who wrote the story.\n",
      "\n",
      "Question:\n",
      "What was the story based on?\n",
      "\n",
      "Answer:\n",
      "[/INST]\n",
      "\n",
      "=== Shapes ===\n",
      "Shape - number of tokens: torch.Size([1, 184])\n",
      "Shape - selected_layer: torch.Size([1, 184, 4096])\n",
      "\n",
      "=== Generated Answer ===\n",
      "Allegations from unnamed and unattributable sources or hearsay accounts.\n",
      "\n",
      "=== Ground-truth Answer ===\n",
      "allegations either by unnamed and unattributable sources, or hearsay accounts of what named individuals had said\n",
      "\n",
      "=== Similarity Scores ===\n",
      "ROUGE-L F1: 0.6400\n",
      "Sentence-BERT Cosine Similarity: 0.8867\n",
      "Is generated answer correct: True\n"
     ]
    }
   ],
   "source": [
    "# Visualize one generation with the ID dataset\n",
    "# -----------------------------------\n",
    "from src.inference.inference_utils import analyze_single_generation, build_prompt, get_layer_output, extract_last_token_activations\n",
    "\n",
    "_ = analyze_single_generation(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer, \n",
    "    dataset=id_fit_dataset,\n",
    "    sample_idx=0,\n",
    "    build_prompt_fn=build_prompt,\n",
    "    get_layer_output_fn=get_layer_output,\n",
    "    layer_idx=-1,\n",
    "    extract_token_activations_fn=extract_last_token_activations,\n",
    "    offset=-5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve ID embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start retrieving ID embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:14<00:00,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...end!\n",
      "ID embeddings: Time elapsed: 00 min 14 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Retrieve ID embeddings and save results \n",
    "# -----------------------------------\n",
    "from src.inference.inference_utils import batch_extract_token_activations_with_generation, build_prompt, get_layer_output, extract_last_token_activations\n",
    "from src.utils.general import print_time_elapsed\n",
    "\n",
    "# Runs batched inference on a dataset using a decoder-only language model.\n",
    "# For each batch, generates answers, computes semantic similarity scores, extracts token-level activations,\n",
    "# and appends the results to a pickle file.\n",
    "print(\"\\nStart retrieving ID embeddings...\")\n",
    "t0 = time.time()\n",
    "\n",
    "batch_extract_token_activations_with_generation(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=id_fit_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    idx_start_sample=0,\n",
    "    max_samples=len(id_fit_dataset),\n",
    "    output_path = OUTPUT_DIR + \"id_fit_results.pkl\",\n",
    "    build_prompt_fn=build_prompt,\n",
    "    get_layer_output_fn=get_layer_output,\n",
    "    layer_idx=-1,  \n",
    "    extract_token_activations_fn=extract_last_token_activations,\n",
    "    offset=-5\n",
    ")\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"...end!\")\n",
    "print_time_elapsed(t0, t1, label=\"ID embeddings: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free memory \n",
    "del id_fit_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load OOD/Hallucinations test datasets \n",
    "\n",
    "To evaluate the Hallucination detection in question answering using OOD detection methods, we will use datasets in the SQuAD style:\n",
    "\n",
    "***SQuAD 2.0:** SQuAD 2.0 extends the original **SQuAD 1.1** dataset by adding around 50,000 unanswerable questions. These questions are carefully designed to look similar to answerable ones, making it more challenging for models to determine when there isn’t enough information in the context to provide an answer.*\n",
    "\n",
    "**Test Dataset Composition** \\\n",
    "Our test set will include two types of samples:\n",
    "- ***Impossible samples***: Questions that cannot be answered based on the provided context (i.e., the answer is not present in the text). These are taken from the training split of SQuAD 2.0, selecting only the unanswerable questions.\n",
    "- ***Possible samples***: Questions where the answer is explicitly present in the context. These are drawn from the validation split of SQuAD 1.1. This ensures there is no overlap with the in-distribution (ID) data from the SQuAD 1.1 training split.\n",
    "\n",
    "**Note on Evaluation Scope**\\\n",
    "Currently, our evaluation focuses on whether the model can answer questions using only the information provided in the input context. We do not test the model’s internal knowledge or ability to answer questions without supporting context. However, this setup closely matches the OOD scenario: if the information is not in the text, the model should recognize and indicate this.\n",
    "\n",
    "**Additional Dataset: TriviaQA**\\\n",
    "To further test generalization, we will also use the TriviaQA dataset. Like SQuAD, TriviaQA provides question-answering prompts with supporting context. The model must either extract the correct answer from the context or correctly identify when the answer is not present.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Dataset Information =====\n",
      "Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers', 'original_index', 'is_impossible'],\n",
      "    num_rows: 1000\n",
      "})\n",
      "Mean ground-truth answer length: 2.94, Max length: 25\n",
      "Mean context + question length: 132.44, Max length: 513\n",
      "\n",
      "===== Dataset Information =====\n",
      "Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers', 'original_index', 'is_impossible'],\n",
      "    num_rows: 1000\n",
      "})\n",
      "No valid ground-truth answers to compute length stats.\n",
      "Mean context + question length: 129.01, Max length: 451\n"
     ]
    }
   ],
   "source": [
    "# Load test datasets\n",
    "# -----------------------------------\n",
    "from src.data_reader.squad_loader import load_id_test_dataset, load_od_test_dataset\n",
    "\n",
    "# Load possible test dataset \n",
    "id_test_dataset = load_id_test_dataset()\n",
    "id_test_dataset = id_test_dataset.shuffle(SEED) \n",
    "id_test_dataset = id_test_dataset.slice(idx_start=0, idx_end=1000)\n",
    "id_test_dataset.print_info()\n",
    "\n",
    "# Load impossible test dataset \n",
    "od_test_dataset = load_od_test_dataset()\n",
    "od_test_dataset = od_test_dataset.shuffle(SEED) \n",
    "od_test_dataset = od_test_dataset.slice(idx_start=0, idx_end=1000)\n",
    "od_test_dataset.print_info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= Analyze one generation  =========\n",
      "----- Prompt construction: 0.000 sec\n",
      "----- Tokenization: 0.021 sec\n",
      "----- Token extraction: 0.044 sec\n",
      "----- Generation: 0.076 sec\n",
      "----- Decoding: 0.000 sec\n",
      "----- Similarity scoring: 0.004 sec\n",
      "\n",
      "=== Prompt ===\n",
      "<s>[INST]\n",
      "\n",
      "Just give the answer, without a complete sentence. Reply with 'Impossible to answer' if answer not in context.            \n",
      "\n",
      "Context:\n",
      "In the Catholic Church, canon law is the system of laws and legal principles made and enforced by the Church's hierarchical authorities to regulate its external organization and government and to order and direct the activities of Catholics toward the mission of the Church.\n",
      "\n",
      "Question:\n",
      "What mission this Canon law directly activities of all Christians towards?\n",
      "\n",
      "Answer:\n",
      "[/INST]\n",
      "\n",
      "=== Shapes ===\n",
      "Shape - number of tokens: torch.Size([1, 119])\n",
      "Shape - selected_layer: torch.Size([1, 119, 4096])\n",
      "\n",
      "=== Generated Answer ===\n",
      "Mission\n",
      "\n",
      "=== Ground-truth Answer ===\n",
      "\n",
      "\n",
      "=== Similarity Scores ===\n",
      "ROUGE-L F1: 0.0000\n",
      "Sentence-BERT Cosine Similarity: 0.3657\n",
      "Is generated answer correct: False\n"
     ]
    }
   ],
   "source": [
    "# Visualize one generation with the test impossible dataset\n",
    "# -----------------------------------\n",
    "from src.inference.inference_utils import analyze_single_generation, build_impossible_prompt, get_layer_output, extract_last_token_activations\n",
    "\n",
    "_ = analyze_single_generation(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer, \n",
    "    dataset=od_test_dataset,\n",
    "    sample_idx=500,\n",
    "    build_prompt_fn=build_impossible_prompt,\n",
    "    get_layer_output_fn=get_layer_output,\n",
    "    layer_idx=-1,\n",
    "    extract_token_activations_fn=extract_last_token_activations,\n",
    "    offset=-5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve test embeddings \n",
    "\n",
    "Test embeddings which may be OOD/Hallucinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start retrieving test impossible embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:50<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...end!\n",
      "Test impossible embeddings: Time elapsed: 00 min 50 sec\n",
      "\n",
      "\n",
      "Start retrieving test possible embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:51<00:00,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...end!\n",
      "Test possible embeddings: Time elapsed: 00 min 51 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Retrieve test embeddings and save results \n",
    "# -----------------------------------\n",
    "from src.inference.inference_utils import batch_extract_token_activations, build_prompt, get_layer_output, extract_last_token_activations\n",
    "from src.utils.general import print_time_elapsed\n",
    "\n",
    "# Runs batched inference on a dataset using a decoder-only language model.\n",
    "# For each batch, gextracts token-level activations, and appends the results to a pickle file.\n",
    "print(\"\\nStart retrieving test impossible embeddings...\")\n",
    "t2 = time.time()\n",
    "batch_extract_token_activations(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=od_test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    idx_start_sample=0,\n",
    "    max_samples=len(od_test_dataset),\n",
    "    save_to_pkl = True,\n",
    "    output_path = OUTPUT_DIR + \"od_test_results.pkl\",\n",
    "    build_prompt_fn=build_prompt,\n",
    "    get_layer_output_fn=get_layer_output,\n",
    "    layer_idx=-1,  \n",
    "    extract_token_activations_fn=extract_last_token_activations,\n",
    "    offset=-5\n",
    ")\n",
    "t3 = time.time()\n",
    "print(\"...end!\")\n",
    "print_time_elapsed(t2, t3, label=\"Test impossible embeddings: \")\n",
    "\n",
    "\n",
    "print(\"\\nStart retrieving test possible embeddings...\")\n",
    "t4 = time.time()\n",
    "batch_extract_token_activations(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=id_test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    idx_start_sample=0,\n",
    "    max_samples=len(id_test_dataset),\n",
    "    save_to_pkl = True,\n",
    "    output_path = OUTPUT_DIR + \"id_test_results.pkl\",\n",
    "    build_prompt_fn=build_prompt,\n",
    "    get_layer_output_fn=get_layer_output,\n",
    "    layer_idx=-1,  \n",
    "    extract_token_activations_fn=extract_last_token_activations,\n",
    "    offset=-5\n",
    ")\n",
    "t5 = time.time()\n",
    "print(\"...end!\")\n",
    "print_time_elapsed(t4, t5, label=\"Test possible embeddings: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free memory \n",
    "del od_test_dataset \n",
    "del id_test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load extracted embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10000 samples from: ../results/raw/id_fit_results.pkl\n",
      "Size before filtering incorrect samples: 10000.\n",
      "Size after filtering: 8865. Filtered 1135 samples.\n"
     ]
    }
   ],
   "source": [
    "# Load ID embeddings \n",
    "# -----------------------------------\n",
    "from src.data_reader.pickle_io import load_pickle_batches\n",
    "from src.utils.general import filter_correct_entries\n",
    "\n",
    "# Load extracted embeddings \n",
    "id_fit_embeddings = load_pickle_batches(OUTPUT_DIR + \"id_fit_results.pkl\")\n",
    "# Only keep rows with simiar generated and ground-truth answers\n",
    "id_fit_embeddings = filter_correct_entries(id_fit_embeddings) \n",
    "# Concatenate all embeddings \n",
    "id_fit_embeddings = torch.cat(id_fit_embeddings['activations'], dim=0) # shape: [N, D]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1000 samples from: ../results/raw/id_test_results.pkl\n",
      "Loaded 1000 samples from: ../results/raw/od_test_results.pkl\n"
     ]
    }
   ],
   "source": [
    "# Load test embeddings \n",
    "# -----------------------------------\n",
    "from src.data_reader.pickle_io import load_pickle_batches\n",
    "\n",
    "# Load extracted possible and impossible embeddings \n",
    "od_test_embeddings = load_pickle_batches(OUTPUT_DIR + \"id_test_results.pkl\")\n",
    "id_test_embeddings = load_pickle_batches(OUTPUT_DIR + \"od_test_results.pkl\")\n",
    "# Concatenate possible and impossible all embeddings \n",
    "od_test_embeddings = torch.cat(od_test_embeddings['activations'], dim=0) # shape: [N, D]\n",
    "id_test_embeddings = torch.cat(id_test_embeddings['activations'], dim=0) # shape: [N, D]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform DKNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructs the FAISS index from ID data.\n",
    "# -----------------------------------\n",
    "from src.analysis.dknn import fit_to_dataset\n",
    "index = fit_to_dataset(id_fit_embeddings, use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute DKNN scores on test data\n",
    "# -----------------------------------\n",
    "from src.analysis.dknn import score_tensor\n",
    "k=50\n",
    "dknn_scores_id  = score_tensor(index, id_test_embeddings, nearest=k, batch_size=BATCH_SIZE)\n",
    "dknn_scores_ood = score_tensor(index, od_test_embeddings, nearest=k, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auROC: 0.5229\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC: 0.5229\n"
     ]
    }
   ],
   "source": [
    "# Compute and plot auroc \n",
    "# -----------------------------------\n",
    "from src.analysis.dknn import compute_auroc\n",
    "auroc, fpr, tpr, thresholds = compute_auroc(dknn_scores_id, dknn_scores_ood, plot=True)\n",
    "print(f\"AUROC: {auroc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":'("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brouillon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.inference.inference_utils import build_prompt, get_layer_output, compute_token_offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_average_token_activations(\n",
    "    selected_layer: torch.Tensor,\n",
    "    attention_mask: torch.Tensor,\n",
    "    device: torch.device,\n",
    "    start_offset : int = 0,\n",
    "    end_offset : int = 0\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Extract the mean activation vector over a token span for each sequence in a batch.\n",
    "    The span is defined by applying start_offset (from the first non-padding token)\n",
    "    and end_offset (from the last non-padding token).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    selected_layer : torch.Tensor\n",
    "        Output tensor from the selected model layer (batch_size x seq_len x hidden_size).\n",
    "    attention_mask : torch.Tensor\n",
    "        Attention mask (batch_size x seq_len).\n",
    "    device : torch.device\n",
    "        Device for computation.\n",
    "    start_offset : int\n",
    "        Offset from first non-padding token (to skip e.g. [INST]).\n",
    "    end_offset : int\n",
    "        Offset from last non-padding token (to skip e.g. [/INST]).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        Averaged embeddings (batch_size x hidden_size)\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, _ = selected_layer.shape\n",
    "\n",
    "    # Detect left padding if any sequence starts with padding\n",
    "    is_left_padding = (attention_mask[:, 0] == 0).any()\n",
    "\n",
    "    # Find the index of the first and the  last non-padding token for each sequence\n",
    "    if is_left_padding:\n",
    "        #--- For left padding, first non-padding token is at index: number of padding tokens\n",
    "        first_indices = attention_mask.argmax(dim=1)\n",
    "        #--- For left padding, last non-padding token is at the end: compute its index by flipping and offsetting from the end\n",
    "        last_indices = (attention_mask.size(1) - 1) - attention_mask.flip(dims=[1]).argmax(dim=1)\n",
    "    else:\n",
    "        #--- For right padding, first non-padding token is always at index 0\n",
    "        first_indices = torch.zeros(batch_size, dtype=torch.long, device=device)\n",
    "        #--- For right padding, last non-padding token is at: (number of non-padding tokens) - 1\n",
    "        last_indices = (attention_mask.sum(dim=1) - 1)\n",
    "\n",
    "    first_indices = first_indices.to(device)\n",
    "    last_indices = last_indices.to(device)\n",
    "\n",
    "    # Apply offsets (e.g., skip <s> [INST] or [\\INST])\n",
    "    target_first_indices = first_indices + start_offset #-1\n",
    "    target_last_indices = last_indices + end_offset #+1\n",
    "\n",
    "    # Clamp indices to valid range\n",
    "    target_first_indices = torch.clamp(target_first_indices, min=0, max=seq_len - 1)\n",
    "    target_last_indices = torch.clamp(target_last_indices, min=0, max=seq_len - 1)\n",
    "\n",
    "    # Compute mask for averaging\n",
    "    #--- Create a tensor of positions: shape (1, seq_len), then expand to (batch_size, seq_len)\n",
    "    positions = torch.arange(seq_len, device=device).unsqueeze(0).expand(batch_size, seq_len)\n",
    "    #--- Build a boolean mask: True where the position is within [target_first_indices, target_last_indices] for each sequence\n",
    "    mask = (positions >= target_first_indices.unsqueeze(1)) & (positions <= target_last_indices.unsqueeze(1))\n",
    "    #--- Convert the boolean mask to float and add a singleton dimension for broadcasting with selected_layer\n",
    "    mask = mask.float().unsqueeze(-1)  # (batch_size, seq_len, 1)\n",
    "\n",
    "    # Apply mask and compute mean\n",
    "    #--- Apply the mask to the activations: zero out tokens outside the target interval\n",
    "    masked = selected_layer * mask\n",
    "    #--- Count the number of selected tokens for each sequence (avoid division by zero with clamp)\n",
    "    counts = mask.sum(dim=1).clamp(min=1e-6)\n",
    "    #--- Compute the mean activation vector for each sequence over the selected interval\n",
    "    avg = masked.sum(dim=1) / counts # (batch_size, hidden_size)\n",
    "\n",
    "    # Optionally, return also the indices used\n",
    "    indices = torch.stack([target_first_indices, target_last_indices], dim=1)\n",
    "\n",
    "    return avg, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizer, PreTrainedModel, BatchEncoding\n",
    "import torch\n",
    "from datasets import  Dataset\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Any, Callable\n",
    "import time\n",
    "\n",
    "from src.evaluation.similarity_metrics import rouge_l_simScore, sentence_bert_simScore\n",
    "from src.inference.inference_utils import extract_batch, generate_answers\n",
    "\n",
    "def batch_extract_token_activations_with_generation(\n",
    "    model: PreTrainedModel,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    dataset: Dataset,\n",
    "    batch_size: int = 2,\n",
    "    idx_start_sample: int = 0,\n",
    "    max_samples: int = 2,\n",
    "    output_path: str = \"outputs/all_batch_results.pkl\",\n",
    "    build_prompt_fn: Callable[[str, str], str] = None,\n",
    "    get_layer_output_fn: Callable = None,\n",
    "    layer_idx: int = -1,  \n",
    "    extract_token_activations_fn: Callable = None,\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs batched inference on a dataset using a decoder-only language model.\n",
    "    For each batch, generates answers, computes semantic similarity scores, extracts token-level activations,\n",
    "    and appends the results to a pickle file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : PreTrainedModel\n",
    "        The causal language model to evaluate (e.g., LLaMA).\n",
    "    tokenizer : PreTrainedTokenizer\n",
    "        The corresponding tokenizer.\n",
    "    dataset : Dataset\n",
    "        The input dataset.\n",
    "    batch_size : int\n",
    "        Number of samples per batch.\n",
    "    idx_start_sample : int\n",
    "        Index of the first sample to process from the dataset.\n",
    "    max_samples : int\n",
    "        Total number of examples to process from the dataset, starting from idx_start_sample. \n",
    "    output_path : str\n",
    "        Path to the pickle file for saving intermediate results.\n",
    "    build_prompt_fn : Callable\n",
    "        Function to build a prompt from context and question.\n",
    "    get_layer_output_fn : Callable\n",
    "        Function to extract the output of a specific model layer.\n",
    "    layer_idx : int\n",
    "        Index of the transformer layer to extract activations from (default: -1 for last layer).\n",
    "    extract_token_activations_fn : Callable\n",
    "        Function to extract token activations from a model layer.\n",
    "    **kwargs :\n",
    "        Extra keyword arguments passed to extract_token_activations_fn.\n",
    "    \"\"\"\n",
    "    for i in tqdm(range(idx_start_sample, idx_start_sample + max_samples, batch_size)):\n",
    "        batch_answers = []               # Generated answers\n",
    "        batch_gt_answers = []            # Ground-truth answers\n",
    "        batch_is_correct = []            # 0/1 labels indicating correctness\n",
    "        batch_dataset_ids = []           # 'id' field from dataset\n",
    "        batch_dataset_original_idx = []  # Original indices from dataset\n",
    "        batch_rouge_scores = []          # Rouge-L scores\n",
    "        batch_sbert_scores = []          # Sentence-Bert scores\n",
    "\n",
    "        batch = extract_batch(dataset, i, batch_size)\n",
    "        prompts = [build_prompt_fn(s[\"context\"], s[\"question\"]) for s in batch]\n",
    "        print(f\"prompts: {prompts[0]},\\n{prompts[1]}\")\n",
    "        answers = [s[\"answers\"][\"text\"] for s in batch]\n",
    "        inputs = tokenizer(prompts, padding=True, truncation=True, return_tensors=\"pt\").to(model.device)\n",
    "        print(f\"inputs:\", inputs)\n",
    "        selected_layer = get_layer_output_fn(model, inputs, layer_idx)\n",
    "        selected_token_vecs, target_indices = extract_token_activations_fn(\n",
    "                selected_layer, \n",
    "                inputs[\"attention_mask\"], \n",
    "                device=selected_layer.device,\n",
    "                **kwargs) \n",
    "        print(\"selected_token_vecs\", selected_token_vecs)\n",
    "        \n",
    "        for k in range(len(prompts)):\n",
    "            print(\"=========DECODING==========\")\n",
    "            print(\"target_indices[k]\", target_indices[k])\n",
    "            print(f\"\\nDecoding START target_indices[k]:-----{tokenizer.decode(inputs['input_ids'][k][target_indices[k][0]:target_indices[k][1]+1])}-----Decoding end\\n\")\n",
    "            print(f\"SELECTED TOKENS ---{tokenizer.decode(inputs['input_ids'][k][target_indices[k][0]])} and {tokenizer.decode(inputs['input_ids'][k][target_indices[k][1]])}---\")\n",
    "        \n",
    "        output_ids = generate_answers(model, inputs, tokenizer)\n",
    "        \n",
    "        for j in range(len(prompts)):\n",
    "            # --- Decode token IDs into text ---\n",
    "            prompt_len = len(inputs[\"input_ids\"][j]) # Length of prompt j\n",
    "            generated_answer_ids = output_ids[j][prompt_len:] # Remove prompt prefix to isolate the generated answer\n",
    "            generated_answer = tokenizer.decode(generated_answer_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "            # --- Compute semantic similarity between model's answer and ground-truth ---    \n",
    "            rouge_l_score = rouge_l_simScore(generated_answer, answers[j])\n",
    "            if rouge_l_score >= 0.5:\n",
    "                is_correct = True\n",
    "                sbert_score = None\n",
    "            else:\n",
    "                sbert_score = sentence_bert_simScore(generated_answer, answers[j])\n",
    "                is_correct = (sbert_score >= 0.4)\n",
    "\n",
    "            # --- Store everything ---\n",
    "            batch_dataset_ids.append(batch[j]['id'])\n",
    "            batch_dataset_original_idx.append(batch[j]['original_index'])\n",
    "            batch_answers.append(generated_answer)\n",
    "            batch_gt_answers.append(answers[j])\n",
    "            batch_is_correct.append(int(is_correct))\n",
    "            batch_rouge_scores.append(rouge_l_score)\n",
    "            batch_sbert_scores.append(sbert_score)\n",
    "\n",
    "        # --- Save progress to pickle after each batch ---\n",
    "        batch_results = {\n",
    "            \"id\": batch_dataset_ids,\n",
    "            \"original_indices\": batch_dataset_original_idx,\n",
    "            \"gen_answers\": batch_answers,\n",
    "            \"ground_truths\": batch_gt_answers,\n",
    "            \"activations\": [selected_token_vecs[i].unsqueeze(0).cpu() for i in range(selected_token_vecs.size(0))],\n",
    "            \"is_correct\": batch_is_correct,\n",
    "            \"sbert_scores\": batch_sbert_scores,\n",
    "            \"rouge_scores\": batch_rouge_scores\n",
    "        }\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Input text =====\n",
      "[INST] <<SYS>>\n",
      "Just give the answer, without a complete sentence. Reply with 'Impossible to answer' if answer not in context.\n",
      "<<SYS>>\n",
      "\n",
      "Context:\n",
      "Under a front page headline \"The Truth\", the paper printed allegations provided to them that some fans picked the pockets of crushed victims, that others urinated on members of the emergency services as they tried to help and that some even assaulted a police constable \"whilst he was administering the kiss of life to a patient.\" Despite the headline, written by Kelvin MacKenzie, the story was based on allegations either by unnamed and unattributable sources, or hearsay accounts of what named individuals had said – a fact made clear to MacKenzie by Harry Arnold, the reporter who wrote the story.\n",
      "\n",
      "Question:\n",
      "What was the story based on?\n",
      "\n",
      "Answer:\n",
      "[/INST]\n",
      "\n",
      "===== Decoded text between `start_offset` and `end_offset` =====\n",
      "----START TEXT---<<SYS>>\n",
      "Just give the answer, without a complete sentence. Reply with 'Impossible to answer' if answer not in context.\n",
      "<<SYS>>\n",
      "\n",
      "Context:\n",
      "Under a front page headline \"The Truth\", the paper printed allegations provided to them that some fans picked the pockets of crushed victims, that others urinated on members of the emergency services as they tried to help and that some even assaulted a police constable \"whilst he was administering the kiss of life to a patient.\" Despite the headline, written by Kelvin MacKenzie, the story was based on allegations either by unnamed and unattributable sources, or hearsay accounts of what named individuals had said – a fact made clear to MacKenzie by Harry Arnold, the reporter who wrote the story.\n",
      "\n",
      "Question:\n",
      "What was the story based on?\n",
      "\n",
      "Answer:\n",
      "----END TEXT---\n",
      "\n",
      "start_offset: 4, end_offset:-4\n"
     ]
    }
   ],
   "source": [
    "text = build_prompt(id_fit_dataset[0][\"context\"], id_fit_dataset[0][\"question\"])\n",
    "start_offset, end_offset = compute_token_offsets(\n",
    "    text=text,\n",
    "    tokenizer=tokenizer,\n",
    "    start_phrase=\"<<SYS>>\",\n",
    "    end_phrase=\"Answer:\\n\",\n",
    "    debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompts: [INST] <<SYS>>\n",
      "Just give the answer, without a complete sentence. Reply with 'Impossible to answer' if answer not in context.\n",
      "<<SYS>>\n",
      "\n",
      "Context:\n",
      "Under a front page headline \"The Truth\", the paper printed allegations provided to them that some fans picked the pockets of crushed victims, that others urinated on members of the emergency services as they tried to help and that some even assaulted a police constable \"whilst he was administering the kiss of life to a patient.\" Despite the headline, written by Kelvin MacKenzie, the story was based on allegations either by unnamed and unattributable sources, or hearsay accounts of what named individuals had said – a fact made clear to MacKenzie by Harry Arnold, the reporter who wrote the story.\n",
      "\n",
      "Question:\n",
      "What was the story based on?\n",
      "\n",
      "Answer:\n",
      "[/INST],\n",
      "[INST] <<SYS>>\n",
      "Just give the answer, without a complete sentence. Reply with 'Impossible to answer' if answer not in context.\n",
      "<<SYS>>\n",
      "\n",
      "Context:\n",
      "The term Carnival is traditionally used in areas with a large Catholic presence. However, the Philippines, a predominantly Roman Catholic country, does not celebrate Carnival anymore since the dissolution of the Manila Carnival after 1939, the last carnival in the country. In historically Lutheran countries, the celebration is known as Fastelavn, and in areas with a high concentration of Anglicans and Methodists, pre-Lenten celebrations, along with penitential observances, occur on Shrove Tuesday. In Eastern Orthodox nations, Maslenitsa is celebrated during the last week before Great Lent. In German-speaking Europe and the Netherlands, the Carnival season traditionally opens on 11/11 (often at 11:11 a.m.). This dates back to celebrations before the Advent season or with harvest celebrations of St. Martin's Day.\n",
      "\n",
      "Question:\n",
      "When does Fastelavn occur?\n",
      "\n",
      "Answer:\n",
      "[/INST]\n",
      "inputs: {'input_ids': tensor([[    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     1,   518, 25580,\n",
      "         29962,  3532, 14816, 29903,  6778,    13, 14084,  2367,   278,  1234,\n",
      "         29892,  1728,   263,  4866, 10541, 29889, 10088,   368,   411,   525,\n",
      "          1888, 27338,   304,  1234, 29915,   565,  1234,   451,   297,  3030,\n",
      "         29889,    13,  9314, 14816, 29903,  6778,    13,    13,  2677, 29901,\n",
      "            13, 29177,   263,  4565,  1813,  2343,  1220,   376,  1576,  1605,\n",
      "          2806,   613,   278,  5650, 13350, 16831,   800,  4944,   304,   963,\n",
      "           393,   777, 24909, 18691,   278,   772,  9737,   310,  2181, 15392,\n",
      "          6879,  9893, 29892,   393,  4045,  5065,   262,   630,   373,  5144,\n",
      "           310,   278, 11176, 14703,  5786,   408,   896,  1898,   304,  1371,\n",
      "           322,   393,   777,  1584, 29159,   287,   263, 10974,  1040,   519,\n",
      "           376,  1332, 16613,   540,   471,  4113,  1531,   292,   278, 20057,\n",
      "           310,  2834,   304,   263, 16500,  1213, 19454,   278,  2343,  1220,\n",
      "         29892,  3971,   491, 27326,  3845,  4326, 29968,   264,  3914, 29892,\n",
      "           278,  5828,   471,  2729,   373, 16831,   800,  2845,   491,   443,\n",
      "         17514,   322,   443,  1131,  1091,  9246,  8974, 29892,   470,   540,\n",
      "          1503,   388, 15303,   310,   825,  4257, 15724,   750,  1497,   785,\n",
      "           263,  2114,  1754,  2821,   304,  4326, 29968,   264,  3914,   491,\n",
      "         10686, 20720, 29892,   278,  1634,  9555,  1058,  5456,   278,  5828,\n",
      "         29889,    13,    13, 16492, 29901,    13,  5618,   471,   278,  5828,\n",
      "          2729,   373, 29973,    13,    13, 22550, 29901,    13, 29961, 29914,\n",
      "         25580, 29962],\n",
      "        [    1,   518, 25580, 29962,  3532, 14816, 29903,  6778,    13, 14084,\n",
      "          2367,   278,  1234, 29892,  1728,   263,  4866, 10541, 29889, 10088,\n",
      "           368,   411,   525,  1888, 27338,   304,  1234, 29915,   565,  1234,\n",
      "           451,   297,  3030, 29889,    13,  9314, 14816, 29903,  6778,    13,\n",
      "            13,  2677, 29901,    13,  1576,  1840,  1704, 29876,  2561,   338,\n",
      "         11399,   635,  1304,   297, 10161,   411,   263,  2919, 11865, 10122,\n",
      "         29889,  2398, 29892,   278, 26260, 29892,   263,   758, 24130, 10835,\n",
      "          5917, 11865,  4234, 29892,   947,   451, 10894,   403,  1704, 29876,\n",
      "          2561, 15128,  1951,   278, 23556,   918,   310,   278,  2315,  4233,\n",
      "          1704, 29876,  2561,  1156, 29871, 29896, 29929, 29941, 29929, 29892,\n",
      "           278,  1833,  1559, 29876,  2561,   297,   278,  4234, 29889,   512,\n",
      "          3603,  1711, 24760,   273, 10916, 29892,   278, 10894,   362,   338,\n",
      "          2998,   408, 23786,   295,   485, 29876, 29892,   322,   297, 10161,\n",
      "           411,   263,  1880, 26702,   310,  3218,   506,   550,   322,  8108,\n",
      "          2879, 29892,   758, 29899, 29931, 19889, 10894,   800, 29892,  3412,\n",
      "           411,  6584,   277,  2556,  5820,  2925, 29892,  6403,   373,  1383,\n",
      "           307,   345,   323,  1041,  3250, 29889,   512, 16162, 23757, 17324,\n",
      "         19079, 29892,  8622,  2435,  1169, 29874,   338, 26301,  2645,   278,\n",
      "          1833,  4723,  1434,  7027,   365,   296, 29889,   512,  5332, 29899,\n",
      "          5965,  5086,  4092,   322,   278, 24553, 29892,   278,  1704, 29876,\n",
      "          2561,  4259, 11399,   635, 13246,   373, 29871, 29896, 29896, 29914,\n",
      "         29896, 29896,   313, 29877, 15535,   472, 29871, 29896, 29896, 29901,\n",
      "         29896, 29896,   263, 29889, 29885,  6250,   910, 10116,  1250,   304,\n",
      "         10894,   800,  1434,   278, 21255,  4259,   470,   411,  4023, 10147,\n",
      "         10894,   800,   310,   624, 29889,  6502, 29915, 29879,  8373, 29889,\n",
      "            13,    13, 16492, 29901,    13, 10401,   947, 23786,   295,   485,\n",
      "         29876,  6403, 29973,    13,    13, 22550, 29901,    13, 29961, 29914,\n",
      "         25580, 29962]], device='cuda:0'), 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "selected_token_vecs tensor([[ 1.3207, -1.1394,  1.0503,  ..., -0.6873,  0.8425, -0.9228],\n",
      "        [ 0.5228, -1.4229,  0.8777,  ..., -0.6418,  0.1306, -0.6244]],\n",
      "       device='cuda:1')\n",
      "=========DECODING==========\n",
      "target_indices[k] tensor([107, 267], device='cuda:1')\n",
      "\n",
      "Decoding START target_indices[k]:-----\n",
      "Context:\n",
      "Under a front page headline \"The Truth\", the paper printed allegations provided to them that some fans picked the pockets of crushed victims, that others urinated on members of the emergency services as they tried to help and that some even assaulted a police constable \"whilst he was administering the kiss of life to a patient.\" Despite the headline, written by Kelvin MacKenzie, the story was based on allegations either by unnamed and unattributable sources, or hearsay accounts of what named individuals had said – a fact made clear to MacKenzie by Harry Arnold, the reporter who wrote the story.\n",
      "\n",
      "Question:\n",
      "What was the story based on?\n",
      "\n",
      "Answer:\n",
      "-----Decoding end\n",
      "\n",
      "SELECTED TOKENS ---\n",
      " and \n",
      "---\n",
      "=========DECODING==========\n",
      "target_indices[k] tensor([ 40, 267], device='cuda:1')\n",
      "\n",
      "Decoding START target_indices[k]:-----\n",
      "Context:\n",
      "The term Carnival is traditionally used in areas with a large Catholic presence. However, the Philippines, a predominantly Roman Catholic country, does not celebrate Carnival anymore since the dissolution of the Manila Carnival after 1939, the last carnival in the country. In historically Lutheran countries, the celebration is known as Fastelavn, and in areas with a high concentration of Anglicans and Methodists, pre-Lenten celebrations, along with penitential observances, occur on Shrove Tuesday. In Eastern Orthodox nations, Maslenitsa is celebrated during the last week before Great Lent. In German-speaking Europe and the Netherlands, the Carnival season traditionally opens on 11/11 (often at 11:11 a.m.). This dates back to celebrations before the Advent season or with harvest celebrations of St. Martin's Day.\n",
      "\n",
      "Question:\n",
      "When does Fastelavn occur?\n",
      "\n",
      "Answer:\n",
      "-----Decoding end\n",
      "\n",
      "SELECTED TOKENS ---\n",
      " and \n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.51it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_extract_token_activations_with_generation(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=id_fit_dataset,\n",
    "    batch_size=2,\n",
    "    idx_start_sample= 0,\n",
    "    max_samples= 2,\n",
    "    output_path = \"outputs/all_batch_results.pkl\",\n",
    "    build_prompt_fn=build_impossible_prompt,\n",
    "    get_layer_output_fn=get_layer_output,\n",
    "    layer_idx = -1,  \n",
    "    extract_token_activations_fn=extract_average_token_activations,\n",
    "    start_offset = start_offset,\n",
    "    end_offset = end_offset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Input text =====\n",
      "[INST] <<SYS>>\n",
      "Just give the answer, without a complete sentence. Reply with 'Impossible to answer' if answer not in context.\n",
      "<<SYS>>\n",
      "\n",
      "Context:\n",
      "Under a front page headline \"The Truth\", the paper printed allegations provided to them that some fans picked the pockets of crushed victims, that others urinated on members of the emergency services as they tried to help and that some even assaulted a police constable \"whilst he was administering the kiss of life to a patient.\" Despite the headline, written by Kelvin MacKenzie, the story was based on allegations either by unnamed and unattributable sources, or hearsay accounts of what named individuals had said – a fact made clear to MacKenzie by Harry Arnold, the reporter who wrote the story.\n",
      "\n",
      "Question:\n",
      "What was the story based on?\n",
      "\n",
      "Answer:\n",
      "[/INST]\n",
      "\n",
      "===== Decoded text between `start_offset` and `end_offset` =====\n",
      "----START TEXT---\n",
      "Context:\n",
      "Under a front page headline \"The Truth\", the paper printed allegations provided to them that some fans picked the pockets of crushed victims, that others urinated on members of the emergency services as they tried to help and that some even assaulted a police constable \"whilst he was administering the kiss of life to a patient.\" Despite the headline, written by Kelvin MacKenzie, the story was based on allegations either by unnamed and unattributable sources, or hearsay accounts of what named individuals had said – a fact made clear to MacKenzie by Harry Arnold, the reporter who wrote the story.\n",
      "\n",
      "Question:\n",
      "What was the story based on?\n",
      "\n",
      "Answer:\n",
      "----END TEXT---\n",
      "\n",
      "start_offset: 40, end_offset:-4\n"
     ]
    }
   ],
   "source": [
    "from src.inference.inference_utils import (\n",
    "    build_prompt, \n",
    "    compute_token_offsets, \n",
    "    get_layer_output\n",
    ")\n",
    "text = build_prompt(id_fit_dataset[0][\"context\"], id_fit_dataset[0][\"question\"])\n",
    "start_offset, end_offset = compute_token_offsets(\n",
    "    text=text,\n",
    "    tokenizer=tokenizer,\n",
    "    start_phrase=\"\\nContext:\",\n",
    "    end_phrase=\"Answer:\\n\",\n",
    "    debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompts: [INST] <<SYS>>\n",
      "Just give the answer, without a complete sentence. Reply with 'Impossible to answer' if answer not in context.\n",
      "<<SYS>>\n",
      "\n",
      "Context:\n",
      "Under a front page headline \"The Truth\", the paper printed allegations provided to them that some fans picked the pockets of crushed victims, that others urinated on members of the emergency services as they tried to help and that some even assaulted a police constable \"whilst he was administering the kiss of life to a patient.\" Despite the headline, written by Kelvin MacKenzie, the story was based on allegations either by unnamed and unattributable sources, or hearsay accounts of what named individuals had said – a fact made clear to MacKenzie by Harry Arnold, the reporter who wrote the story.\n",
      "\n",
      "Question:\n",
      "What was the story based on?\n",
      "\n",
      "Answer:\n",
      "[/INST],\n",
      "[INST] <<SYS>>\n",
      "Just give the answer, without a complete sentence. Reply with 'Impossible to answer' if answer not in context.\n",
      "<<SYS>>\n",
      "\n",
      "Context:\n",
      "The term Carnival is traditionally used in areas with a large Catholic presence. However, the Philippines, a predominantly Roman Catholic country, does not celebrate Carnival anymore since the dissolution of the Manila Carnival after 1939, the last carnival in the country. In historically Lutheran countries, the celebration is known as Fastelavn, and in areas with a high concentration of Anglicans and Methodists, pre-Lenten celebrations, along with penitential observances, occur on Shrove Tuesday. In Eastern Orthodox nations, Maslenitsa is celebrated during the last week before Great Lent. In German-speaking Europe and the Netherlands, the Carnival season traditionally opens on 11/11 (often at 11:11 a.m.). This dates back to celebrations before the Advent season or with harvest celebrations of St. Martin's Day.\n",
      "\n",
      "Question:\n",
      "When does Fastelavn occur?\n",
      "\n",
      "Answer:\n",
      "[/INST]\n",
      "inputs: {'input_ids': tensor([[    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     1,   518, 25580,\n",
      "         29962,  3532, 14816, 29903,  6778,    13, 14084,  2367,   278,  1234,\n",
      "         29892,  1728,   263,  4866, 10541, 29889, 10088,   368,   411,   525,\n",
      "          1888, 27338,   304,  1234, 29915,   565,  1234,   451,   297,  3030,\n",
      "         29889,    13,  9314, 14816, 29903,  6778,    13,    13,  2677, 29901,\n",
      "            13, 29177,   263,  4565,  1813,  2343,  1220,   376,  1576,  1605,\n",
      "          2806,   613,   278,  5650, 13350, 16831,   800,  4944,   304,   963,\n",
      "           393,   777, 24909, 18691,   278,   772,  9737,   310,  2181, 15392,\n",
      "          6879,  9893, 29892,   393,  4045,  5065,   262,   630,   373,  5144,\n",
      "           310,   278, 11176, 14703,  5786,   408,   896,  1898,   304,  1371,\n",
      "           322,   393,   777,  1584, 29159,   287,   263, 10974,  1040,   519,\n",
      "           376,  1332, 16613,   540,   471,  4113,  1531,   292,   278, 20057,\n",
      "           310,  2834,   304,   263, 16500,  1213, 19454,   278,  2343,  1220,\n",
      "         29892,  3971,   491, 27326,  3845,  4326, 29968,   264,  3914, 29892,\n",
      "           278,  5828,   471,  2729,   373, 16831,   800,  2845,   491,   443,\n",
      "         17514,   322,   443,  1131,  1091,  9246,  8974, 29892,   470,   540,\n",
      "          1503,   388, 15303,   310,   825,  4257, 15724,   750,  1497,   785,\n",
      "           263,  2114,  1754,  2821,   304,  4326, 29968,   264,  3914,   491,\n",
      "         10686, 20720, 29892,   278,  1634,  9555,  1058,  5456,   278,  5828,\n",
      "         29889,    13,    13, 16492, 29901,    13,  5618,   471,   278,  5828,\n",
      "          2729,   373, 29973,    13,    13, 22550, 29901,    13, 29961, 29914,\n",
      "         25580, 29962],\n",
      "        [    1,   518, 25580, 29962,  3532, 14816, 29903,  6778,    13, 14084,\n",
      "          2367,   278,  1234, 29892,  1728,   263,  4866, 10541, 29889, 10088,\n",
      "           368,   411,   525,  1888, 27338,   304,  1234, 29915,   565,  1234,\n",
      "           451,   297,  3030, 29889,    13,  9314, 14816, 29903,  6778,    13,\n",
      "            13,  2677, 29901,    13,  1576,  1840,  1704, 29876,  2561,   338,\n",
      "         11399,   635,  1304,   297, 10161,   411,   263,  2919, 11865, 10122,\n",
      "         29889,  2398, 29892,   278, 26260, 29892,   263,   758, 24130, 10835,\n",
      "          5917, 11865,  4234, 29892,   947,   451, 10894,   403,  1704, 29876,\n",
      "          2561, 15128,  1951,   278, 23556,   918,   310,   278,  2315,  4233,\n",
      "          1704, 29876,  2561,  1156, 29871, 29896, 29929, 29941, 29929, 29892,\n",
      "           278,  1833,  1559, 29876,  2561,   297,   278,  4234, 29889,   512,\n",
      "          3603,  1711, 24760,   273, 10916, 29892,   278, 10894,   362,   338,\n",
      "          2998,   408, 23786,   295,   485, 29876, 29892,   322,   297, 10161,\n",
      "           411,   263,  1880, 26702,   310,  3218,   506,   550,   322,  8108,\n",
      "          2879, 29892,   758, 29899, 29931, 19889, 10894,   800, 29892,  3412,\n",
      "           411,  6584,   277,  2556,  5820,  2925, 29892,  6403,   373,  1383,\n",
      "           307,   345,   323,  1041,  3250, 29889,   512, 16162, 23757, 17324,\n",
      "         19079, 29892,  8622,  2435,  1169, 29874,   338, 26301,  2645,   278,\n",
      "          1833,  4723,  1434,  7027,   365,   296, 29889,   512,  5332, 29899,\n",
      "          5965,  5086,  4092,   322,   278, 24553, 29892,   278,  1704, 29876,\n",
      "          2561,  4259, 11399,   635, 13246,   373, 29871, 29896, 29896, 29914,\n",
      "         29896, 29896,   313, 29877, 15535,   472, 29871, 29896, 29896, 29901,\n",
      "         29896, 29896,   263, 29889, 29885,  6250,   910, 10116,  1250,   304,\n",
      "         10894,   800,  1434,   278, 21255,  4259,   470,   411,  4023, 10147,\n",
      "         10894,   800,   310,   624, 29889,  6502, 29915, 29879,  8373, 29889,\n",
      "            13,    13, 16492, 29901,    13, 10401,   947, 23786,   295,   485,\n",
      "         29876,  6403, 29973,    13,    13, 22550, 29901,    13, 29961, 29914,\n",
      "         25580, 29962]], device='cuda:0'), 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "selected_token_vecs tensor([[ 1.3207, -1.1394,  1.0503,  ..., -0.6873,  0.8425, -0.9228],\n",
      "        [ 0.5228, -1.4229,  0.8777,  ..., -0.6418,  0.1306, -0.6244]],\n",
      "       device='cuda:1')\n",
      "=========DECODING==========\n",
      "target_indices[k] tensor([107, 267], device='cuda:1')\n",
      "\n",
      "Decoding START target_indices[k]:-----\n",
      "Context:\n",
      "Under a front page headline \"The Truth\", the paper printed allegations provided to them that some fans picked the pockets of crushed victims, that others urinated on members of the emergency services as they tried to help and that some even assaulted a police constable \"whilst he was administering the kiss of life to a patient.\" Despite the headline, written by Kelvin MacKenzie, the story was based on allegations either by unnamed and unattributable sources, or hearsay accounts of what named individuals had said – a fact made clear to MacKenzie by Harry Arnold, the reporter who wrote the story.\n",
      "\n",
      "Question:\n",
      "What was the story based on?\n",
      "\n",
      "Answer:\n",
      "-----Decoding end\n",
      "\n",
      "SELECTED TOKENS ---\n",
      " and \n",
      "---\n",
      "=========DECODING==========\n",
      "target_indices[k] tensor([ 40, 267], device='cuda:1')\n",
      "\n",
      "Decoding START target_indices[k]:-----\n",
      "Context:\n",
      "The term Carnival is traditionally used in areas with a large Catholic presence. However, the Philippines, a predominantly Roman Catholic country, does not celebrate Carnival anymore since the dissolution of the Manila Carnival after 1939, the last carnival in the country. In historically Lutheran countries, the celebration is known as Fastelavn, and in areas with a high concentration of Anglicans and Methodists, pre-Lenten celebrations, along with penitential observances, occur on Shrove Tuesday. In Eastern Orthodox nations, Maslenitsa is celebrated during the last week before Great Lent. In German-speaking Europe and the Netherlands, the Carnival season traditionally opens on 11/11 (often at 11:11 a.m.). This dates back to celebrations before the Advent season or with harvest celebrations of St. Martin's Day.\n",
      "\n",
      "Question:\n",
      "When does Fastelavn occur?\n",
      "\n",
      "Answer:\n",
      "-----Decoding end\n",
      "\n",
      "SELECTED TOKENS ---\n",
      " and \n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.27it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_extract_token_activations_with_generation(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=id_fit_dataset,\n",
    "    batch_size=2,\n",
    "    idx_start_sample= 0,\n",
    "    max_samples= 2,\n",
    "    output_path = \"outputs/all_batch_results.pkl\",\n",
    "    build_prompt_fn=build_impossible_prompt,\n",
    "    get_layer_output_fn=get_layer_output,\n",
    "    layer_idx = -1,  \n",
    "    extract_token_activations_fn=extract_average_token_activations,\n",
    "    start_offset = start_offset,\n",
    "    end_offset = end_offset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizer, PreTrainedModel, BatchEncoding\n",
    "import torch\n",
    "from datasets import  Dataset\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Any, Callable, Tuple\n",
    "import time\n",
    "from src.inference.inference_utils import generate_answers, extract_batch\n",
    "\n",
    "def batch_extract_answer_token_activations(\n",
    "    model: PreTrainedModel,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    dataset: Dataset,\n",
    "    batch_size: int = 4,\n",
    "    idx_start_sample: int = 0,\n",
    "    max_samples: int = 1000,\n",
    "    output_path: str = \"outputs/all_batch_results.pkl\",\n",
    "    build_prompt_fn: Callable[[str, str], str] = None,\n",
    "    get_layer_output_fn: Callable = None,\n",
    "    layer_idx: int = -1,  \n",
    "    extract_token_activations_fn: Callable = None, \n",
    "    include_prompt: bool = True,\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs batched inference on a dataset using a decoder-only language model.\n",
    "    For each batch, generates answers, extracts token-level activations for the generated answer,\n",
    "    and appends the results to a pickle file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : PreTrainedModel\n",
    "        The causal language model to evaluate (e.g., LLaMA).\n",
    "    tokenizer : PreTrainedTokenizer\n",
    "        The corresponding tokenizer.\n",
    "    dataset : Dataset\n",
    "        The input dataset.\n",
    "    batch_size : int\n",
    "        Number of samples per batch.\n",
    "    idx_start_sample : int\n",
    "        Index of the first sample to process from the dataset.\n",
    "    max_samples : int\n",
    "        Total number of examples to process from the dataset, starting from idx_start_sample. \n",
    "    output_path : str\n",
    "        Path to the pickle file for saving intermediate results.\n",
    "    build_prompt_fn : Callable\n",
    "        Function to build a prompt from context and question.\n",
    "    get_layer_output_fn : Callable\n",
    "        Function to extract the output of a specific model layer. \n",
    "    layer_idx : int\n",
    "        Index of the transformer layer to extract activations from (default: -1 for last layer).\n",
    "    extract_token_activations_fn : Callable\n",
    "        Function to extract token activations from a model layer (default is average).\n",
    "    include_prompt : bool\n",
    "        Whether to include the prompt in the embedding extraction.\n",
    "        *Note:* Tokenization will always include the prompt.  \n",
    "    **kwargs :\n",
    "        Extra keyword arguments passed to extract_token_activations_fn, including start_offset.\n",
    "    \"\"\"\n",
    "    for i in tqdm(range(idx_start_sample, idx_start_sample + max_samples, batch_size)):\n",
    "        batch_answers = []               # Generated answers\n",
    "        batch_generated_embeddings = []  # Embeddings of generated answers\n",
    "        batch_dataset_ids = []           # 'id' field from dataset\n",
    "        batch_dataset_original_idx = []  # Original indices from dataset\n",
    "\n",
    "        # Extract a batch from the dataset\n",
    "        batch = extract_batch(dataset, i, batch_size)\n",
    "        prompts = [build_prompt_fn(s[\"context\"], s[\"question\"]) for s in batch]\n",
    "        \n",
    "        # Tokenize the prompt \n",
    "        inputs = tokenizer(prompts, padding=True, truncation=True, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        # Generate the answers for the batch\n",
    "        output_ids = generate_answers(model, inputs, tokenizer)\n",
    "        \n",
    "        for j in range(len(prompts)):\n",
    "            print(f\"***************** j={j} *****************\")\n",
    "            # --- Decode token IDs into text ---\n",
    "            prompt_len = len(inputs[\"input_ids\"][j])  # Length of the prompt for example j\n",
    "            generated_answer_ids = output_ids[j][prompt_len:]  # Remove prompt part\n",
    "            generated_answer = tokenizer.decode(generated_answer_ids, skip_special_tokens=True).strip()\n",
    "            print(\"prompt_len: \", prompt_len)\n",
    "            \n",
    "            # Create the full sequence: prompt + generated answer\n",
    "            full_input_sequence = tokenizer.decode(inputs[\"input_ids\"][j], skip_special_tokens=True) + generated_answer\n",
    "            print(\"full_input_sequence\", full_input_sequence)\n",
    "            \n",
    "            # --- Tokenize the full sequence (prompt + generated answer) for activation extraction ---\n",
    "            # We cannot directly use `output_ids` and need to retokenize since we need attention_mask for get_layer_output_fn\n",
    "            full_inputs = tokenizer(full_input_sequence, padding=True, truncation=True, return_tensors=\"pt\").to(model.device)\n",
    "            print(\"full_inputs\", full_inputs)\n",
    "\n",
    "            # --- Extract token embeddings for the full sequence ---\n",
    "            selected_layer = get_layer_output_fn(model, full_inputs, layer_idx)\n",
    "\n",
    "            # --- Start offset : if include_prompt is True, we start after the prompt ---\n",
    "            if include_prompt:\n",
    "                start_offset = prompt_len  # Start after the prompt if we include the prompt\n",
    "            else:\n",
    "                start_offset = kwargs.get(\"start_offset\", 0)  # Get start_offset from kwargs if no prompt included\n",
    "            \n",
    "            # Call the specified activation extraction function\n",
    "            selected_token_vecs, target_indices  = extract_token_activations_fn(\n",
    "                selected_layer, \n",
    "                full_inputs[\"attention_mask\"], \n",
    "                device=selected_layer.device,\n",
    "                start_offset=start_offset,  # Pass start_offset (calculated above)\n",
    "                **kwargs  # Pass other kwargs as needed (e.g., end_offset, etc.)\n",
    "            )\n",
    "\n",
    "            print(\"=========DECODING==========\")\n",
    "            print(\"target_indices\", target_indices)\n",
    "            print(f\"\\nDecoding START target_indices:-----{tokenizer.decode(full_inputs['input_ids'][0][target_indices[0][0]:target_indices[0][1]+1].tolist())}-----Decoding end\\n\")\n",
    "\n",
    "            print(\"=========END DECODING==========\")\n",
    "\n",
    "            # --- Store everything ---\n",
    "            batch_dataset_ids.append(batch[j]['id'])\n",
    "            batch_dataset_original_idx.append(batch[j]['original_index'])\n",
    "            batch_answers.append(generated_answer)\n",
    "            batch_generated_embeddings.append(selected_token_vecs.cpu())\n",
    "\n",
    "        # --- Save progress to pickle after each batch ---\n",
    "        '''\n",
    "        batch_results = {\n",
    "            \"id\": batch_dataset_ids,\n",
    "            \"original_indices\": batch_dataset_original_idx,\n",
    "            \"gen_answers\": batch_answers,\n",
    "            \"activations\": batch_generated_embeddings\n",
    "        }\n",
    "        '''\n",
    "        #append_to_pickle(output_path, batch_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************** j=0 *****************\n",
      "prompt_len:  341\n",
      "full_input_sequence [INST] <<SYS>>\n",
      "Just give the answer, without a complete sentence. Reply with 'Impossible to answer' if answer not in context.\n",
      "<<SYS>>\n",
      "\n",
      "Context:\n",
      "Madonna gave another provocative performance later that year at the 2003 MTV Video Music Awards, while singing \"Hollywood\" with Britney Spears, Christina Aguilera, and Missy Elliott. Madonna sparked controversy for kissing Spears and Aguilera suggestively during the performance. In October 2003, Madonna provided guest vocals on Spears' single \"Me Against the Music\". It was followed with the release of Remixed & Revisited. The EP contained remixed versions of songs from American Life and included \"Your Honesty\", a previously unreleased track from the Bedtime Stories recording sessions. Madonna also signed a contract with Callaway Arts & Entertainment to be the author of five children's books. The first of these books, titled The English Roses, was published in September 2003. The story was about four English schoolgirls and their envy and jealousy of each other. Kate Kellway from The Guardian commented, \"[Madonna] is an actress playing at what she can never be—a JK Rowling, an English rose.\" The book debuted at the top of The New York Times Best Seller list and became the fastest-selling children's picture book of all time.\n",
      "\n",
      "Question:\n",
      "What was the title of the first book Madonna penned?\n",
      "\n",
      "Answer:\n",
      "[/INST]The English Roses\n",
      "full_inputs {'input_ids': tensor([[    1,   518, 25580, 29962,  3532, 14816, 29903,  6778,    13, 14084,\n",
      "          2367,   278,  1234, 29892,  1728,   263,  4866, 10541, 29889, 10088,\n",
      "           368,   411,   525,  1888, 27338,   304,  1234, 29915,   565,  1234,\n",
      "           451,   297,  3030, 29889,    13,  9314, 14816, 29903,  6778,    13,\n",
      "            13,  2677, 29901,    13, 21878, 11586,  4846,  1790, 25725,  1230,\n",
      "          4180,  2678,   393,  1629,   472,   278, 29871, 29906, 29900, 29900,\n",
      "         29941, 28982, 13987,  6125,  9220, 29892,  1550, 23623,   376, 19984,\n",
      "         16239, 29908,   411,  3230,  3801,  5013,  1503, 29892,  2819,  1099,\n",
      "         24106,   309,  1572, 29892,   322,  4750, 29891, 26656,  1501, 29889,\n",
      "         26432, 16267,   287, 19341, 29891,   363, 20057,   292,  5013,  1503,\n",
      "           322, 24106,   309,  1572,  4368,  3598,  2645,   278,  4180, 29889,\n",
      "           512,  5533, 29871, 29906, 29900, 29900, 29941, 29892, 26432,  4944,\n",
      "         17838, 17985,   373,  5013,  1503, 29915,  2323,   376,  6816, 11454,\n",
      "           303,   278,  6125,  1642,   739,   471,  5643,   411,   278,  6507,\n",
      "           310,  5240, 11925,   669,   830,  1730,  1573, 29889,   450, 16502,\n",
      "         11122,  1083, 11925,  6910,   310, 12516,   515,  3082,  4634,   322,\n",
      "          5134,   376, 10858,  7906, 14596,   613,   263,  9251,   443,   276,\n",
      "          4611,  5702,   515,   278, 14195,  2230,   624,  3842, 16867, 21396,\n",
      "         29889, 26432,   884,  8794,   263,  8078,   411,  3037,   433,  1582,\n",
      "         11401,   669, 18189,   304,   367,   278,  4148,   310,  5320,  4344,\n",
      "         29915, 29879,  8277, 29889,   450,   937,   310,  1438,  8277, 29892,\n",
      "         25278,   450,  4223,  5678,   267, 29892,   471,  6369,   297,  3839,\n",
      "         29871, 29906, 29900, 29900, 29941, 29889,   450,  5828,   471,  1048,\n",
      "          3023,  4223,  3762, 29887,  9968,   322,  1009,  8829, 29891,   322,\n",
      "          1444, 20521, 29891,   310,  1269,   916, 29889, 23738,   476,   514,\n",
      "          1582,   515,   450, 29429, 19952, 29892, 14704, 21878, 11586, 29962,\n",
      "           338,   385, 20993,  8743,   472,   825,  1183,   508,  2360,   367,\n",
      "         30003, 29874,   435, 29968, 11438,  1847, 29892,   385,  4223, 11492,\n",
      "          1213,   450,  3143,  2553,  3860,   472,   278,  2246,   310,   450,\n",
      "          1570,  3088, 10277,  6407,   317,  4539,  1051,   322,  3897,   278,\n",
      "          5172,   342, 29899, 29879,  7807,  4344, 29915, 29879,  7623,  3143,\n",
      "           310,   599,   931, 29889,    13,    13, 16492, 29901,    13,  5618,\n",
      "           471,   278,  3611,   310,   278,   937,  3143, 26432,   282,  2108,\n",
      "           287, 29973,    13,    13, 22550, 29901,    13, 29961, 29914, 25580,\n",
      "         29962,  1576,  4223,  5678,   267]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "=========DECODING==========\n",
      "target_indices tensor([[341, 344]], device='cuda:0')\n",
      "\n",
      "Decoding START target_indices:-----The English Roses-----Decoding end\n",
      "\n",
      "=========END DECODING==========\n",
      "***************** j=1 *****************\n",
      "prompt_len:  341\n",
      "full_input_sequence [INST] <<SYS>>\n",
      "Just give the answer, without a complete sentence. Reply with 'Impossible to answer' if answer not in context.\n",
      "<<SYS>>\n",
      "\n",
      "Context:\n",
      "Following the success of Le Journal de Mickey (1934–44), dedicated comics magazines and full-colour comics albums became the primary outlet for comics in the mid-20th century. As in the US, at the time comics were seen as infantile and a threat to culture and literacy; commentators stated that \"none bear up to the slightest serious analysis\",[c] and that comics were \"the sabotage of all art and all literature\".[d]\n",
      "\n",
      "Question:\n",
      "In the United States in the middle of the 20th century comics were seen as a risk to culture and what?\n",
      "\n",
      "Answer:\n",
      "[/INST]Literacy\n",
      "full_inputs {'input_ids': tensor([[    1,   518, 25580, 29962,  3532, 14816, 29903,  6778,    13, 14084,\n",
      "          2367,   278,  1234, 29892,  1728,   263,  4866, 10541, 29889, 10088,\n",
      "           368,   411,   525,  1888, 27338,   304,  1234, 29915,   565,  1234,\n",
      "           451,   297,  3030, 29889,    13,  9314, 14816, 29903,  6778,    13,\n",
      "            13,  2677, 29901,    13, 29943,  2952,   292,   278,  2551,   310,\n",
      "           951,  8237,   316, 20279,  1989,   313, 29896, 29929, 29941, 29946,\n",
      "         29994, 29946, 29946,   511, 16955,   419,  1199,  2320,   834,  1475,\n",
      "           322,  2989, 29899,  1054,   473,   419,  1199, 20618,  3897,   278,\n",
      "          7601,   714,  1026,   363,   419,  1199,   297,   278,  7145, 29899,\n",
      "         29906, 29900,   386,  6462, 29889,  1094,   297,   278,  3148, 29892,\n",
      "           472,   278,   931,   419,  1199,   892,  3595,   408, 28056,   488,\n",
      "           322,   263, 28469,   304,  9257,   322,  4631,  4135, 29936,  3440,\n",
      "          4097,  8703,   393,   376,  9290, 11460,   701,   304,   278,  7248,\n",
      "           342, 10676,  7418,   613, 29961, 29883, 29962,   322,   393,   419,\n",
      "          1199,   892,   376,  1552, 15296,   327,   482,   310,   599,  1616,\n",
      "           322,   599, 12845,  1642, 29961, 29881, 29962,    13,    13, 16492,\n",
      "         29901,    13,   797,   278,  3303,  3900,   297,   278,  7256,   310,\n",
      "           278, 29871, 29906, 29900,   386,  6462,   419,  1199,   892,  3595,\n",
      "           408,   263, 12045,   304,  9257,   322,   825, 29973,    13,    13,\n",
      "         22550, 29901,    13, 29961, 29914, 25580, 29962, 24938,  4135]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========DECODING==========\n",
      "target_indices tensor([[198, 198]], device='cuda:0')\n",
      "\n",
      "Decoding START target_indices:-----acy-----Decoding end\n",
      "\n",
      "=========END DECODING==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_extract_answer_token_activations(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=id_fit_dataset,\n",
    "    batch_size=2,\n",
    "    idx_start_sample= 2,\n",
    "    max_samples= 2,\n",
    "    output_path = \"outputs/all_batch_results.pkl\",\n",
    "    build_prompt_fn=build_prompt,\n",
    "    get_layer_output_fn=get_layer_output,\n",
    "    layer_idx = -1,  \n",
    "    extract_token_activations_fn=extract_average_token_activations,\n",
    "    include_prompt = True,\n",
    "    #start_offset = 0,\n",
    "    end_offset = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_token_vecs, target_indices = extract_token_activations_fn(\n",
    "                selected_layer, \n",
    "                inputs[\"attention_mask\"], \n",
    "                device=selected_layer.device,\n",
    "                **kwargs) \n",
    "        print(\"selected_token_vecs\", selected_token_vecs)\n",
    "        \n",
    "        for k in range(len(prompts)):\n",
    "            print(\"=========DECODING==========\")\n",
    "            print(\"target_indices[k]\", target_indices[k])\n",
    "            print(f\"\\nDecoding START target_indices[k]:-----{tokenizer.decode(inputs['input_ids'][k][target_indices[k][0]:target_indices[k][1]+1])}-----Decoding end\\n\")\n",
    "            print(f\"SELECTED TOKENS ---{tokenizer.decode(inputs['input_ids'][k][target_indices[k][0]])} and {tokenizer.decode(inputs['input_ids'][k][target_indices[k][1]])}---\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************** j=0 *****************\n",
      "generated_answer The English Roses\n",
      "prompt_len:  341\n",
      "***************** j=1 *****************\n",
      "generated_answer Literacy\n",
      "prompt_len:  341\n",
      "**********************************\n",
      "full_input_sequence ['[INST] <<SYS>>\\nJust give the answer, without a complete sentence. Reply with \\'Impossible to answer\\' if answer not in context.\\n<<SYS>>\\n\\nContext:\\nMadonna gave another provocative performance later that year at the 2003 MTV Video Music Awards, while singing \"Hollywood\" with Britney Spears, Christina Aguilera, and Missy Elliott. Madonna sparked controversy for kissing Spears and Aguilera suggestively during the performance. In October 2003, Madonna provided guest vocals on Spears\\' single \"Me Against the Music\". It was followed with the release of Remixed & Revisited. The EP contained remixed versions of songs from American Life and included \"Your Honesty\", a previously unreleased track from the Bedtime Stories recording sessions. Madonna also signed a contract with Callaway Arts & Entertainment to be the author of five children\\'s books. The first of these books, titled The English Roses, was published in September 2003. The story was about four English schoolgirls and their envy and jealousy of each other. Kate Kellway from The Guardian commented, \"[Madonna] is an actress playing at what she can never be—a JK Rowling, an English rose.\" The book debuted at the top of The New York Times Best Seller list and became the fastest-selling children\\'s picture book of all time.\\n\\nQuestion:\\nWhat was the title of the first book Madonna penned?\\n\\nAnswer:\\n[/INST]The English Roses', '[INST] <<SYS>>\\nJust give the answer, without a complete sentence. Reply with \\'Impossible to answer\\' if answer not in context.\\n<<SYS>>\\n\\nContext:\\nFollowing the success of Le Journal de Mickey (1934–44), dedicated comics magazines and full-colour comics albums became the primary outlet for comics in the mid-20th century. As in the US, at the time comics were seen as infantile and a threat to culture and literacy; commentators stated that \"none bear up to the slightest serious analysis\",[c] and that comics were \"the sabotage of all art and all literature\".[d]\\n\\nQuestion:\\nIn the United States in the middle of the 20th century comics were seen as a risk to culture and what?\\n\\nAnswer:\\n[/INST]Literacy']\n",
      "full_inputs {'input_ids': tensor([[    1,   518, 25580, 29962,  3532, 14816, 29903,  6778,    13, 14084,\n",
      "          2367,   278,  1234, 29892,  1728,   263,  4866, 10541, 29889, 10088,\n",
      "           368,   411,   525,  1888, 27338,   304,  1234, 29915,   565,  1234,\n",
      "           451,   297,  3030, 29889,    13,  9314, 14816, 29903,  6778,    13,\n",
      "            13,  2677, 29901,    13, 21878, 11586,  4846,  1790, 25725,  1230,\n",
      "          4180,  2678,   393,  1629,   472,   278, 29871, 29906, 29900, 29900,\n",
      "         29941, 28982, 13987,  6125,  9220, 29892,  1550, 23623,   376, 19984,\n",
      "         16239, 29908,   411,  3230,  3801,  5013,  1503, 29892,  2819,  1099,\n",
      "         24106,   309,  1572, 29892,   322,  4750, 29891, 26656,  1501, 29889,\n",
      "         26432, 16267,   287, 19341, 29891,   363, 20057,   292,  5013,  1503,\n",
      "           322, 24106,   309,  1572,  4368,  3598,  2645,   278,  4180, 29889,\n",
      "           512,  5533, 29871, 29906, 29900, 29900, 29941, 29892, 26432,  4944,\n",
      "         17838, 17985,   373,  5013,  1503, 29915,  2323,   376,  6816, 11454,\n",
      "           303,   278,  6125,  1642,   739,   471,  5643,   411,   278,  6507,\n",
      "           310,  5240, 11925,   669,   830,  1730,  1573, 29889,   450, 16502,\n",
      "         11122,  1083, 11925,  6910,   310, 12516,   515,  3082,  4634,   322,\n",
      "          5134,   376, 10858,  7906, 14596,   613,   263,  9251,   443,   276,\n",
      "          4611,  5702,   515,   278, 14195,  2230,   624,  3842, 16867, 21396,\n",
      "         29889, 26432,   884,  8794,   263,  8078,   411,  3037,   433,  1582,\n",
      "         11401,   669, 18189,   304,   367,   278,  4148,   310,  5320,  4344,\n",
      "         29915, 29879,  8277, 29889,   450,   937,   310,  1438,  8277, 29892,\n",
      "         25278,   450,  4223,  5678,   267, 29892,   471,  6369,   297,  3839,\n",
      "         29871, 29906, 29900, 29900, 29941, 29889,   450,  5828,   471,  1048,\n",
      "          3023,  4223,  3762, 29887,  9968,   322,  1009,  8829, 29891,   322,\n",
      "          1444, 20521, 29891,   310,  1269,   916, 29889, 23738,   476,   514,\n",
      "          1582,   515,   450, 29429, 19952, 29892, 14704, 21878, 11586, 29962,\n",
      "           338,   385, 20993,  8743,   472,   825,  1183,   508,  2360,   367,\n",
      "         30003, 29874,   435, 29968, 11438,  1847, 29892,   385,  4223, 11492,\n",
      "          1213,   450,  3143,  2553,  3860,   472,   278,  2246,   310,   450,\n",
      "          1570,  3088, 10277,  6407,   317,  4539,  1051,   322,  3897,   278,\n",
      "          5172,   342, 29899, 29879,  7807,  4344, 29915, 29879,  7623,  3143,\n",
      "           310,   599,   931, 29889,    13,    13, 16492, 29901,    13,  5618,\n",
      "           471,   278,  3611,   310,   278,   937,  3143, 26432,   282,  2108,\n",
      "           287, 29973,    13,    13, 22550, 29901,    13, 29961, 29914, 25580,\n",
      "         29962,  1576,  4223,  5678,   267],\n",
      "        [    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     1,   518, 25580, 29962,\n",
      "          3532, 14816, 29903,  6778,    13, 14084,  2367,   278,  1234, 29892,\n",
      "          1728,   263,  4866, 10541, 29889, 10088,   368,   411,   525,  1888,\n",
      "         27338,   304,  1234, 29915,   565,  1234,   451,   297,  3030, 29889,\n",
      "            13,  9314, 14816, 29903,  6778,    13,    13,  2677, 29901,    13,\n",
      "         29943,  2952,   292,   278,  2551,   310,   951,  8237,   316, 20279,\n",
      "          1989,   313, 29896, 29929, 29941, 29946, 29994, 29946, 29946,   511,\n",
      "         16955,   419,  1199,  2320,   834,  1475,   322,  2989, 29899,  1054,\n",
      "           473,   419,  1199, 20618,  3897,   278,  7601,   714,  1026,   363,\n",
      "           419,  1199,   297,   278,  7145, 29899, 29906, 29900,   386,  6462,\n",
      "         29889,  1094,   297,   278,  3148, 29892,   472,   278,   931,   419,\n",
      "          1199,   892,  3595,   408, 28056,   488,   322,   263, 28469,   304,\n",
      "          9257,   322,  4631,  4135, 29936,  3440,  4097,  8703,   393,   376,\n",
      "          9290, 11460,   701,   304,   278,  7248,   342, 10676,  7418,   613,\n",
      "         29961, 29883, 29962,   322,   393,   419,  1199,   892,   376,  1552,\n",
      "         15296,   327,   482,   310,   599,  1616,   322,   599, 12845,  1642,\n",
      "         29961, 29881, 29962,    13,    13, 16492, 29901,    13,   797,   278,\n",
      "          3303,  3900,   297,   278,  7256,   310,   278, 29871, 29906, 29900,\n",
      "           386,  6462,   419,  1199,   892,  3595,   408,   263, 12045,   304,\n",
      "          9257,   322,   825, 29973,    13,    13, 22550, 29901,    13, 29961,\n",
      "         29914, 25580, 29962, 24938,  4135]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========DECODING==========\n",
      "target_indices[k] tensor([341, 344], device='cuda:0')\n",
      "\n",
      "Decoding START target_indices[k]:-----The English Roses-----Decoding end\n",
      "\n",
      "=========END DECODING==========\n",
      "=========DECODING==========\n",
      "target_indices[k] tensor([344, 344], device='cuda:0')\n",
      "\n",
      "Decoding START target_indices[k]:-----acy-----Decoding end\n",
      "\n",
      "=========END DECODING==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_extract_answer_token_activations(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=id_fit_dataset,\n",
    "    batch_size=2,\n",
    "    idx_start_sample= 2,\n",
    "    max_samples= 2,\n",
    "    output_path = \"outputs/all_batch_results.pkl\",\n",
    "    build_prompt_fn=build_prompt,\n",
    "    get_layer_output_fn=get_layer_output,\n",
    "    layer_idx = -1,  \n",
    "    extract_token_activations_fn=extract_average_token_activations,\n",
    "    include_prompt = True,\n",
    "    #start_offset = 0,\n",
    "    end_offset = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizer, PreTrainedModel, BatchEncoding\n",
    "import torch\n",
    "from datasets import  Dataset\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Any, Callable, Tuple\n",
    "import time\n",
    "from src.inference.inference_utils import (\n",
    "    build_prompt,\n",
    "    get_layer_output,\n",
    "    generate_answers, \n",
    "    extract_batch, \n",
    "    extract_last_token_activations,\n",
    "    extract_average_token_activations,\n",
    "    extract_max_token_activations\n",
    ")\n",
    "from src.data_reader.pickle_io import append_to_pickle\n",
    "\n",
    "\n",
    "def batch_extract_answer_token_activations(\n",
    "    model: PreTrainedModel,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    dataset: Dataset,\n",
    "    batch_size: int = 4,\n",
    "    idx_start_sample: int = 0,\n",
    "    max_samples: int = 1000,\n",
    "    save_to_pkl: bool = False,\n",
    "    output_path: str = \"outputs/all_batch_results.pkl\",\n",
    "    build_prompt_fn: Callable[[str, str], str] = None,\n",
    "    get_layer_output_fn: Callable = None,\n",
    "    layer_idx: int = -1,  \n",
    "    extract_token_activations_fn: Callable = None, \n",
    "    include_prompt: bool = True,\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs batched inference on a dataset using a decoder-only language model.\n",
    "    For each batch, generates answers, extracts token-level activations for the generated answer,\n",
    "    and appends the results to a pickle file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : PreTrainedModel\n",
    "        The causal language model to evaluate (e.g., LLaMA).\n",
    "    tokenizer : PreTrainedTokenizer\n",
    "        The corresponding tokenizer.\n",
    "    dataset : Dataset\n",
    "        The input dataset.\n",
    "    batch_size : int\n",
    "        Number of samples per batch.\n",
    "    idx_start_sample : int\n",
    "        Index of the first sample to process from the dataset.\n",
    "    max_samples : int\n",
    "        Total number of examples to process from the dataset, starting from idx_start_sample. \n",
    "    save_to_pkl : bool\n",
    "        If True, activations are appended to the pickle file at output_path.\n",
    "        If False, the function returns a list of activations.\n",
    "    output_path : str\n",
    "        Path to the pickle file for saving intermediate results.\n",
    "    build_prompt_fn : Callable\n",
    "        Function to build a prompt from context and question.\n",
    "    get_layer_output_fn : Callable\n",
    "        Function to extract the output of a specific model layer. \n",
    "    layer_idx : int\n",
    "        Index of the transformer layer to extract activations from (default: -1 for last layer).\n",
    "    extract_token_activations_fn : Callable\n",
    "        Function to extract token activations from a model layer (default is average).\n",
    "    include_prompt : bool\n",
    "        Whether to include the prompt in the embedding extraction.\n",
    "        *Note:* Tokenization will always include the prompt.  \n",
    "    **kwargs :\n",
    "        Extra keyword arguments passed to extract_token_activations_fn, including start_offset.\n",
    "    \"\"\"    \n",
    "    batch_activations = []  # Chosen token activation vectors\n",
    "\n",
    "    for i in tqdm(range(idx_start_sample, idx_start_sample + max_samples, batch_size)):\n",
    "        batch_answers = []   # Generated answers\n",
    " \n",
    "        # Extract a batch from the dataset\n",
    "        batch = extract_batch(dataset, i, batch_size)\n",
    "        prompts = [build_prompt_fn(s[\"context\"], s[\"question\"]) for s in batch]\n",
    "\n",
    "        # Tokenize the prompt \n",
    "        inputs = tokenizer(prompts, padding=True, truncation=True, return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        # Compute the number of non-padding tokens in each prompt (true prompt length)\n",
    "        prompt_non_pad_len = inputs[\"attention_mask\"].sum(dim=1).tolist()  # Shape (batch_size,)\n",
    "        print(\"prompt_lens\", prompt_non_pad_len)\n",
    "\n",
    "        # Generate the answers for the batch\n",
    "        output_ids = generate_answers(model, inputs, tokenizer)\n",
    "\n",
    "        # Build full sequences (prompt + generated answer) for each sample in the batch\n",
    "        full_sequences = []\n",
    "        for j in range(len(prompts)):\n",
    "            # --- Total length of the tokenized prompt, padding included ---\n",
    "            prompt_len = len(inputs[\"input_ids\"][j])  # Length of the prompt for example j\n",
    "            # --- Decode token IDs into text ---\n",
    "            generated_answer_ids = output_ids[j][prompt_len:]  # Remove prompt part\n",
    "            generated_answer = tokenizer.decode(generated_answer_ids, skip_special_tokens=True).strip()\n",
    "            # --- Decode the prompt tokens to text ---\n",
    "            prompt_text = tokenizer.decode(inputs[\"input_ids\"][j], skip_special_tokens=True)\n",
    "            # --- Combine prompt and answer for full sequence ---\n",
    "            full_sequences.append(prompt_text + generated_answer)\n",
    "            # --- Store generated answers ---\n",
    "            batch_answers.append(generated_answer)\n",
    "            print(\"generated_answer:\",generated_answer )\n",
    "\n",
    "        # Tokenize the full sequences (prompt + answer) again, with padding and truncation\n",
    "        # We need to retokenize and cannot directly use `output_ids` since we need attention_mask for get_layer_output_fn\n",
    "        full_inputs = tokenizer(full_sequences, padding=True, truncation=True, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        # Extract activations from the specified model layer for all sequences in the batch\n",
    "        selected_layer = get_layer_output_fn(model, full_inputs, layer_idx)\n",
    "\n",
    "        # Compute the start offsets for activation extraction\n",
    "        if include_prompt:\n",
    "            # --- If include_prompt is True, use the value from kwargs (or zeros if not provided) ---\n",
    "            start_offsets = kwargs.get(\"start_offset\", torch.zeros(len(prompts), device=selected_layer.device)) # Shape (batch_size,) \n",
    "        else:\n",
    "            # --- If include_prompt is False, use the true prompt length (non-padding tokens) ---\n",
    "            start_offsets = torch.tensor(prompt_non_pad_len, device=selected_layer.device)  # Shape (batch_size,) \n",
    "\n",
    "        # Remove start_offset from kwargs to avoid passing it twice to the extraction function\n",
    "        kwargs.pop(\"start_offset\", None)\n",
    "\n",
    "        # Call the specified activation extraction function\n",
    "        selected_token_vecs, target_indices = extract_token_activations_fn(\n",
    "            selected_layer,\n",
    "            full_inputs[\"attention_mask\"],\n",
    "            device=selected_layer.device,\n",
    "            start_offset=start_offsets,  # Shape (batch_size,) \n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "        # 9. (Optional) Decode for verification \n",
    "        if False:\n",
    "            for k in range(len(prompts)):\n",
    "                print(\"=========DECODING==========\")\n",
    "                print(\"target_indices[k]\", target_indices[k])\n",
    "                print(f\"\\nDecoding START target_indices[k]:-----{tokenizer.decode(full_inputs['input_ids'][k][int(target_indices[k][0]):int(target_indices[k][1])+1])}-----Decoding end\\n\")\n",
    "                print(\"=========END DECODING==========\")\n",
    "\n",
    "        # --- Store everything ---\n",
    "        batch_dataset_ids = [s['id'] for s in batch]  # 'id' field from dataset\n",
    "        batch_dataset_original_idx = [s['original_index'] for s in batch] # Original indices from dataset\n",
    "        activations = [selected_token_vecs[j].unsqueeze(0).cpu() for j in range(selected_token_vecs.size(0))] # Embeddings of generated answers\n",
    "\n",
    "       \n",
    "        # --- Save progress to pickle after each batch ---\n",
    "        batch_results = {\n",
    "            \"id\": batch_dataset_ids,\n",
    "            \"original_indices\": batch_dataset_original_idx,\n",
    "            \"gen_answers\": batch_answers,\n",
    "            \"activations\": activations\n",
    "        }\n",
    "\n",
    "        if save_to_pkl:\n",
    "            append_to_pickle(output_path, batch_results)\n",
    "        else:\n",
    "            batch_activations.extend(activations)\n",
    "        \n",
    "    if not save_to_pkl:\n",
    "        return batch_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:06<00:00,  1.33s/it]\n"
     ]
    }
   ],
   "source": [
    "from src.inference.inference_utils import batch_extract_answer_token_activations\n",
    "batch_extract_answer_token_activations(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=id_fit_dataset,\n",
    "    batch_size=2,\n",
    "    idx_start_sample= 5,\n",
    "    max_samples=10,\n",
    "    save_to_pkl = False, \n",
    "    output_path = \"../raw/TEST/all_batch_results.pkl\",\n",
    "    build_prompt_fn=build_prompt,\n",
    "    get_layer_output_fn=get_layer_output,\n",
    "    layer_idx = -1,  \n",
    "    extract_token_activations_fn=extract_max_token_activations,\n",
    "    include_prompt = False,\n",
    "    start_offset = 40,\n",
    "    end_offset = 0\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ood_env",
   "language": "python",
   "name": "ood_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
